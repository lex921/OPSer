
# CentOS7.9+Doker19.03.13 + K8S1.19.9 安装


## 重要概念
1、cluster是 计算、存储和网络资源的集合，k8s利用这些资源运行各种基于容器的应用。   
2、master是cluster的大脑，他的主要职责是调度，即决定将应用放在那里运行。master运行linux操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个master。     
3、node的职责是运行容器应用。node由master管理，node负责监控并汇报容器的状态，同时根据master的要求管理容器的生命周期。node运行在linux的操作系统上，可以是物理机或者是虚拟机。     
4、pod是k8s的最小工作单元。每个pod包含一个或者多个容器。pod中的容器会作为一个整体被master调度到一个node上运行。       
5、controller k8s通常不会直接创建pod,而是通过controller来管理pod的。controller中定义了pod的部署特性，比如有几个剧本，在什么样的node上运行等。为了满足不同的业务场景，k8s提供了多种controller，包括deployment、replicaset、daemonset、statefulset、job等。     
6、deployment是最常用的controller。deployment可以管理pod的多个副本，并确保pod按照期望的状态运行。      
7、replicaset实现了pod的多副本管理。使用deployment时会自动创建replicaset，也就是说deployment是通过replicaset来管理pod的多个副本的，我们通常不需要直接使用replicaset。     
8、daemonset用于每个node最多只运行一个pod副本的场景。正如其名称所示的，daemonset通常用于运行daemon。     
9、statefuleset能够保证pod的每个副本在整个生命周期中名称是不变的，而其他controller不提供这个功能。当某个pod发生故障需要删除并重新启动时，pod的名称会发生变化，同时statefulset会保证副本按照固定的顺序启动、更新或者删除。     
10、job用于运行结束就删除的应用，而其他controller中的pod通常是长期持续运行的。     
11、service k8s的 service定义了外界访问一组特定pod的方式。service有自己的IP和端口，service为pod提供了负载均衡。    
k8s运行容器pod与访问容器这两项任务分别由controller和service执行。      
12、namespace 可以将一个物理的cluster逻辑上划分成多个虚拟cluster，每个cluster就是一个namespace。不同的namespace里的资源是完全隔离的。     


## 一、基础环境准备：

centos7.9:
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2

[root@master-1 network-scripts]# vim /etc/sysconfig/network-scripts/ifcfg-ens33
UUID=dffff97a-dd35-4535-a5b9-3856a9c49c51

TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
DEVICE=ens33
IPADDR=172.16.201.134
PREFIX=24
GATEWAY=172.16.201.2
DNS1=114.114.114.114
IPV6_PRIVACY=no
ONBOOT=yes





#### 1、设置主机名
echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts

hostnamectl set-hostname master-1
hostnamectl set-hostname node-1
hostnamectl set-hostname node-2


#### 2、关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

#### 3、关闭selinux
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
[root@master-1 ~]# sestatus
SELinux status:                 disabled

#### 4、关闭swap
swapoff -a
vi /etc/fstab
#####/dev/mapper/centos-swap swap                    swap    defaults        0 0

echo vm.swappiness=0 >> /etc/sysctl.conf


#### 5、安装ntp
yum -y install ntp
systemctl enable ntpd
systemctl restart ntpd

crontab -e
*/10 * * * * ntpdate 1.cn.pool.ntp.org

#### 6、将桥接的IPV4流量传递到iptables 的链
[root@master-1 ~]# cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

echo '
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1' >> /etc/hosts



[root@master-1 ~]# cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

[root@master-1 ~]# sysctl --system    
* Applying /usr/lib/sysctl.d/00-system.conf ...
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
* Applying /etc/sysctl.conf ...


#### 7、安装Docker
##### 1）设置镜像的仓库
[root@master-1 yum.repos.d]# cd /etc/yum.repos.d/            
[root@master-1 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O/etc/yum.repos.d/docker-ce.repo        

##### 2）安装docker
[root@master-1 yum.repos.d]# yum install -y yum-utils device-mapper-persistent-data lvm2​

[root@master-1 yum.repos.d]# yum install docker-ce-19.03.13 docker-ce-cli-19.03.13 containerd.io     

指定安装的docker版本为19.03.13

##### 3)启动
[root@master ~] systemctl start docker

##### 4)配置systemd服务
[root@master-1 system]# cd  /usr/lib/systemd/system/       
[root@master-1 system]# vim docker.service      
[Unit]     
Description=Docker Application Container Engine     
Documentation=https://docs.docker.com       
After=network-online.target firewalld.service      
Wants=network-online.target      

[Service]     
Type=notify       

ExecStart=/usr/bin/dockerd     
ExecReload=/bin/kill -s HUP $MAINPID     
LimitNOFILE=infinity        
LimitNPROC=infinity       
LimitCORE=infinity      

TimeoutStartSec=0  
Delegate=yes       
KillMode=process     
Restart=on-failure       
StartLimitBurst=3     
StartLimitInterval=60s      
      
[Install]    
WantedBy=multi-user.target       


[root@master-1 system]# vim  docker.service     
[Unit]     
Description=Docker Application Container Engine  
Documentation=https://docs.docker.com       
After=network-online.target firewalld.service     
Wants=network-online.target      

[Service]      
Type=notify     
ExecStart=/usr/bin/dockerd      
ExecReload=/bin/kill -s HUP $MAINPID      
LimitNOFILE=infinity      
LimitNPROC=infinity       
LimitCORE=infinity      
TimeoutStartSec=0      
Delegate=yes    
KillMode=process       
Restart=on-failure     
StartLimitBurst=3       
StartLimitInterval=60s    

[Install]            
WantedBy=multi-user.target          

###### 5)启动一套
[root@master-1 system]# systemctl daemon-reload              
[root@master-1 system]# systemctl enable docker.service                   
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /etc/systemd/system/docker.service.          

[root@master-1 system]# ps -aux | grep dockerd      
root       1670  0.2  3.6 591900 67708 ?        Ssl  10:52   0:00 /usr/bin/dockerd

[root@master-1 system]# systemctl status docker     
● docker.service - Docker Application Container Engine           
   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)      
   Active: active (running) since Wed 2021-09-22 10:52:20 CST; 3min 51s ago      
     Docs: https://docs.docker.com      
 Main PID: 1670 (dockerd)       
   CGroup: /system.slice/docker.service         
           ├─1670 /usr/bin/dockerd        
           └─1677 containerd --config /var/run/docker/containerd/containerd.toml --log-level info       

##### 6)镜像加速
[root@master-1 system]# cd /etc/docker       
[root@master-1 docker]# vim daemon.json                  
{             
"registry-mirrors": ["https://23h04een.mirror.aliyuncs.com"]                    
}           

##### 7)查看版本      
[root@master-1 docker]# docker version      
Client: Docker Engine - Community     
 Version:           19.03.13     
 API version:       1.40         
 Go version:        go1.13.15          
 Git commit:        4484c46d9d             
 Built:             Wed Sep 16 17:03:45 2020     
 OS/Arch:           linux/amd64      
 Experimental:      false       

Server: Docker Engine - Community       
 Engine:
  Version:          19.03.13       
  API version:      1.40 (minimum version 1.12)      
  Go version:       go1.13.15     
  Git commit:       4484c46d9d       
  Built:            Wed Sep 16 17:02:21 2020     
  OS/Arch:          linux/amd64    
  Experimental:     false 
 containerd:     
  Version:          1.4.9                 
  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3       
 runc:       
  Version:          1.0.1       
  GitCommit:        v1.0.1-0-g4144b63      
 docker-init:             
  Version:          0.18.0       
  GitCommit:        fec3683       
[root@master-1 docker]#          

##### 8)测试
[root@master-1 docker]# docker pull hello-world            
Using default tag: latest      
latest: Pulling from library/hello-world      
b8dfde127a29: Pull complete           
Digest: sha256:61bd3cb6014296e214ff4c6407a5a7e7092dfa8eefdbbec539e133e97f63e09f        
Status: Downloaded newer image for hello-world:latest      
docker.io/library/hello-world:latest           
         
[root@master-1 docker]# docker run hello-world         

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/

[root@master-1 docker]#                    
若是出现了上图的内容则说明hello-world运行成功。



#####初始化脚本：

[root@master-1 ~]# cat set


echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts
echo "################/etc/hosts is ok #####################"

systemctl stop firewalld && systemctl disable firewalld
echo "################firewalld is ok #####################"

sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
echo "################selinux is ok #####################"

swapoff -a
echo "################swapoff is ok #####################"

yum -y install ntp;systemctl enable ntpd;systemctl restart ntpd
echo "################ntpd is ok #####################"

echo '
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.d/k8s.conf

sysctl --system

echo "################bridge is ok #####################"

yum install wget net-tools vim -y
cd /etc/yum.repos.d/
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O/etc/yum.repos.d/docker-ce.repo  
yum install -y yum-utils device-mapper-persistent-data lvm2
yum install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 containerd.io 
echo '
{             
"registry-mirrors": ["https://23h04een.mirror.aliyuncs.com"]                    
} 
' >> /etc/docker/daemon.json

systemctl start docker; systemctl enable docker.service
systemctl status docker;

docker version
echo "################docker is ok #####################"



docker pull hello-world
docker run hello-world

echo "################docker running is ok #####################"
[root@master-1 ~]# 

执行：
[root@master-1 ~]# ./set 

#####手动修改：
1、vi /etc/fstab
#####/dev/mapper/centos-swap swap                    swap    defaults        0 0
2、vim /usr/lib/systemd/system/docker.service        
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd

#REBOOT


##### 9)复制虚拟机，到其他两台，修改ip 和 主机名，配置免密     
[root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 
修改IP
注掉UUID=

echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts

hostnamectl set-hostname master-1
hostnamectl set-hostname node-1
hostnamectl set-hostname node-2

   
分别复制key到master   
[root@node-1 ~]# ssh-keygen
[root@node-1 .ssh]# ssh-copy-id root@172.16.201.134  
[root@node-2 .ssh]# ssh-keygen   
[root@node-2 .ssh]# ssh-copy-id root@172.16.201.134            

分别复制key到node-1、node-2    
[root@master-1 .ssh]# ssh-keygen  
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.135
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.136

ssh-copy-id root@172.16.201.134
ssh-copy-id root@172.16.201.135
ssh-copy-id root@172.16.201.136

ssh-copy-id master-1
ssh-copy-id node-1
ssh-copy-id node-2

ssh master-1
ssh node-1
ssh node-2

master：       
[root@master-1 .ssh]# cat authorized_keys            
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0Z5AFJNpJMy/dKI39iGMUOVOMGiczhzqdnRKLX2ikyPrAHpXCdOz+3nhXE4m3V1pGWo9gXZNyUG8b2cqtVHLCLGsq4j1qTB9JqVBsWl0kE137E5UEJ0bs9OoKMofeiBQhKPTbS9uCoTrJbJHLr0DAFEE30EYmMtq6thPkTn6eTAzaMHfVH16b76orOLYQ1SWKYPrFMAAz8uDBQ8ncbslneYJF9K9rVHVzNdLJ5/FUmygI2sKXwVpcH7DwpuNK0wPOrMlp0HWeeVeaBoW73POi3Gd2ON7NUP37/U6veai6p7HbAU7AuteUdQlhdE8sj9kD49aDQCwuv4UkEFzuPRO1 root@node-1         
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5EpSsO0h0nfj3kTAKkF/IW5MnmWBN0W3SAw26lh51K/TVmt9v6nvESpqiD1NkSFpNxqx77JljDI0N6fwZN8V3+FyPKU863fPZT0WU1KlwLUp70yCRXjpyO1QaT/f1sdlPs+Tkrair2vXbav41snbT5/2Sze3QiNUMS/Z6g3RfwpXmmd5epetp3VwwkGdrJGt1LDrPnE+YUx8rjQknv1rZCJISXsejdeQFDA01/CPFQxt1Qhu/uhywS2g+qIZPbxf/vBdm779x2q/ctX+3bjnaPmZdaIXG2JRgD5a//f0Uur/wjVHwrJzgPbo9GIE4dGdP8dW7Ni04Uym4aL9q2Iv5 root@node-2          

##### 10)测试
[root@node-2 .ssh]# ssh root@172.16.201.134         
Last login: Wed Sep 22 11:14:30 2021 from 172.16.201.1       
[root@master-1 ~]# exit      
[root@node-2 .ssh]#       

master：      
[root@master-1 ~]# ssh root@172.16.201.136       
Last login: Wed Sep 22 11:14:35 2021 from 172.16.201.1      
[root@node-2 ~]# exit       
logout
Connection to 172.16.201.136 closed.       
[root@master-1 ~]# ssh root@172.16.201.135      
Last login: Wed Sep 22 11:14:33 2021 from 172.16.201.1           
[root@node-1 ~]# exit       
logout                 
Connection to 172.16.201.135 closed.            






##二、安装 Kubernetes

k8s所有版本的github库：
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#downloads-for-v1222


####1、添加阿里云YUM软件源
[root@master-1 ~]#  cd /etc/yum.repos.d/
[root@master-1 yum.repos.d]# vim kubernetes.repo     
 
[kubernetes]
name = kubernetes
baseurl =  https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled = 1
gpgcheck = 0
repo_gpgcheck = 0
gpgkey =  https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg 
https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg

######注意：gpgcheck = 0，repo_gpgcheck = 0一定关掉，否则报错。


####2、版本查看
[root@master-1 yum.repos.d]# yum repolist

[root@master-1 yum.repos.d]# yum list kubelet --showduplicates |sort -r
 * updates: mirrors.cn99.com
Loading mirror speeds from cached hostfile
Loaded plugins: fastestmirror
kubelet.x86_64                       1.22.3-0                         kubernetes
kubelet.x86_64                       1.22.2-0                         kubernetes
kubelet.x86_64                       1.22.1-0                         kubernetes
kubelet.x86_64                       1.22.0-0                         kubernetes
kubelet.x86_64                       1.21.6-0                         kubernetes
kubelet.x86_64                       1.21.5-0                         kubernetes
kubelet.x86_64                       1.21.4-0                         kubernetes
kubelet.x86_64                       1.21.3-0                         kubernetes
kubelet.x86_64                       1.21.2-0                         kubernetes
kubelet.x86_64                       1.21.1-0                         kubernetes
kubelet.x86_64                       1.21.0-0                         kubernetes
kubelet.x86_64                       1.20.9-0                         kubernetes
kubelet.x86_64                       1.20.8-0                         kubernetes
kubelet.x86_64                       1.20.7-0                         kubernetes
kubelet.x86_64                       1.20.6-0                         kubernetes
kubelet.x86_64                       1.20.5-0                         kubernetes
kubelet.x86_64                       1.20.4-0                         kubernetes
kubelet.x86_64                       1.20.2-0                         kubernetes
kubelet.x86_64                       1.20.12-0                        kubernetes
kubelet.x86_64                       1.20.11-0                        kubernetes
kubelet.x86_64                       1.20.1-0                         kubernetes
kubelet.x86_64                       1.20.10-0                        kubernetes
kubelet.x86_64                       1.20.0-0                         kubernetes
kubelet.x86_64                       1.19.9-0                         kubernetes
kubelet.x86_64                       1.19.8-0                         kubernetes
kubelet.x86_64                       1.19.7-0                         kubernetes
kubelet.x86_64                       1.19.6-0                         kubernetes
kubelet.x86_64                       1.19.5-0                         kubernetes
kubelet.x86_64                       1.19.4-0                         kubernetes
kubelet.x86_64                       1.19.3-0                         kubernetes
kubelet.x86_64                       1.19.2-0                         kubernetes
kubelet.x86_64                       1.19.16-0                        kubernetes
kubelet.x86_64                       1.19.15-0                        kubernetes
kubelet.x86_64                       1.19.14-0                        kubernetes
kubelet.x86_64                       1.19.13-0                        kubernetes
kubelet.x86_64                       1.19.12-0                        kubernetes
kubelet.x86_64                       1.19.11-0                        kubernetes
kubelet.x86_64                       1.19.1-0                         kubernetes
kubelet.x86_64                       1.19.10-0                        kubernetes
kubelet.x86_64                       1.19.0-0                         kubernetes
kubelet.x86_64                       1.18.9-0                         kubernetes
kubelet.x86_64                       1.18.8-0                         kubernetes
kubelet.x86_64                       1.18.6-0                         kubernetes
kubelet.x86_64                       1.18.5-0                         kubernetes
kubelet.x86_64                       1.18.4-1                         kubernetes
kubelet.x86_64                       1.18.4-0                         kubernetes
kubelet.x86_64                       1.18.3-0                         kubernetes
kubelet.x86_64                       1.18.2-0                         kubernetes
kubelet.x86_64                       1.18.20-0                        kubernetes
kubelet.x86_64                       1.18.19-0                        kubernetes
kubelet.x86_64                       1.18.18-0                        kubernetes
kubelet.x86_64                       1.18.17-0                        kubernetes
kubelet.x86_64                       1.18.16-0                        kubernetes
kubelet.x86_64                       1.18.15-0                        kubernetes
kubelet.x86_64                       1.18.14-0                        kubernetes
kubelet.x86_64                       1.18.13-0                        kubernetes
kubelet.x86_64                       1.18.12-0                        kubernetes
kubelet.x86_64                       1.18.1-0                         kubernetes
kubelet.x86_64                       1.18.10-0                        kubernetes
kubelet.x86_64                       1.18.0-0                         kubernetes
kubelet.x86_64                       1.17.9-0                         kubernetes
kubelet.x86_64                       1.17.8-0                         kubernetes
kubelet.x86_64                       1.17.7-1                         kubernetes
kubelet.x86_64                       1.17.7-0                         kubernetes
kubelet.x86_64                       1.17.6-0                         kubernetes
kubelet.x86_64                       1.17.5-0                         kubernetes
kubelet.x86_64                       1.17.4-0                         kubernetes
kubelet.x86_64                       1.17.3-0                         kubernetes
kubelet.x86_64                       1.17.2-0                         kubernetes
kubelet.x86_64                       1.17.17-0                        kubernetes
kubelet.x86_64                       1.17.16-0                        kubernetes
kubelet.x86_64                       1.17.15-0                        kubernetes
kubelet.x86_64                       1.17.14-0                        kubernetes
kubelet.x86_64                       1.17.13-0                        kubernetes
kubelet.x86_64                       1.17.12-0                        kubernetes
kubelet.x86_64                       1.17.11-0                        kubernetes
kubelet.x86_64                       1.17.1-0                         kubernetes
kubelet.x86_64                       1.17.0-0                         kubernetes
kubelet.x86_64                       1.16.9-0                         kubernetes
kubelet.x86_64                       1.16.8-0                         kubernetes
kubelet.x86_64                       1.16.7-0                         kubernetes
kubelet.x86_64                       1.16.6-0                         kubernetes
kubelet.x86_64                       1.16.5-0                         kubernetes
kubelet.x86_64                       1.16.4-0                         kubernetes
kubelet.x86_64                       1.16.3-0                         kubernetes
kubelet.x86_64                       1.16.2-0                         kubernetes
kubelet.x86_64                       1.16.15-0                        kubernetes
kubelet.x86_64                       1.16.14-0                        kubernetes
kubelet.x86_64                       1.16.13-0                        kubernetes
kubelet.x86_64                       1.16.12-0                        kubernetes
kubelet.x86_64                       1.16.11-1                        kubernetes
kubelet.x86_64                       1.16.11-0                        kubernetes
kubelet.x86_64                       1.16.1-0                         kubernetes
kubelet.x86_64                       1.16.10-0                        kubernetes
kubelet.x86_64                       1.16.0-0                         kubernetes
kubelet.x86_64                       1.15.9-0                         kubernetes
kubelet.x86_64                       1.15.8-0                         kubernetes
kubelet.x86_64                       1.15.7-0                         kubernetes
kubelet.x86_64                       1.15.6-0                         kubernetes
kubelet.x86_64                       1.15.5-0                         kubernetes
kubelet.x86_64                       1.15.4-0                         kubernetes
kubelet.x86_64                       1.15.3-0                         kubernetes
kubelet.x86_64                       1.15.2-0                         kubernetes
kubelet.x86_64                       1.15.12-0                        kubernetes
kubelet.x86_64                       1.15.11-0                        kubernetes
kubelet.x86_64                       1.15.1-0                         kubernetes
kubelet.x86_64                       1.15.10-0                        kubernetes
kubelet.x86_64                       1.15.0-0                         kubernetes
kubelet.x86_64                       1.14.9-0                         kubernetes
kubelet.x86_64                       1.14.8-0                         kubernetes
kubelet.x86_64                       1.14.7-0                         kubernetes
kubelet.x86_64                       1.14.6-0                         kubernetes
kubelet.x86_64                       1.14.5-0                         kubernetes
kubelet.x86_64                       1.14.4-0                         kubernetes
kubelet.x86_64                       1.14.3-0                         kubernetes
kubelet.x86_64                       1.14.2-0                         kubernetes
kubelet.x86_64                       1.14.1-0                         kubernetes
kubelet.x86_64                       1.14.10-0                        kubernetes
kubelet.x86_64                       1.14.0-0                         kubernetes
kubelet.x86_64                       1.13.9-0                         kubernetes
kubelet.x86_64                       1.13.8-0                         kubernetes
kubelet.x86_64                       1.13.7-0                         kubernetes
kubelet.x86_64                       1.13.6-0                         kubernetes
kubelet.x86_64                       1.13.5-0                         kubernetes
kubelet.x86_64                       1.13.4-0                         kubernetes
kubelet.x86_64                       1.13.3-0                         kubernetes
kubelet.x86_64                       1.13.2-0                         kubernetes
kubelet.x86_64                       1.13.12-0                        kubernetes
kubelet.x86_64                       1.13.11-0                        kubernetes
kubelet.x86_64                       1.13.1-0                         kubernetes
kubelet.x86_64                       1.13.10-0                        kubernetes
kubelet.x86_64                       1.13.0-0                         kubernetes
kubelet.x86_64                       1.12.9-0                         kubernetes
kubelet.x86_64                       1.12.8-0                         kubernetes
kubelet.x86_64                       1.12.7-0                         kubernetes
kubelet.x86_64                       1.12.6-0                         kubernetes
kubelet.x86_64                       1.12.5-0                         kubernetes
kubelet.x86_64                       1.12.4-0                         kubernetes
kubelet.x86_64                       1.12.3-0                         kubernetes
kubelet.x86_64                       1.12.2-0                         kubernetes
kubelet.x86_64                       1.12.1-0                         kubernetes
kubelet.x86_64                       1.12.10-0                        kubernetes
kubelet.x86_64                       1.12.0-0                         kubernetes
kubelet.x86_64                       1.11.9-0                         kubernetes
kubelet.x86_64                       1.11.8-0                         kubernetes
kubelet.x86_64                       1.11.7-0                         kubernetes
kubelet.x86_64                       1.11.6-0                         kubernetes
kubelet.x86_64                       1.11.5-0                         kubernetes
kubelet.x86_64                       1.11.4-0                         kubernetes
kubelet.x86_64                       1.11.3-0                         kubernetes
kubelet.x86_64                       1.11.2-0                         kubernetes
kubelet.x86_64                       1.11.1-0                         kubernetes
kubelet.x86_64                       1.11.10-0                        kubernetes
kubelet.x86_64                       1.11.0-0                         kubernetes
kubelet.x86_64                       1.10.9-0                         kubernetes
kubelet.x86_64                       1.10.8-0                         kubernetes
kubelet.x86_64                       1.10.7-0                         kubernetes
kubelet.x86_64                       1.10.6-0                         kubernetes
kubelet.x86_64                       1.10.5-0                         kubernetes
kubelet.x86_64                       1.10.4-0                         kubernetes
kubelet.x86_64                       1.10.3-0                         kubernetes
kubelet.x86_64                       1.10.2-0                         kubernetes
kubelet.x86_64                       1.10.13-0                        kubernetes
kubelet.x86_64                       1.10.12-0                        kubernetes
kubelet.x86_64                       1.10.11-0                        kubernetes
kubelet.x86_64                       1.10.1-0                         kubernetes
kubelet.x86_64                       1.10.10-0                        kubernetes
kubelet.x86_64                       1.10.0-0                         kubernetes
 * extras: mirrors.cn99.com
 * base: mirrors.163.com
Available Packages
[root@master-1 yum.repos.d]# 

本文安装的kubelet版本是1.16.4，该版本支持的docker最高版本是1.22.2


####3、安装kubelet、 kubeadm、 kubectl
[root@master-1 rpm-gpg]# yum install  kubelet-1.19.9-0 kubeadm-1.19.9-0 kubectl-1.19.9-0 --nogpgcheck -y
Installed:
  kubeadm.x86_64 0:1.19.9-0                           kubectl.x86_64 0:1.19.9-0                           kubelet.x86_64 0:1.19.9-0                          
Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-7.el7                 cri-tools.x86_64 0:1.13.0-0                          kubernetes-cni.x86_64 0:0.8.7-0                   
  libnetfilter_cthelper.x86_64 0:1.0.0-11.el7          libnetfilter_cttimeout.x86_64 0:1.0.0-7.el7          libnetfilter_queue.x86_64 0:1.0.2-2.el7_2         
  socat.x86_64 0:1.7.3.2-2.el7                        

Complete!


######安装包说明
kubelet 运行在集群所有节点上，用于启动Pod和容器等对象的工具
kubeadm 用于初始化集群，启动集群的命令工具
kubectl 用于和集群通信的命令行，通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件


######报错： 
Failing package is: kubectl-1.16.2-0.x86_64
GPG Keys are configured as: https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg

解：用--nogpgcheck跳过key校验


######此处不可以启动kubelet，由于还有fannel等插件未安装完成，启动会报失败
[root@master-1 ~]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@node-1 yum.repos.d]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@node-2 yum.repos.d]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.


[root@master-1 yum.repos.d]# systemctl start kubelet       
[root@master-1 yum.repos.d]# systemctl status kubelet


#备注：以上操作所有的节点都要操作。
#如果以下步操作失败，可以通过 kubeadm reset 命令来清理环境重新安装。


####4、部署Kubernetes Master
######1)master：初始化kubeadm
[root@master-1 ~]# kubeadm init \
--apiserver-advertise-address=172.16.201.134 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.19.9 \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.244.0.0/16

######参数说明：
–image-repository string：    这个用于指定从什么位置来拉取镜像（1.13版本才有的），默认值是k8s.gcr.io，我们将其指定为国内镜像地址：registry.aliyuncs.com/google_containers
–kubernetes-version string： 指定kubenets版本号，默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（v1.15.1）来跳过网络请求。
–apiserver-advertise-address  指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。
–pod-network-cidr      指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 –pod-network-cidr有自己的要求，这里设置为10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。


[root@master-1 .ssh]# kubeadm init \
> --apiserver-advertise-address=172.16.201.134 \
> --image-repository registry.aliyuncs.com/google_containers \
> --kubernetes-version v1.19.9 \
> --service-cidr=10.1.0.0/16 \
> --pod-network-cidr=10.244.0.0/16
W1029 14:38:51.937827    2491 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.19.9
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master-1] and IPs [10.1.0.1 172.16.201.134]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master-1] and IPs [172.16.201.134 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master-1] and IPs [172.16.201.134 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 68.505356 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.19" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master-1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 5hc28a.77wkxjobg7zkk1ag
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.201.134:6443 --token j3wis7.tg9szxlukb8ywtgv \
    --discovery-token-ca-cert-hash sha256:e1563969d45a83c8befb63037ce439d016fad27dbef0aae182507d2a121925c3
[root@master-1 .ssh]# 

######注意：
建议至少2 cpu ,2G，非硬性要求，1cpu，1G也可以搭建起集群。
1个cpu的话初始化master的时候会报 [WARNING NumCPU]: the number of available CPUs 1 is less than the required 2
部署插件或者pod时可能会报warning：FailedScheduling：Insufficient cpu, Insufficient memory
如果出现这种提示，说明你的虚拟机分配的CPU为1核，需要重新设置虚拟机master节点内核数。


######问题：
[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/

######解决
更改docker的启动参数
$ vim /usr/lib/systemd/system/docker.service
#####ExecStart=/usr/bin/dockerd
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
重启docker
[root@master-1 ~]# systemctl daemon-reload
[root@master-1 ~]# systemctl restart docker



######2)node节点执行加入集群：
[root@node-1 ~]# kubeadm join 172.16.201.134:6443 --token j3wis7.tg9szxlukb8ywtgv \
    --discovery-token-ca-cert-hash sha256:e1563969d45a83c8befb63037ce439d016fad27dbef0aae182507d2a121925c3

[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

######3)node查看状态：
[root@node-1 yum.repos.d]# docker ps -a
CONTAINER ID        IMAGE                                                COMMAND                  CREATED             STATUS                         PORTS               NAMES
4e8fd3436209        registry.aliyuncs.com/google_containers/kube-proxy   "/usr/local/bin/kube…"   2 minutes ago       Up 2 minutes                                       k8s_kube-proxy_kube-proxy-bkck2_kube-system_13d1d3e9-de92-4601-9486-cb9fae99f600_0
251063bf8160        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 2 minutes ago       Up 2 minutes                                       k8s_POD_kube-proxy-bkck2_kube-system_13d1d3e9-de92-4601-9486-cb9fae99f600_0
[root@node-1 yum.repos.d]# 

[root@node-2 yum.repos.d]# docker ps -a
CONTAINER ID        IMAGE                                                COMMAND                  CREATED             STATUS                         PORTS               NAMES
529215b26f16        registry.aliyuncs.com/google_containers/kube-proxy   "/usr/local/bin/kube…"   2 minutes ago       Up 2 minutes                                       k8s_kube-proxy_kube-proxy-phjdh_kube-system_3f1d0d74-2dcd-4d48-82f8-37b2a5558a0c_0
2f5f9eaf2f56        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 2 minutes ago       Up 2 minutes                                       k8s_POD_kube-proxy-phjdh_kube-system_3f1d0d74-2dcd-4d48-82f8-37b2a5558a0c_0



[root@node-1 yum.repos.d]#  docker images
REPOSITORY                                           TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy   v1.19.9             4a76fb49d490        6 months ago        118MB
hello-world                                          latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/pause        3.2                 80d28bedfe5d        19 months ago       683kB
[root@node-1 yum.repos.d]# 


[root@node-2 yum.repos.d]# docker images
REPOSITORY                                           TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy   v1.19.9             4a76fb49d490        6 months ago        118MB
hello-world                                          latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/pause        3.2                 80d28bedfe5d        19 months ago       683kB

######4) master查看状态：
[root@master-1 kubernetes]# docker images
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy                v1.19.9             4a76fb49d490        6 months ago        118MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.19.9             1a4f1f05177f        6 months ago        119MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.19.9             a8fd6520f73d        6 months ago        111MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.19.9             8f1e66e40394        6 months ago        46.5MB
hello-world                                                       latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/etcd                      3.4.13-0            0369cf4303ff        13 months ago       253MB
registry.aliyuncs.com/google_containers/coredns                   1.7.0               bfe3a36ebd25        15 months ago       45.2MB
registry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        19 months ago       683kB
[root@master-1 kubernetes]# 



######5) master:使用kubectl工具
[root@master-1 ~]#  kubectl get nodes
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")


[root@master-1 ~]# mkdir -p $HOME/.kube
[root@master-1 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
cp: overwrite ‘/root/.kube/config’? y
[root@master-1 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@master-1 ~]#  kubectl get nodes                                      
NAME       STATUS     ROLES    AGE     VERSION
master-1   NotReady   master   2m54s   v1.19.9
node-1     NotReady   <none>   2m9s    v1.19.9
node-2     NotReady   <none>   97s     v1.19.9
重新创建集群时，这个目录还是存在的，于是我尝试在执行这几个命令前先执行rm -rf $HOME/.kube命令删除这个目录


######6) master:安装Pod网络插件（CNI）(master)
master:目前是NotReady
[root@master-1 kubernetes]#  kubectl get nodes
NAME       STATUS     ROLES    AGE   VERSION
master-1   NotReady   master   30m   v1.19.9
node-1     NotReady   <none>   13m   v1.19.9
node-2     NotReady   <none>   13m   v1.19.9


[root@master-1 ~]#wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

[root@master-1 ~]# cat kube-flannel.yml
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
  - configMap
  - secret
  - emptyDir
  - hostPath
  allowedHostPaths:
  - pathPrefix: "/etc/cni/net.d"
  - pathPrefix: "/etc/kube-flannel"
  - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN', 'NET_RAW']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unused in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs: ['use']
  resourceNames: ['psp.flannel.unprivileged']
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.14.0
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.14.0
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
[root@master-1 ~]# 

######7)master:安装 ，执行命令：
[root@master-1 ~]#kubectl apply -f kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created


######8)master:查看安装状态： 
[root@master-1 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS            RESTARTS   AGE
coredns-6d56c8448f-mp5lz           0/1     Pending           0          38m
coredns-6d56c8448f-wnqxj           0/1     Pending           0          38m
etcd-master-1                      1/1     Running           0          38m
kube-apiserver-master-1            1/1     Running           0          38m
kube-controller-manager-master-1   1/1     Running           0          38m
kube-flannel-ds-mmhsm              1/1     Running           0          37s
kube-flannel-ds-rz2xj              0/1     PodInitializing   0          37s
kube-flannel-ds-ts9fm              1/1     Running           0          37s
kube-proxy-bkck2                   1/1     Running           0          22m
kube-proxy-c6fdx                   1/1     Running           0          38m
kube-proxy-phjdh                   1/1     Running           0          21m
kube-scheduler-master-1            1/1     Running           0          38m

######master:安装好了，kube-flannel都Running了
[root@master-1 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-6d56c8448f-mp5lz           1/1     Running   0          75m
coredns-6d56c8448f-wnqxj           1/1     Running   0          75m
etcd-master-1                      1/1     Running   0          38m
kube-apiserver-master-1            1/1     Running   0          38m
kube-controller-manager-master-1   1/1     Running   0          38m
kube-flannel-ds-mmhsm              1/1     Running   0          38s
kube-flannel-ds-rz2xj              1/1     Running   0          38s
kube-flannel-ds-ts9fm              1/1     Running   0          38s
kube-proxy-bkck2                   1/1     Running   0          22m
kube-proxy-c6fdx                   1/1     Running   0          38m
kube-proxy-phjdh                   1/1     Running   0          21m
kube-scheduler-master-1            1/1     Running   0          38m



######master:再次查看node，可以看到状态为ready
[root@master-1 ~]# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
master-1   Ready    master   39m   v1.19.9
node-1     Ready    <none>   22m   v1.19.9
node-2     Ready    <none>   22m   v1.19.9

######开机器启动配置：
systemctl enable kubelet      
systemctl status kubelet
systemctl restart kubelet



######master:
[root@master-1 ~]# docker images
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
quay.io/coreos/flannel                                            v0.14.0             8522d622299c        4 months ago        67.9MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.19.9             4a76fb49d490        6 months ago        118MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.19.9             1a4f1f05177f        6 months ago        119MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.19.9             8f1e66e40394        6 months ago        46.5MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.19.9             a8fd6520f73d        6 months ago        111MB
hello-world                                                       latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/etcd                      3.4.13-0            0369cf4303ff        13 months ago       253MB
registry.aliyuncs.com/google_containers/coredns                   1.7.0               bfe3a36ebd25        15 months ago       45.2MB
registry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        19 months ago       683kB

######master:
[root@master-1 ~]# docker ps -a
CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS                     PORTS               NAMES
992baf6abefa        8522d622299c                                        "/opt/bin/flanneld -…"   2 minutes ago       Up 2 minutes                                   k8s_kube-flannel_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
78be74ae6cad        quay.io/coreos/flannel                              "cp -f /etc/kube-fla…"   2 minutes ago       Exited (0) 2 minutes ago                       k8s_install-cni_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
e30ad41e86d3        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 3 minutes ago       Up 3 minutes                                   k8s_POD_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
72df4a6ccc2a        4a76fb49d490                                        "/usr/local/bin/kube…"   40 minutes ago      Up 40 minutes                                  k8s_kube-proxy_kube-proxy-c6fdx_kube-system_89fdfc40-c7c8-4152-91ac-94a45f83cb1d_0
ee455bc6ca60        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-proxy-c6fdx_kube-system_89fdfc40-c7c8-4152-91ac-94a45f83cb1d_0
802350660d95        8f1e66e40394                                        "kube-scheduler --au…"   40 minutes ago      Up 40 minutes                                  k8s_kube-scheduler_kube-scheduler-master-1_kube-system_0b7a3f4a1ea56044c3ad536d56a87725_0
ee4008c4f1d5        a8fd6520f73d                                        "kube-controller-man…"   40 minutes ago      Up 40 minutes                                  k8s_kube-controller-manager_kube-controller-manager-master-1_kube-system_6f065f021a2517457fa9368ab63b44f2_0
effb22b25ffa        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-scheduler-master-1_kube-system_0b7a3f4a1ea56044c3ad536d56a87725_0
60930447b83b        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-controller-manager-master-1_kube-system_6f065f021a2517457fa9368ab63b44f2_0
d85b5859f2bb        1a4f1f05177f                                        "kube-apiserver --ad…"   41 minutes ago      Up 41 minutes                                  k8s_kube-apiserver_kube-apiserver-master-1_kube-system_7fb363525d550d682b33d84cb74dda9d_0
61ee22184e58        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 41 minutes ago      Up 41 minutes                                  k8s_POD_kube-apiserver-master-1_kube-system_7fb363525d550d682b33d84cb74dda9d_0
d0fc20c3ba74        0369cf4303ff                                        "etcd --advertise-cl…"   41 minutes ago      Up 41 minutes                                  k8s_etcd_etcd-master-1_kube-system_d9c8fc0f468e5e6013762f18eeab5cda_0
fdc7cad113c4        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 41 minutes ago      Up 41 minutes                                  k8s_POD_etcd-master-1_kube-system_d9c8fc0f468e5e6013762f18eeab5cda_0


######master:
[root@master-1 ~]# kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
cdeix4.h84buwepsp1yx14v   23h         2021-09-23T12:04:22+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token



######master:
[root@master-1 ~]# kubectl get node -o wide
NAME       STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
master-1   Ready    master   42m   v1.19.9   172.16.201.134   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-1     Ready    <none>   26m   v1.19.9   172.16.201.135   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-2     Ready    <none>   25m   v1.19.9   172.16.201.136   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13




#####master:问题：健康检查错误：
[root@master-1 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"}    

解：
cd /etc/kubernetes/manifests/
[root@master-1 manifests]# vim kube-controller-manager.yaml
[root@master-1 manifests]# vim kube-scheduler.yaml 
2个文件的    - --port=0 注释掉后等一会



[root@master-1 manifests]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   

[root@master-1 manifests]# kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   


[root@master-1 manifests]# kubectl get ns
NAME              STATUS   AGE
default           Active   6m39s
kube-node-lease   Active   6m42s
kube-public       Active   6m42s
kube-system       Active   6m42s

[root@master-1 manifests]# kubectl get no
NAME       STATUS   ROLES    AGE     VERSION
master-1   Ready    master   7m23s   v1.19.9
node-1     Ready    <none>   6m38s   v1.19.9
node-2     Ready    <none>   6m6s    v1.19.9

[root@master-1 manifests]#  kubectl get pod -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d56c8448f-7c4dp           1/1     Running   0          7m28s
kube-system   coredns-6d56c8448f-fkwql           1/1     Running   0          7m28s
kube-system   etcd-master-1                      1/1     Running   0          7m47s
kube-system   kube-apiserver-master-1            1/1     Running   0          7m46s
kube-system   kube-controller-manager-master-1   1/1     Running   0          2m3s
kube-system   kube-flannel-ds-cvmdw              1/1     Running   0          3m48s
kube-system   kube-flannel-ds-gcckr              1/1     Running   0          3m48s
kube-system   kube-flannel-ds-nkl2p              1/1     Running   0          3m48s
kube-system   kube-proxy-fkj8w                   1/1     Running   0          6m31s
kube-system   kube-proxy-glbfj                   1/1     Running   0          7m28s
kube-system   kube-proxy-pxfg7                   1/1     Running   0          7m3s
kube-system   kube-scheduler-master-1            1/1     Running   0          103s


[root@master-1 manifests]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   8m


[root@master-1 ~]# kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         104m


[root@master-1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.201.134:6443
KubeDNS is running at https://172.16.201.134:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

####5、 测试kubernetes集群
######1、安装nginx来测试


[root@master-1 ~]# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
[root@master-1 ~]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-lkmk7   1/1     Running   0          3m53s   10.244.2.5   node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h50m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   2m28s   app=nginx

####得到service端口转换后的端口为：80:30698


######访问效果测试：
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.134:30698/
200
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.135:30698/
200
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.136:30698/
200


######2、测试扩容情况（扩容到3个副本）
[root@master-1 ~]# kubectl scale deployment nginx --replicas=3
deployment.apps/nginx scaled
[root@master-1 ~]# 


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS              RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-c25s6   0/1     ContainerCreating   0          16s     <none>       node-1   <none>           <none>
pod/nginx-6799fc88d8-lkmk7   1/1     Running             0          9m34s   10.244.2.5   node-2   <none>           <none>
pod/nginx-6799fc88d8-xwjqr   0/1     ContainerCreating   0          16s     <none>       node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h56m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   8m9s    app=nginx


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-c25s6   1/1     Running   0          50s   10.244.1.4   node-1   <none>           <none>
pod/nginx-6799fc88d8-lkmk7   1/1     Running   0          10m   10.244.2.5   node-2   <none>           <none>
pod/nginx-6799fc88d8-xwjqr   1/1     Running   0          50s   10.244.2.6   node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h56m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   8m43s   app=nginx



[root@master-1 manifests]#  kubectl get pods  --all-namespaces -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
kube-system   coredns-6d56c8448f-7c4dp           1/1     Running   0          31m   10.244.0.2       master-1   <none>           <none>
kube-system   coredns-6d56c8448f-fkwql           1/1     Running   0          31m   10.244.2.2       node-2     <none>           <none>
kube-system   etcd-master-1                      1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-apiserver-master-1            1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-controller-manager-master-1   1/1     Running   0          26m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-flannel-ds-cvmdw              1/1     Running   0          27m   172.16.201.135   node-1     <none>           <none>
kube-system   kube-flannel-ds-gcckr              1/1     Running   0          27m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-flannel-ds-nkl2p              1/1     Running   0          27m   172.16.201.136   node-2     <none>           <none>
kube-system   kube-proxy-fkj8w                   1/1     Running   0          30m   172.16.201.136   node-2     <none>           <none>
kube-system   kube-proxy-glbfj                   1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-proxy-pxfg7                   1/1     Running   0          31m   172.16.201.135   node-1     <none>           <none>
kube-system   kube-scheduler-master-1            1/1     Running   0          25m   172.16.201.134   master-1   <none>           <none>



[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide        
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
default                nginx-6799fc88d8-c25s6                       1/1     Running   0          11m     10.244.1.4       node-1     <none>           <none>
default                nginx-6799fc88d8-lkmk7                       1/1     Running   0          20m     10.244.2.5       node-2     <none>           <none>
default                nginx-6799fc88d8-xwjqr                       1/1     Running   0          11m     10.244.2.6       node-2     <none>           <none>
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          7h6m    10.244.1.2       node-1     <none>           <none>
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          7h6m    10.244.2.2       node-2     <none>           <none>
kube-system            etcd-master-1                                1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-apiserver-master-1                      1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-controller-manager-master-1             1/1     Running   1          6h21m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          6h29m   172.16.201.135   node-1     <none>           <none>
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          6h29m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          6h29m   172.16.201.136   node-2     <none>           <none>
kube-system            kube-proxy-bkck2                             1/1     Running   0          6h50m   172.16.201.135   node-1     <none>           <none>
kube-system            kube-proxy-c6fdx                             1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-proxy-phjdh                             1/1     Running   0          6h50m   172.16.201.136   node-2     <none>           <none>
kube-system            kube-scheduler-master-1                      1/1     Running   1          6h21m   172.16.201.134   master-1   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          5h1m    10.244.1.3       node-1     <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          5h1m    10.244.2.4       node-2     <none>           <none>







####查看顺序：
######查看 namespace信息：
[root@master-1 ~]# kubectl get namespace
NAME                   STATUS   AGE
default                Active   7h23m
kube-node-lease        Active   7h23m
kube-public            Active   7h23m
kube-system            Active   7h23m
kubernetes-dashboard   Active   5h17m

######查看namespace的service信息：
[root@master-1 ~]# kubectl get service -o wide --namespace=default
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        7h22m   <none>
nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   34m     app=nginx

另一种写法：
[root@master-1 ~]# kubectl get service/nginx
NAME    TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
nginx   NodePort   10.1.198.44   <none>        80:30698/TCP   43m


######查看svc nginx日志
[root@master-1 ~]# kubectl describe svc nginx
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.1.198.44
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30698/TCP
Endpoints:                10.244.1.4:80,10.244.2.5:80,10.244.2.6:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
[root@master-1 ~]# 

######查看pod name
[root@master-1 ~]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-c25s6   1/1     Running   0          24m   10.244.1.4   node-1   <none>           <none>
nginx-6799fc88d8-lkmk7   1/1     Running   0          34m   10.244.2.5   node-2   <none>           <none>
nginx-6799fc88d8-xwjqr   1/1     Running   0          24m   10.244.2.6   node-2   <none>           <none>
[root@master-1 ~]# 
[root@master-1 ~]# 
[root@master-1 ~]# 

######查看pod nginx-6799fc88d8-c25s6 日志：
[root@master-1 ~]# kubectl describe pod nginx-6799fc88d8-c25s6
Name:         nginx-6799fc88d8-c25s6
Namespace:    default
Priority:     0
Node:         node-1/172.16.201.135
Start Time:   Wed, 22 Sep 2021 19:00:21 +0800
Labels:       app=nginx
              pod-template-hash=6799fc88d8
Annotations:  <none>
Status:       Running
IP:           10.244.1.4
IPs:
  IP:           10.244.1.4
Controlled By:  ReplicaSet/nginx-6799fc88d8
Containers:
  nginx:
    Container ID:   docker://5cf5621c44344d802efb5d3c565aeaf98ce7f67732c009a10d798bf9919737d4
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:853b221d3341add7aaadf5f81dd088ea943ab9c918766e295321294b035f3f3e
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 22 Sep 2021 19:00:55 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-b4d9n (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-b4d9n:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-b4d9n
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  25m   default-scheduler  Successfully assigned default/nginx-6799fc88d8-c25s6 to node-1
  Normal  Pulling    25m   kubelet            Pulling image "nginx"
  Normal  Pulled     24m   kubelet            Successfully pulled image "nginx" in 32.403763647s
  Normal  Created    24m   kubelet            Created container nginx
  Normal  Started    24m   kubelet            Started container nginx



docker exec -it 5cf5621c4434 /bin/bash

######删除nginx：
[root@master-1 ~]# kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   3/3     3            3           21h
[root@master-1 ~]# kubectl delete deployment nginx 
deployment.apps "nginx" deleted

[root@master-1 ~]# kubectl get deployment
No resources found in default namespace.

[root@master-1 ~]# kubectl delete service nginx
service "nginx" deleted

[root@master-1 ~]#  kubectl get pod,svc -o wide        
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   28h   <none>




###出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点
######查看k8s-master表示不运行pod
[root@master-1 nacos-k8s]# kubectl describe node k8s-master |grep Taints
Taints: node-role.kubernetes.io/master:NoSchedule
######查看k8s-master表示运行pod
[root@master-1 nacos-k8s]# kubectl describe node master-1 |grep Taints                 
Taints:             <none>

######让 master节点参与POD负载的命令为
[root@master-1 nacos-k8s]# kubectl taint nodes master-1 node-role.kubernetes.io/master-
node/master-1 untainted
######让 master节点恢复不参与POD负载的命令为 
[root@master-1 nacos-k8s]# kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule





##三、搭建K8S Dashboard
1. 下载dashboard文件：
[root@master-1 ~]#  wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml
[root@master-1 ~]# 


[root@master-1 ~]# cat recommended.yaml


apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001
  type: NodePort
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
type: Opaque

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kubernetes-dashboard
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: kubernetes-dashboard
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: kubernetes-dashboard

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.2.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            # Uncomment the following line to manually specify Kubernetes API server Host
            # If not specified, Dashboard will attempt to auto discover the API server and connect
            # to it. Uncomment only if the default does not work.
            # - --apiserver-host=http://my-address:port
          volumeMounts:
            - name: kubernetes-dashboard-certs
              mountPath: /certs
              # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.6
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
[root@master-1 ~]# 



2.下载镜像镜像【k8s.gcr.io#kubernetes-dashboard-amd64.tar】
链接: https://pan.baidu.com/s/1vQUGpx89TZw-HyxCVE972w 提取码: 5pui 复制这段内容后打开百度网盘手机App，操作更方便哦


3. 创建kubernetes-dashboard：注意：修改【recommended.yaml】文件里面的镜像地址
kubectl create -f  recommended.yaml



4. 卸载之前安装的内容：
kubectl delete -f  recommended.yaml


6、修改访问端口
[root@master-1 ~]# sed -i '/targetPort: 8443/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort' recommended.yaml​ 
 访问nodePort: 30001 修改成30001


7. 重新安装最新版本dashboard：
kubectl create -f recommended.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml


[root@master-1 ~]# kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          126m
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          126m
kube-system            etcd-master-1                                1/1     Running   1          126m
kube-system            kube-apiserver-master-1                      1/1     Running   1          126m
kube-system            kube-controller-manager-master-1             1/1     Running   1          81m
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          89m
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          89m
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          89m
kube-system            kube-proxy-bkck2                             1/1     Running   0          110m
kube-system            kube-proxy-c6fdx                             1/1     Running   1          126m
kube-system            kube-proxy-phjdh                             1/1     Running   0          110m
kube-system            kube-scheduler-master-1                      1/1     Running   1          81m
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          72s
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          72s


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   135m   <none>


kubernetes-dashboard直接访问30001端口即可：
[root@master-1 ~]# kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          6m12s
pod/kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          6m12s

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
service/dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP        6m12s
service/kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP   6m12s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           6m12s
deployment.apps/kubernetes-dashboard        1/1     1            1           6m12s

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-79c5968bdc   1         1         1       6m12s
replicaset.apps/kubernetes-dashboard-9f9799597         1         1         1       6m12s
[root@master-1 ~]# 


[root@master-1 ~]#  kubectl get services -o wide --all-namespaces
NAMESPACE              NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE    SELECTOR
default                kubernetes                  ClusterIP   10.1.0.1       <none>        443/TCP                  156m   <none>
kube-system            kube-dns                    ClusterIP   10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   156m   k8s-app=kube-dns
kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP                 30m    k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP            30m    k8s-app=kubernetes-dashboard



8. 获取token：

[root@master-1 ~]# kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
Name:         namespace-controller-token-rzwtv
Type:  kubernetes.io/service-account-token
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImgzZVplbFI3YUx0R0pwX1lrNTlQQ3pNWTdHcmJmUkMxb1h3VHNtd0NGbVkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1yend0diIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjlhYTM3YTNkLTE4MTEtNDRhMS05ZDUxLThkMzBkZjQzNDdlNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.zl8IraeIK3zIYhzyYl3lTWCqGfYgW_O2gnOAEKi44XQhvpFFoOaxxkS7wqjXWaenfCUZPHJtNUWYTH804Ku2y_Om6g6fJNv8C8Aj2dyYpgnaifLcIEVi3pGWLmV2eG78rf1sXbObIylSrshoTWGwV-CVXRWNvZGAMFs7uV4tY8tddA7UZi8QGjBP33Ea0s7rbnPUOQmWhe82szWqQZz5MTg9mFHzFsyJSfl2tzZVzGL7fp4Df5Ce3PSmoGGirUmL3pjnfSqbASMYD9xbGXHsDJC1N4h1OiAG6HxwnuD8DrS4hNPgWiAWnSysf1mKni9UiAL54xN6UA-aSjvxNX7N1Q
[root@master-1 ~]# 



9.如果谷歌浏览器访问不了，则用火狐访问：
http://nodeIp:nodePort
https://172.16.201.136:30001/#/workloads?namespace=default
接受风险，输入令牌，即可图形化管理k8s集群


查看服务被分配到哪个节点上：
[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide                 
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          3h16m   10.244.1.2       node-1     <none>           <none>
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          3h16m   10.244.2.2       node-2     <none>           <none>
kube-system            etcd-master-1                                1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-apiserver-master-1                      1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-controller-manager-master-1             1/1     Running   1          151m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          159m    172.16.201.135   node-1     <none>           <none>
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          159m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          159m    172.16.201.136   node-2     <none>           <none>
kube-system            kube-proxy-bkck2                             1/1     Running   0          3h      172.16.201.135   node-1     <none>           <none>
kube-system            kube-proxy-c6fdx                             1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-proxy-phjdh                             1/1     Running   0          3h      172.16.201.136   node-2     <none>           <none>
kube-system            kube-scheduler-master-1                      1/1     Running   1          151m    172.16.201.134   master-1   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          71m     10.244.1.3       node-1     <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          71m     10.244.2.4       node-2     <none>           <none>



10、清理测试环境
[root@master-1 ~]# kubectl delete -f recommended.yaml
namespace "kubernetes-dashboard" deleted
serviceaccount "kubernetes-dashboard" deleted
service "kubernetes-dashboard" deleted
secret "kubernetes-dashboard-certs" deleted
secret "kubernetes-dashboard-csrf" deleted
secret "kubernetes-dashboard-key-holder" deleted
configmap "kubernetes-dashboard-settings" deleted
role.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
clusterrole.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
rolebinding.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
clusterrolebinding.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
deployment.apps "kubernetes-dashboard" deleted
service "dashboard-metrics-scraper" deleted
deployment.apps "dashboard-metrics-scraper" deleted




##四、管理K8S

######查看合并后的kubeconfig设置，或者一个指定的kubeconfig配置文件
[root@master-1 ~]# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.16.201.134:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED



######查看集群信息
[root@master-1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.201.134:6443
KubeDNS is running at https://172.16.201.134:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.


######列出 Pod 中的容器
可以使用 range 操作进一步控制格式化，以单独操作每个元素。
[root@master-1 ~]# kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' |\
> sort
coredns-6d56c8448f-mp5lz:       registry.aliyuncs.com/google_containers/coredns:1.7.0, 
coredns-6d56c8448f-wnqxj:       registry.aliyuncs.com/google_containers/coredns:1.7.0, 
dashboard-metrics-scraper-79c5968bdc-bcm4d:     kubernetesui/metrics-scraper:v1.0.6, 
etcd-master-1:  registry.aliyuncs.com/google_containers/etcd:3.4.13-0, 
kube-apiserver-master-1:        registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9, 
kube-controller-manager-master-1:       registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9, 
kube-flannel-ds-mmhsm:  quay.io/coreos/flannel:v0.14.0, 
kube-flannel-ds-rz2xj:  quay.io/coreos/flannel:v0.14.0, 
kube-flannel-ds-ts9fm:  quay.io/coreos/flannel:v0.14.0, 
kube-proxy-bkck2:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kube-proxy-c6fdx:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kube-proxy-phjdh:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kubernetes-dashboard-9f9799597-529zk:   kubernetesui/dashboard:v2.2.0, 
kube-scheduler-master-1:        registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9, 

######列出以标签过滤后的 Pod 的所有容器
要获取匹配特定标签的 Pod，请使用 -l 参数。以下匹配仅与标签 app=nginx 相符的 Pod。
kubectl get pods --all-namespaces -o=jsonpath="{.items[*].spec.containers[*].image}" -l app=nginx

######列出以命名空间过滤后的 Pod 的所有容器
要获取匹配特定命名空间的 Pod，请使用 namespace 参数。以下仅匹配 kube-system 命名空间下的 Pod。
[root@master-1 ~]# kubectl get pods --namespace kube-system -o jsonpath="{.items[*].spec.containers[*].image}"
registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9[root@master-1 ~]# 

######使用 go-template 代替 jsonpath 来获取容器
作为 jsonpath 的替代，Kubectl 支持使用 go-templates 来格式化输出
[root@master-1 ~]# kubectl get pods --all-namespaces -o go-template --template="{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"
registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9 kubernetesui/metrics-scraper:v1.0.6 kubernetesui/dashboard:v2.2.0 [root@master-1 ~]# 


#####注意：重新初始化得先执行这几步
docker rm -f `docker ps -a -q`
rm -rf /etc/kubernetes/*
rm -rf /var/lib/etcd/
kubeadm reset




#####移除NODE节点的方法（master执行）
1：先将节点设置为维护模式(k8s-node1是节点名称)

[root@k8s-master ~]# kubectl drain k8s-node1 --delete-local-data --force --ignore-daemonsets
node/k8s-node1 cordoned
node/k8s-node1 drained
2：然后删除节点

[root@k8s-master ~]# kubectl delete node k8s-node1
node "k8s-node1" drained
3：查看节点

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE    VERSION
k8s-master   Ready    master   18m    v1.17.0
k8s-node2    Ready    <none>   5m7s   v1.17.0
发现k8s-node1节点已经被删除了

####如果这个时候再想添加进来这个node，需要执行两步操作
1：停掉kubelet(需要添加进来的节点操作)

[root@k8s-node2 ~]# systemctl stop kubelet
2：删除相关文件

[root@k8s-node2 ~]# rm -rf /etc/kubernetes/*
3：添加节点

kubeadm join 192.168.182.135:6443 --token 7rpjfp.n3vg39zrgstzr0rs \
--discovery-token-ca-cert-hash sha256:8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234
4：查看节点

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   24m   v1.17.0
k8s-node1    NotReady   <none>   6s    v1.17.0
k8s-node2    Ready      <none>   10m   v1.17.0





####忘掉token再次添加进k8s集群
1：主节点执行命令

获取token
[root@k8s-master ~]# kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
7rpjfp.n3vg39zrgstzr0rs   23h         2019-12-30T20:01:50+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token

2： 获取ca证书sha256编码hash值
[root@k8s-master ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234

3：从节点执行如下的命令
[root@k8s-master ~]# systemctl stop kubelet

4：删除相关文件
[root@k8s-master ~]# rm -rf /etc/kubernetes/*

5：加入集群
指定主节点IP，端口是6443
在生成的证书前有sha256:
kubeadm join 192.168.64.10:6443 --token 7rpjfp.n3vg39zrgstzr0rs  --discovery-token-ca-cert-hash sha256:8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234


####常用命令
1、查看node
#######o wide以yaml格式显示详细信息
kubectl get node -o wide

2、创建deployments
kubectl run net-test --image=alpine --replicas=2 sleep 10

3、查看deployments详情
kubectl describe deployment net-test

4、删除deployments
kubectl delete deployment net-test -n default

5、查看pod
kubectl get pod -o wide

6、查看pod的详情
kubectl describe pod net-test-5767cb94df-7lwtq

7、手动扩容缩容
#######通过执行扩容命令，对某个deployment直接进行扩容：
$ kubectl  scale deployment net-test --replicas=4
#######当要缩容，减少副本数量即可：
$ kubectl  scale deployment net-test --replicas=2






####三个类型端口
#####1：三个类型端口所应用位置的不同
port是service的的端口

targetport是pod也就是容器的端口

nodeport是容器所在宿主机的端口(实质上也是通过service暴露给了宿主机，而port却没有)

#####2：在作用上

######port
的主要作用是集群内其他pod访问本pod的时候，需要的一个port
如nginx的pod访问mysql的pod，那么mysql的pod的service可以如下定义，由此可以这样理解，port是service的port，nginx访问service的33306
apiVersion: v1
kind: Service
metadata:
name: mysql-service
spec:
ports:
- port: 33306
targetPort: 3306
selector:
name: mysql-pod
targetport

同样的，看上面的targetport，targetport说过是pod暴露出来的port端口，当nginx的一个请求到达service的33306端口时，service就会将此请求根据selector中的name，将请求转发到mysql-pod这个pod的3306端口上


######nodeport
nodeport就很好理解了，它是集群外的客户访问，集群内的服务时，所访问的port，比如客户访问下面的集群中的nginx，就是这样的方式，ip:30001
apiVersion: v1
kind: Service
metadata:
name: nginx-service
spec:
type: NodePort // 有配置NodePort，外部流量可访问k8s中的服务
ports:
- port: 30080 // 服务访问端口
targetPort: 80 // 容器端口
nodePort: 30001 // NodePort
selector: name: nginx-pod

nodeport是集群外流量访问集群内服务的端口类型，比如客户访问nginx，apache，port是集群内的pod互相通信用的端口类型，比如nginx访问mysql，而mysql是不需要让客户访问到的，最后targetport，顾名思义，目标端口，也就是最终端口，也就是pod的端口。

#####3：总结一下
nodeport是集群外流量访问集群内服务的端口类型，比如客户访问nginx，apache，port是集群内的pod互相通信用的端口类型
比如nginx访问mysql，而mysql是不需要让客户访问到的，最后targetport，顾名思义，目标端口，也就是最终端口，也就是pod的端口。



[root@master-1 ~]# kubectl get svc  --all-namespaces
NAMESPACE              NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                  ClusterIP   10.1.0.1       <none>        443/TCP                  6h5m
kube-system            kube-dns                    ClusterIP   10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   6h5m
kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP                 3h59m
kubernetes-dashboard   kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP            3h59m










##六、Ingress安装配置
##(一)、DaemonSet+HostNetwork部署ingress-controller的方式
###1、基础环境准备（ingress、httpd、tomcat安装、部署）

Ingress-Nginx官网地址
https://kubernetes.github.io/ingress-nginx/

Ingress-Nginx GitHub地址
https://github.com/kubernetes/ingress-nginx


    internet
        |
   [ Ingress ]
   --|-----|--
   [ Services ]

#####Ingress-nginx介绍：

在Kubernetes中，服务和Pod的IP地址仅可以在集群网络内部使用，对于集群外的应用是不可见的。为了使外部的应用能够访问集群内的服务，在Kubernetes中目前提供了以下几种方案：
* NodePort
* LoadBalancer
* Ingress
在之前的博文中介绍过NodePort，简单来说，就是通过service这种资源对象，为后端pod提供一个统一的访问接口，然后将service的统一访问接口映射到群集节点上，最终实现client通过映射到群集节点上的端口访问到后端pod提供的服务。

但是，这种方式有一个弊端，就是当新生成一个pod服务就需要创建对应的service将其映射到节点端口，当运行的pod过多时，我们节点暴露给client端的端口也会随之增加，这样我们整个k8s群集的危险系数就会增加，因为我们在搭建群集之处，官方明确指出，必须关闭firewalld防火墙及清空iptables规则，现在我们又暴露了那么多端口给client，安全系数可想而知。


1、Ingress-nginx组成

* ingress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），
 动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）；
* ingress资源对象：将Nginx的配置抽象成一个Ingress对象，每添加一个新的Service资
 源对象只需写一个新的Ingress规则的yaml文件即可（或修改已存在的ingress规则的yaml文件）

2、Ingress-nginx可以解决什么问题？

1)动态配置服务
　　如果按照传统方式, 当新增加一个服务时, 我们可能需要在流量入口加一个反向代理指
	向我们新的k8s服务. 而如果用了Ingress-nginx, 只需要配置好这个服务, 当服务启动
	时, 会自动注册到Ingress的中, 不需要而外的操作。
2)减少不必要的端口映射
　　配置过k8s的都清楚, 第一步是要关闭防火墙的, 主要原因是k8s的很多服务会
	以NodePort方式映射出去, 这样就相当于给宿主机打了很多孔, 既不安全也不优雅. 
	而Ingress可以避免这个问题, 除了Ingress自身服务可能需要映射出去, 其他服务都不要
	用NodePort方式

3、Ingress-nginx工作原理

登录后复制
1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化，
2）然后读取它，按照自定义的规则，规则就是写明了哪个域名对应哪个service，生成一段nginx配置，
3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器会把生成的nginx配置写入/etc/nginx.conf文件中，
4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。



Ingress为弥补NodePort不足而生
NodePort存在的不足：
一个端口只能一个服务使用，端口需提前规划
只支持4层负载均衡

Pod与Ingress的关系“”
通过Service相关联
通过Ingress Controller实现Pod的负载均衡
支持TCP/UDP 4层和HTTP 7层



####可能有些人会对ingress-controller和ingress这两个概念不理解，会感觉太抽象。
做个简单的比喻：
1、ingress-controller相当于在pod中部署了一个“nginx”的负载均衡器，对外只需要暴露这个负载均衡器的端口；
2、ingress则相当于在“nginx”中创建代理的配置文件，指定访问过来的流量被转发到哪个service资源。

下面是一个大概的对照关系：
ingress-controller——>>>nginx
ingress———————>>/etc/nginx/conf.d/xxx.com.conf文件

如您发现文中有错误，请评论区留言指导博主改正，谢谢。



######测试环境架构：

                           | -> 172.16.201.135 ->   /         service1:80
                           |                        /tomcat   service2:80
 www.test01.com -> vip->   |
                           |
                           | -> 172.16.201.136 ->   /         service1:80
                                                    /tomcat   service2:80


wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.0.1/deploy/static/provider/cloud/deploy.yaml

######v1.0.0测试可用
wget https://github.com/kubernetes/ingress-nginx/blob/controller-v1.0.0/deploy/static/provider/exoscale/deploy.yaml



######官方ingress、service 文档（语法参考）：
https://kubernetes.io/zh/docs/concepts/services-networking/ingress/
https://kubernetes.io/zh/docs/concepts/services-networking/service/


####1)ingress部署





######yaml文件配置修改
[root@k8s-master ingress]# vim ingress-deploy.yaml
………………
apiVersion: apps/v1
kind: DaemonSet   # 从Deployment改为DaemonSet
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  #replicas: 1   # 注释掉
………………
      nodeSelector:
      # 如下几行为新加行  作用【允许在master节点运行】
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
………………
          ports:
            - name: http
              containerPort: 80
              hostPort: 80    # 添加处【可在宿主机通过该端口访问Pod】
              protocol: TCP
            - name: https
              containerPort: 443
              hostPort: 443   # 添加处【可在宿主机通过该端口访问Pod】
              protocol: TCP
………………


[root@master-1 ~]# kubectl apply -f ingress-deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
configmap/ingress-nginx-controller created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
service/ingress-nginx-controller-admission created
service/ingress-nginx-controller created
deployment.apps/ingress-nginx-controller created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
serviceaccount/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created






[root@master-1 ~]# kubectl get ns   
NAME              STATUS   AGE
default           Active   30h
ingress-nginx     Active   15m
kube-node-lease   Active   30h
kube-public       Active   30h
kube-system       Active   30h
test-ns           Active   7s



[root@master-1 ~]# kubectl get pod -n ingress-nginx -o wide
NAME                                       READY   STATUS              RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-4wp6d   0/1     ContainerCreating   0          75m   10.244.2.35      node-2   <none>           <none>
ingress-nginx-admission-patch-glk59    0/1     ContainerCreating   0          75m   10.244.1.31      node-1   <none>           <none>
ingress-nginx-controller-s7x2z         1/1     ContainerCreating     0        75m   172.16.201.136   node-2   <none>           <none>
ingress-nginx-controller-xf68j         1/1     ContainerCreating     0        75m   172.16.201.135   node-1   <none>           <none>


[root@master-1 ~]# kubectl get pod -n ingress-nginx -o wide
NAME                                   READY   STATUS      RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-4wp6d   0/1     Completed   0          75m   10.244.2.35      node-2   <none>           <none>
ingress-nginx-admission-patch-glk59    0/1     Completed   0          75m   10.244.1.31      node-1   <none>           <none>
ingress-nginx-controller-s7x2z         1/1     Running     0          75m   172.16.201.136   node-2   <none>           <none>
ingress-nginx-controller-xf68j         1/1     Running     0          75m   172.16.201.135   node-1   <none>           <none>



[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide
NAMESPACE       NAME                                   READY   STATUS      RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-admission-create-4wp6d   0/1     Completed   0          76m     10.244.2.35      node-2     <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-glk59    0/1     Completed   0          76m     10.244.1.31      node-1     <none>           <none>
ingress-nginx   ingress-nginx-controller-s7x2z         1/1     Running     0          76m     172.16.201.136   node-2     <none>           <none>
ingress-nginx   ingress-nginx-controller-xf68j         1/1     Running     0          76m     172.16.201.135   node-1     <none>           <none>
kube-system     coredns-6d56c8448f-mp5lz               1/1     Running     0          2d5h    10.244.1.2       node-1     <none>           <none>
kube-system     coredns-6d56c8448f-wnqxj               1/1     Running     0          2d5h    10.244.2.2       node-2     <none>           <none>
kube-system     etcd-master-1                          1/1     Running     2          2d5h    172.16.201.134   master-1   <none>           <none>
kube-system     kube-apiserver-master-1                1/1     Running     20         2d5h    172.16.201.134   master-1   <none>           <none>
kube-system     kube-controller-manager-master-1       1/1     Running     6          2d5h    172.16.201.134   master-1   <none>           <none>
kube-system     kube-flannel-ds-mmhsm                  1/1     Running     0          2d5h    172.16.201.135   node-1     <none>           <none>
kube-system     kube-flannel-ds-rz2xj                  1/1     Running     2          2d5h    172.16.201.134   master-1   <none>           <none>
kube-system     kube-flannel-ds-ts9fm                  1/1     Running     0          2d5h    172.16.201.136   node-2     <none>           <none>
kube-system     kube-proxy-bkck2                       1/1     Running     0          2d5h    172.16.201.135   node-1     <none>           <none>
kube-system     kube-proxy-c6fdx                       1/1     Running     2          2d5h    172.16.201.134   master-1   <none>           <none>
kube-system     kube-proxy-phjdh                       1/1     Running     0          2d5h    172.16.201.136   node-2     <none>           <none>
kube-system     kube-scheduler-master-1                1/1     Running     6          2d5h    172.16.201.134   master-1   <none>           <none>





[root@master-1 ~]# kubectl get services -o wide --all-namespaces
NAMESPACE       NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
default         kubernetes                           ClusterIP      10.1.0.1       <none>        443/TCP                  2d5h    <none>
ingress-nginx   ingress-nginx-controller-admission   LoadBalancer   10.1.255.129   <pending>     443:30155/TCP            76m     app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
kube-system     kube-dns                             ClusterIP      10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   2d5h    k8s-app=kube-dns
test-ns         httpd-svc                            ClusterIP      10.1.142.189   <none>        80/TCP                   6h35m   k8s-app=httpd01
test-ns         tomcat-svc                           ClusterIP      10.1.29.129    <none>        8080/TCP                 21h     k8s-app=tomcat01



[root@master-1 ~]# kubectl get services -o wide --all-namespaces|grep ingress-nginx
ingress-nginx   ingress-nginx-controller-admission   LoadBalancer   10.1.255.129   <pending>     443:30155/TCP            77m     app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx



[root@master-1 ~]#  kubectl delete -f ingress-deploy.yaml
namespace "ingress-nginx" deleted
serviceaccount "ingress-nginx" deleted
configmap "ingress-nginx-controller" deleted
clusterrole.rbac.authorization.k8s.io "ingress-nginx" deleted
clusterrolebinding.rbac.authorization.k8s.io "ingress-nginx" deleted
role.rbac.authorization.k8s.io "ingress-nginx" deleted
rolebinding.rbac.authorization.k8s.io "ingress-nginx" deleted
service "ingress-nginx-controller-admission" deleted
service "ingress-nginx-controller" deleted
deployment.apps "ingress-nginx-controller" deleted
ingressclass.networking.k8s.io "nginx" deleted
validatingwebhookconfiguration.admissionregistration.k8s.io "ingress-nginx-admission" deleted
serviceaccount "ingress-nginx-admission" deleted
clusterrole.rbac.authorization.k8s.io "ingress-nginx-admission" deleted
clusterrolebinding.rbac.authorization.k8s.io "ingress-nginx-admission" deleted
role.rbac.authorization.k8s.io "ingress-nginx-admission" deleted
rolebinding.rbac.authorization.k8s.io "ingress-nginx-admission" deleted
job.batch "ingress-nginx-admission-create" deleted
job.batch "ingress-nginx-admission-patch" deleted


查看启动情况
[root@master-1 ~]# kubectl get pod -n ingress-nginx -o wide
NAME                                   READY   STATUS      RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
ingress-nginx-admission-create-4wp6d   0/1     Completed   0          77m   10.244.2.35      node-2   <none>           <none>
ingress-nginx-admission-patch-glk59    0/1     Completed   0          77m   10.244.1.31      node-1   <none>           <none>
ingress-nginx-controller-s7x2z         1/1     Running     0          77m   172.16.201.136   node-2   <none>           <none>
ingress-nginx-controller-xf68j         1/1     Running     0          77m   172.16.201.135   node-1   <none>           <none>


[root@master-1 ~]# kubectl get services -o wide --all-namespaces|grep ingress-nginx
ingress-nginx   ingress-nginx-controller-admission   LoadBalancer   10.1.255.129   <pending>     443:30155/TCP            78m     app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
[root@master-1 ~]# 


kubectl get services -o wide --all-namespaces


######注：
node-1是172.16.201.135
node-2是172.16.201.136


物理服务器135、136的30155 都可以访问到：
[root@master-1 ~]# curl -I https://172.16.201.135:30155/
curl: (60) Peer's Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.


[root@master-1 ~]# curl -I https://172.16.201.136:30155/
curl: (60) Peer's Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a "bundle"
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.



172.16.201.135/136上的80/443都打开了
[root@node-1 ~]#  netstat -natp |egrep ":80|:443"
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      105447/nginx: maste 
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      105447/nginx: maste 
tcp        0      0 172.16.201.135:50502    10.1.0.1:443            ESTABLISHED 17624/flanneld      
tcp        0      0 172.16.201.135:42880    10.1.0.1:443            ESTABLISHED 105403/nginx-ingres 
tcp6       0      0 :::80                   :::*                    LISTEN      105447/nginx: maste 
tcp6       0      0 :::443                  :::*                    LISTEN      105447/nginx: maste 

[root@node-2 ~]# netstat -natp |egrep ":80|:443"
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      706/nginx: worker p 
tcp        0      0 0.0.0.0:443             0.0.0.0:*               LISTEN      706/nginx: worker p 
tcp        0      0 172.16.201.136:37770    10.1.0.1:443            ESTABLISHED 113039/nginx-ingres 
tcp        0      0 172.16.201.136:59322    10.1.0.1:443            ESTABLISHED 17394/flanneld      
tcp6       0      0 :::80                   :::*                    LISTEN      700/nginx: worker p 
tcp6       0      0 :::443                  :::*                    LISTEN      700/nginx: worker p 


ingress端口全部开放了

####2)apache  tomcat部署

[root@master ~]# docker pull httpd
[root@master ~]# docker pull tomcat


###改配置文件：
###注：大招：语法不会，下载成型的yaml文件，复制过来。

kubectl  get  pod,svc -n test-ns


[root@master-1 ~]# kubectl delete -f httpd.yaml      
deployment.apps "httpd01" deleted
service "httpd-svc" deleted

[root@master-1 ~]# kubectl apply -f httpd.yaml
deployment.apps/httpd01 created
service/httpd-svc created
                                                                                                                          
[root@master-1 ~]# cat httpd.yaml 
kind: Deployment
apiVersion: apps/v1
metadata:
  name: httpd01
  namespace: test-ns
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: httpd01
  template:
    metadata:
      labels:
        k8s-app: httpd01
    spec:
      containers:
        - name: httpd
          image: httpd

---

kind: Service
apiVersion: v1
metadata:
  name: httpd-svc
  namespace: test-ns
spec:
  selector:
    k8s-app: httpd01
  ports:
    - port: 81
      targetPort: 81

[root@master-1 ~]# 





[root@master-1 ~]# kubectl apply -f tomcat-Deployment.yaml 
deployment.apps/tomcat01 created
service/tomcat-svc created


[root@master-1 ~]# cat tomcat-Deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: tomcat01
  namespace: test-ns
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: tomcat01
  template:
    metadata:
      labels:
        k8s-app: tomcat01
    spec:
      containers:
        - name: tomcat
          image: tomcat

---

kind: Service
apiVersion: v1
metadata:
  name: tomcat-svc
  namespace: test-ns
spec:
  selector:
    k8s-app: tomcat01
  ports:
    - port: 8080
      targetPort: 8080



[root@node-2 ~]# kubectl  get  pod,svc -n test-ns
NAME                           READY   STATUS    RESTARTS   AGE
pod/httpd01-699c8fcff4-bhn84   1/1     Running   0          6h44m
pod/tomcat01-95fc6cd5d-p85g6   1/1     Running   0          22h

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
service/httpd-svc    ClusterIP   10.1.142.189   <none>        80/TCP     6h44m
service/tomcat-svc   ClusterIP   10.1.29.129    <none>        8080/TCP   22h


[root@master-1 ~]# curl -I 10.1.142.189
HTTP/1.1 200 OK
Date: Thu, 23 Sep 2021 12:11:58 GMT
Server: Apache/2.4.49 (Unix)
Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT
ETag: "2d-432a5e4a73a80"
Accept-Ranges: bytes
Content-Length: 45
Content-Type: text/html


[root@master-1 ~]#  curl -I 10.1.29.129:8080
HTTP/1.1 404 
Content-Type: text/html;charset=utf-8
Content-Language: en
Transfer-Encoding: chunked
Date: Thu, 23 Sep 2021 11:56:27 GMT


#####新版本tomcat没有ROOT目录，所以只有一个节点，直接进去加默认测试页面：
[root@node-1 ~]# docker exec -it  k8s_tomcat_tomcat01-95fc6cd5d-p85g6_test-ns_500aacb5-62be-4879-9c14-1ede8dbec968_0 /bin/bash
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat#mkdir webapps/ROOT;cd webapps
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat/webapps# echo "######  K8S  #######" > ROOT/index.jsp  

tomcat这里的话, latest 是tomcat9, webapps 下默认是空的, 当时还确认了半天有没有配置错误, 后来才发现 webapps 下没有内容, 内容在另一个 webapps.dist 下 , 只需要把 webapps.dist 下的内容移到 webapps 或者直接改名文件夹, 再通过浏览器访问 tomcat.whn.com 就能访问到 tomcat 的首页.
#####新版本tomcat没有ROOT目录，所以只有一个节点，直接进去加默认测试页面：

#####tomcat部署静态页面#####
docker run --name tomcat-test -p 8080:8080 tomcat 

docker exec -it  tomcat /bin/bash
docker exec -it  k8s_tomcat_tomcat01-95fc6cd5d-p85g6_test-ns_500aacb5-62be-4879-9c14-1ede8dbec968_0 /bin/bash
docker exec -it  tomcat /bin/bash
docker exec -it  52974bf17162 /bin/bash

echo "test" > ROOT/index.jsp
echo "test" > /usr/local/tomcat/webapps/ROOT/index.jsp
#####tomcat部署静态页面#####



再访问测试页面：
[root@master-1 ~]#  curl 10.1.29.129:8080   
######  K8S  #######

[root@master-1 ~]#  curl -I 10.1.29.129:8080
HTTP/1.1 200 
Set-Cookie: JSESSIONID=2CFC11C9028533DAE8772F47D8632716; Path=/; HttpOnly
Content-Type: text/html;charset=UTF-8
Transfer-Encoding: chunked
Date: Fri, 24 Sep 2021 02:09:11 GMT




[root@node-2 ~]#  kubectl get services -o wide --all-namespaces
NAMESPACE       NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE     SELECTOR
default         kubernetes                           ClusterIP      10.1.0.1       <none>        443/TCP                  2d5h    <none>
ingress-nginx   ingress-nginx-controller-admission   LoadBalancer   10.1.255.129   <pending>     443:30155/TCP            86m     app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
kube-system     kube-dns                             ClusterIP      10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   2d5h    k8s-app=kube-dns
test-ns         httpd-svc                            ClusterIP      10.1.142.189   <none>        80/TCP                   6h45m   k8s-app=httpd01
test-ns         tomcat-svc                           ClusterIP      10.1.29.129    <none>        8080/TCP                 22h     k8s-app=tomcat01





replicas: 20 做下测试
[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide      
NAMESPACE       NAME                                        READY   STATUS      RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-admission-create-wvlxx        0/1     Completed   0          15h     10.244.2.8       node-2     <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-872k8         0/1     Completed   1          15h     10.244.1.8       node-1     <none>           <none>
ingress-nginx   ingress-nginx-controller-6d4d95754c-w2fvb   1/1     Running     0          15h     10.244.1.9       node-1     <none>           <none>
kube-system     coredns-6d56c8448f-mp5lz                    1/1     Running     0          45h     10.244.1.2       node-1     <none>           <none>
kube-system     coredns-6d56c8448f-wnqxj                    1/1     Running     0          45h     10.244.2.2       node-2     <none>           <none>
kube-system     etcd-master-1                               1/1     Running     2          45h     172.16.201.134   master-1   <none>           <none>
kube-system     kube-apiserver-master-1                     1/1     Running     20         45h     172.16.201.134   master-1   <none>           <none>
kube-system     kube-controller-manager-master-1            1/1     Running     6          45h     172.16.201.134   master-1   <none>           <none>
kube-system     kube-flannel-ds-mmhsm                       1/1     Running     0          45h     172.16.201.135   node-1     <none>           <none>
kube-system     kube-flannel-ds-rz2xj                       1/1     Running     2          45h     172.16.201.134   master-1   <none>           <none>
kube-system     kube-flannel-ds-ts9fm                       1/1     Running     0          45h     172.16.201.136   node-2     <none>           <none>
kube-system     kube-proxy-bkck2                            1/1     Running     0          45h     172.16.201.135   node-1     <none>           <none>
kube-system     kube-proxy-c6fdx                            1/1     Running     2          45h     172.16.201.134   master-1   <none>           <none>
kube-system     kube-proxy-phjdh                            1/1     Running     0          45h     172.16.201.136   node-2     <none>           <none>
kube-system     kube-scheduler-master-1                     1/1     Running     6          45h     172.16.201.134   master-1   <none>           <none>
test-ns         httpd01-699c8fcff4-4bf2k                    1/1     Running     0          3m44s   10.244.2.20      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-4ch2q                    1/1     Running     0          3m44s   10.244.1.16      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-4j4dn                    1/1     Running     0          3m44s   10.244.1.14      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-527lk                    1/1     Running     0          3m44s   10.244.2.15      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-562zd                    1/1     Running     0          3m44s   10.244.2.18      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-5cws6                    1/1     Running     0          3m44s   10.244.2.22      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-5nxhd                    1/1     Running     0          3m44s   10.244.2.23      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-76vmt                    1/1     Running     0          3m44s   10.244.1.15      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-bklcv                    1/1     Running     0          3m44s   10.244.2.21      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-cv8td                    1/1     Running     0          3m44s   10.244.2.14      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-fmbt5                    1/1     Running     0          3m44s   10.244.2.17      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-fnwfn                    1/1     Running     0          3m44s   10.244.2.16      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-hqqll                    1/1     Running     0          3m44s   10.244.1.13      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-khljq                    1/1     Running     0          3m44s   10.244.1.18      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-p2wcf                    1/1     Running     0          3m44s   10.244.1.22      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-qrh7l                    1/1     Running     0          3m44s   10.244.1.19      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-rs9fm                    1/1     Running     0          3m43s   10.244.1.21      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-vtgfk                    1/1     Running     0          3m44s   10.244.1.17      node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-vx6kr                    1/1     Running     0          3m44s   10.244.2.19      node-2     <none>           <none>
test-ns         httpd01-699c8fcff4-vxv65                    1/1     Running     0          3m44s   10.244.1.20      node-1     <none>           <none>
test-ns         tomcat01-95fc6cd5d-p85g6                    1/1     Running     0          13h     10.244.1.12      node-1     <none>           <none>
[root@master-1 ~]# 


一切准备完毕：
[root@node-2 ~]# kubectl get pods  --all-namespaces -o wide   |grep -v kube-system
NAMESPACE       NAME                                   READY   STATUS      RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
ingress-nginx   ingress-nginx-admission-create-4wp6d   0/1     Completed   0          87m     10.244.2.35      node-2     <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-glk59    0/1     Completed   0          87m     10.244.1.31      node-1     <none>           <none>
ingress-nginx   ingress-nginx-controller-s7x2z         1/1     Running     0          87m     172.16.201.136   node-2     <none>           <none>
ingress-nginx   ingress-nginx-controller-xf68j         1/1     Running     0          87m     172.16.201.135   node-1     <none>           <none>
test-ns         httpd01-699c8fcff4-bhn84               1/1     Running     0          6h46m   10.244.2.26      node-2     <none>           <none>
test-ns         tomcat01-95fc6cd5d-p85g6               1/1     Running     0          22h     10.244.1.12      node-1     <none>           <none>










####3)、ingress配置域名访问
######（1）创建Ingress-nginx资源对象
略之前弄好了

######（2）定义Ingress规则
[root@master-1 ~]# cat test-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: test-ns
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: test-ingress-class
  rules:
  - host: www.test01.com
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: httpd-svc
            port: 
             number: 80
      - pathType: Prefix
        path: /tomcat
        backend:
          service:
            name: tomcat-svc
            port: 
             number: 8080
[root@master-1 ~]# 


[root@master-1 ~]#  kubectl delete -f  test-ingress.yaml
[root@master-1 ~]#  kubectl apply -f  test-ingress.yaml
ingress.networking.k8s.io/test-ingress created



[root@master-1 ~]# kubectl get pod,svc,ing -n test-ns -o wide  
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd01-699c8fcff4-bhn84   1/1     Running   0          7h16m   10.244.2.26   node-2   <none>           <none>
pod/tomcat01-95fc6cd5d-p85g6   1/1     Running   0          22h     10.244.1.12   node-1   <none>           <none>

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/httpd-svc    ClusterIP   10.1.142.189   <none>        80/TCP     7h16m   k8s-app=httpd01
service/tomcat-svc   ClusterIP   10.1.29.129    <none>        8080/TCP   22h     k8s-app=tomcat01

NAME                              CLASS                HOSTS            ADDRESS                         PORTS   AGE
ingress.extensions/test-ingress   test-ingress-class   www.test01.com   172.16.201.135,172.16.201.136   80      66m
[root@master-1 ~]# 




[root@master-1 ~]# kubectl get pod,services -n ingress-nginx -o wide
NAME                                       READY   STATUS      RESTARTS   AGE    IP               NODE     NOMINATED NODE   READINESS GATES
pod/ingress-nginx-admission-create-4wp6d   0/1     Completed   0          118m   10.244.2.35      node-2   <none>           <none>
pod/ingress-nginx-admission-patch-glk59    0/1     Completed   0          118m   10.244.1.31      node-1   <none>           <none>
pod/ingress-nginx-controller-s7x2z         1/1     Running     0          118m   172.16.201.136   node-2   <none>           <none>
pod/ingress-nginx-controller-xf68j         1/1     Running     0          118m   172.16.201.135   node-1   <none>           <none>

NAME                                         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE    SELECTOR
service/ingress-nginx-controller-admission   LoadBalancer   10.1.255.129   <pending>     443:30155/TCP   118m   app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx
[root@master-1 ~]# 



[root@master-1 ~]# curl -v http://172.16.201.135 -H 'www.test01.com'
* About to connect() to 172.16.201.135 port 80 (#0)
*   Trying 172.16.201.135...
* Connected to 172.16.201.135 (172.16.201.135) port 80 (#0)
> GET / HTTP/1.1
> User-Agent: curl/7.29.0
> Host: 172.16.201.135
> Accept: */*
> 
< HTTP/1.1 404 Not Found
< Date: Fri, 24 Sep 2021 04:21:26 GMT
< Content-Type: text/html
< Content-Length: 146
< Connection: keep-alive
< 
<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx</center>
</body>
</html>
* Connection #0 to host 172.16.201.135 left intact



######访问404,查下ingress-nginx-controller其中一台log

[root@master-1 ~]# kubectl logs -f  pod/ingress-nginx-controller-xf68j -n ingress-nginx
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.0.1
  Build:         abab0396757dcd6f72018ee66611db18df838b17
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.19.9

-------------------------------------------------------------------------------

W0924 08:36:24.168041       6 client_config.go:615] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0924 08:36:24.168552       6 main.go:221] "Creating API client" host="https://10.1.0.1:443"
I0924 08:36:24.308254       6 main.go:265] "Running in Kubernetes cluster" major="1" minor="19" git="v1.19.9" state="clean" commit="9dd794e454ac32d97cde41ae10be801ae98f75df" platform="linux/amd64"
I0924 08:36:24.709243       6 main.go:104] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0924 08:36:24.851204       6 ssl.go:531] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0924 08:36:24.900495       6 nginx.go:253] "Starting NGINX Ingress controller"
I0924 08:36:24.953833       6 event.go:282] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"410b7163-576a-4251-8cee-f52717d25ca9", APIVersion:"v1", ResourceVersion:"242543", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0924 08:36:26.102677       6 nginx.go:295] "Starting NGINX process"
I0924 08:36:26.103288       6 nginx.go:315] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0924 08:36:26.103438       6 leaderelection.go:243] attempting to acquire leader lease ingress-nginx/ingress-controller-leader...
I0924 08:36:26.104643       6 controller.go:152] "Configuration changes detected, backend reload required"
I0924 08:36:26.136938       6 leaderelection.go:253] successfully acquired lease ingress-nginx/ingress-controller-leader
I0924 08:36:26.137800       6 status.go:84] "New leader elected" identity="ingress-nginx-controller-xf68j"
I0924 08:36:26.174548       6 status.go:215] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-s7x2z" node="node-2"
I0924 08:36:26.174611       6 status.go:215] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-xf68j" node="node-1"
I0924 08:36:26.287525       6 controller.go:169] "Backend successfully reloaded"
I0924 08:36:26.287615       6 controller.go:180] "Initial sync, sleeping for 1 second"
I0924 08:36:26.288333       6 event.go:282] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-xf68j", UID:"58b73412-4368-465b-bd3a-711d1268e081", APIVersion:"v1", ResourceVersion:"242637", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
I0924 09:09:23.634551       6 store.go:361] "Ignoring ingress because of error while validating ingress class" ingress="test-ns/test-ingress" error="ingress does not contain a valid IngressClass"



#####错误：
I0924 09:09:23.634551       6 store.go:361] "Ignoring ingress because of error while validating ingress class" ingress="test-ns/test-ingress" error="ingress does not contain a valid IngressClass"


####缺少class，新建class
提示ingress资源未指定IngressClass。
查看官方提供的deploy中确实有创建IngressClass，但是指定了namespace:ingress-nginx
于是将该部分内容复制出来，用于在default空间也创建一个IngressClass。


[root@master-1 ~]# vim class.yaml
---
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    helm.sh/chart: ingress-nginx-4.0.1
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/version: 1.0.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
	name: test-ingress-class
  namespace: test-ns
spec:
  controller: k8s.io/ingress-nginx
	
[root@master-1 ~]# kubectl apply -f class.yaml
ingressclass.networking.k8s.io/test-ingress created

[root@master-1 ~]# kubectl delete -f class.yaml


#####修改test-ingress.yaml，指定ingressClassName使用刚才创建的class：test-ingress-class
[root@master-1 ~]# vim test-ingress.yaml
spec:
  ingressClassName: test-ingress-class
  rules:
  - host: www.test01.com




[root@master-1 ~]# kubectl get pod,svc,ing -n test-ns -o wide  
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd01-699c8fcff4-bhn84   1/1     Running   0          7h25m   10.244.2.26   node-2   <none>           <none>
pod/tomcat01-95fc6cd5d-p85g6   1/1     Running   0          22h     10.244.1.12   node-1   <none>           <none>

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/httpd-svc    ClusterIP   10.1.142.189   <none>        80/TCP     7h25m   k8s-app=httpd01
service/tomcat-svc   ClusterIP   10.1.29.129    <none>        8080/TCP   22h     k8s-app=tomcat01

NAME                              CLASS                HOSTS            ADDRESS                         PORTS   AGE
ingress.extensions/test-ingress   test-ingress-class   www.test01.com   172.16.201.135,172.16.201.136   80      75m
####class 已经有了 ，上午没有配置class。



测试apache tomcat访问：
[root@master-1 ~]# curl -I www.test01.com/tomcat
HTTP/1.1 200 
Date: Fri, 24 Sep 2021 09:29:18 GMT
Content-Type: text/html;charset=UTF-8
Connection: keep-alive
Set-Cookie: JSESSIONID=638BFBB0E0EF463AA5D84A066C0701B9; Path=/; HttpOnly


[root@master-1 ~]# curl -I www.test01.com
HTTP/1.1 200 OK
Date: Fri, 24 Sep 2021 09:29:31 GMT
Content-Type: text/html
Content-Length: 45
Connection: keep-alive
Last-Modified: Mon, 11 Jun 2007 18:53:14 GMT
ETag: "2d-432a5e4a73a80"
Accept-Ranges: bytes



查看日志，已经可以访问：
[root@master-1 ~]# kubectl logs -f  pod/ingress-nginx-controller-xf68j -n ingress-nginx
I0924 09:27:39.304304       6 controller.go:152] "Configuration changes detected, backend reload required"
I0924 09:27:39.767827       6 controller.go:169] "Backend successfully reloaded"
I0924 09:27:39.769350       6 event.go:282] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-xf68j", UID:"58b73412-4368-465b-bd3a-711d1268e081", APIVersion:"v1", ResourceVersion:"242637", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
172.16.201.1 - - [24/Sep/2021:09:27:48 +0000] "HEAD /tomcat HTTP/1.1" 200 0 "-" "curl/7.64.1" 85 0.062 [test-ns-tomcat-svc-8080] [] 10.244.1.12:8080 0 0.062 200 4b33c1fea6a1afc5657e6ea620f759d0
172.16.201.1 - - [24/Sep/2021:09:27:53 +0000] "HEAD / HTTP/1.1" 200 0 "-" "curl/7.64.1" 79 0.011 [test-ns-httpd-svc-80] [] 10.244.2.26:80 0 0.011 200 f252e5f7ca67e6ac85d4f843dfa76017
I0924 09:28:26.158932       6 status.go:287] "updating Ingress status" namespace="test-ns" ingress="test-ingress" currentValue=[] newValue=[{IP:172.16.201.135 Hostname: Ports:[]} {IP:172.16.201.136 Hostname: Ports:[]}]
I0924 09:28:26.168751       6 event.go:282] Event(v1.ObjectReference{Kind:"Ingress", Namespace:"test-ns", Name:"test-ingress", UID:"1e61ec88-23c6-4a32-ab26-9ded9d4e87eb", APIVersion:"networking.k8s.io/v1", ResourceVersion:"250503", FieldPath:""}): type: 'Normal' reason: 'Sync' Scheduled for sync
172.16.201.134 - - [24/Sep/2021:09:29:00 +0000] "GET / HTTP/1.1" 200 45 "-" "curl/7.29.0" 78 0.006 [test-ns-httpd-svc-80] [] 10.244.2.26:80 45 0.006 200 4ef43dbb6645241aae5ffe9df520ef70
172.16.201.134 - - [24/Sep/2021:09:29:03 +0000] "GET /tomcat HTTP/1.1" 200 21 "-" "curl/7.29.0" 84 0.009 [test-ns-tomcat-svc-8080] [] 10.244.1.12:8080 21 0.009 200 d9a7dde6c57304e4ae9043a8c51ee1fb
172.16.201.134 - - [24/Sep/2021:09:29:18 +0000] "HEAD /tomcat HTTP/1.1" 200 0 "-" "curl/7.29.0" 85 0.005 [test-ns-tomcat-svc-8080] [] 10.244.1.12:8080 0 0.005 200 68131e830021a1412254ff48fe2fc960
172.16.201.134 - - [24/Sep/2021:09:29:31 +0000] "HEAD / HTTP/1.1" 200 0 "-" "curl/7.29.0" 79 0.007 [test-ns-httpd-svc-80] [] 10.244.2.26:80 0 0.007 200 90c8e8d35e5a50e6cba61c4cb79feccd





[root@master-1 ~]# kubectl describe ingress test-ingress
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
Error from server (NotFound): ingresses.extensions "test-ingress" not found




####总结上述示例的pod是如何一步一步可以使client访问到的，总结如下：

后端pod===》service====》ingress规则====》写入Ingress-nginx-controller配置文件并自动重载使更改生效===》对Ingress-nginx创建service====》实现client无论通过哪个K8节点的IP+端口都可以访问到后端pod


参考：
https://kubernetes.io/zh/docs/concepts/services-networking/ingress/




##(二)、Deployment+NodePort模式的Service部署方式

只是简单增加两个nodePort字段，指定service在Node节点映射的端口，这样在上层负载均衡继续做代理就会方便很多。
由于这里的nodePort是对service端口的映射，所以会自动在所有Node节点生成端口映射。
接着就可以在上层负载均衡器做反向代理，将流量负载到所有Node的30080和30443端口，
最后对外只需要暴露负载均衡的IP+Port即可。



######yaml文件配置修改
[root@k8s-master ingress]# vim ingress-deploy.yaml
………………
apiVersion: apps/v1
kind: DaemonSet   # 从Deployment改为DaemonSet
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  #replicas: 1   # 注释掉
………………
      nodeSelector:
      # 如下几行为新加行  作用【允许在master节点运行】
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
………………
          ports:
            - name: http
              containerPort: 80
              hostPort: 80    # 添加处【可在宿主机通过该端口访问Pod】
              protocol: TCP
							nodePort: 30080
            - name: https
              containerPort: 443
              hostPort: 443   # 添加处【可在宿主机通过该端口访问Pod】
              protocol: TCP
							nodePort: 30443
………………


可能有些人会对ingress-controller和ingress这两个概念不理解，会感觉太抽象。
做个简单的比喻：
1、ingress-controller相当于在pod中部署了一个“nginx”的负载均衡器，对外只需要暴露这个负载均衡器的端口；
2、ingress则相当于在“nginx”中创建代理的配置文件，指定访问过来的流量被转发到哪个service资源。

下面是一个大概的对照关系：
ingress-controller——>>>nginx
ingress———————>>/etc/nginx/conf.d/xxx.com.conf文件








#####增加站点www.test02.com：
[root@master-1 ~]# cat test-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: test-ns
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: test-ingress-class
  rules:
  - host: www.test02.com
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: tomcat-svc
            port:
             number: 8080


  - host: www.test01.com
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: httpd-svc
            port: 
             number: 80
      - pathType: Prefix
        path: /tomcat
        backend:
          service:
            name: tomcat-svc
            port: 
             number: 8080
[root@master-1 ~]# 


[root@master-1 ~]# kubectl apply -f test-ingress.yaml 
ingress.networking.k8s.io/test-ingress configured


客户端访问：
lex@lexliudeMacBook-Pro Downloads %curl  -I www.test02.com/
HTTP/1.1 200 
Date: Sun, 26 Sep 2021 04:01:32 GMT
Content-Type: text/html;charset=UTF-8
Connection: keep-alive
Set-Cookie: JSESSIONID=F39634B026FC8389D3F5E42DEE21C848; Path=/; HttpOnly



##(三)、配置HTTPS

参考：https://blog.51cto.com/u_14306186/2523096




[root@master-1 https]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=nginxsvc/O=nginxsvc"
Generating a 2048 bit RSA private key
.+++
..................+++
writing new private key to 'tls.key'
-----


[root@master-1 https]# ll
total 8
-rw-r--r-- 1 root root 1143 Sep 26 13:38 tls.crt
-rw-r--r-- 1 root root 1704 Sep 26 13:38 tls.key

[root@master-1 https]# kubectl create secret tls tls-secret --key=tls.key --cert tls.crt
secret/tls-secret created

[root@master-1 https]# kubectl get secret
NAME                  TYPE                                  DATA   AGE
default-token-b4d9n   kubernetes.io/service-account-token   3      4d1h
tls-secret            kubernetes.io/tls                     2      16s

[root@master-1 https]# cat httpd03.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: test-ns
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
      - www.test03.com
    secretName: tls-secret
  ingressClassName: test-ingress-class
  rules:
  - host: www.test03.com
    http:
      paths:
      - pathType: Prefix
        path: /tomcat-tls
        backend:
          service:
            name: tomcat-svc
            port:
             number: 8080
[root@master-1 https]# 


#####注：提交新的Ingress配置，会覆盖之前测试脚本
[root@master-1 https]# kubectl apply -f httpd03.yaml
ingress.networking.k8s.io/test-ingress configured
#####注：提交新的Ingress配置，会覆盖之前测试脚本


[root@master-1 https]# kubectl get pod,svc,ing -n test-ns -o wide  
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd01-699c8fcff4-bhn84   1/1     Running   0          2d2h    10.244.2.26   node-2   <none>           <none>
pod/tomcat01-95fc6cd5d-p85g6   1/1     Running   0          2d18h   10.244.1.12   node-1   <none>           <none>

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/httpd-svc    ClusterIP   10.1.142.189   <none>        80/TCP     2d2h    k8s-app=httpd01
service/tomcat-svc   ClusterIP   10.1.29.129    <none>        8080/TCP   2d18h   k8s-app=tomcat01

NAME                              CLASS                HOSTS            ADDRESS                         PORTS     AGE
ingress.extensions/test-ingress   test-ingress-class   www.test03.com   172.16.201.135,172.16.201.136   80, 443   44h


访问测试正常：
lex@lexliudeMacBook-Pro Downloads %curl  -I https://www.test03.com/tomcat-tls
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.

浏览器访问：https://www.test03.com/tomcat-tls
###### K8S #######



[root@master-1 ~]# kubectl apply -f  test-ingress.yaml
ingress.networking.k8s.io/test-ingress created

####多站点证书通配
  tls:
  - hosts:
      - "*.test04.com"
    secretName: tls-secret
  ingress_nameName: test-ingress-class
  rules:
  - host: "*.test04.com"
    http:
      paths:
      - pathType: Prefix
        path: /tomcat-tls
        backend:
          service:
            name: tomcat-svc
            port:
             number: 8080

[root@master-1 ~]# kubectl apply -f  test-ingress.yaml
ingress.networking.k8s.io/test-ingress configured


[root@master-1 https]#  kubectl get pod,svc,ing -n test-ns -o wide  
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                           READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/httpd01-699c8fcff4-bhn84   1/1     Running   0          2d3h    10.244.2.26   node-2   <none>           <none>
pod/tomcat01-95fc6cd5d-p85g6   1/1     Running   0          2d18h   10.244.1.12   node-1   <none>           <none>

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/httpd-svc    ClusterIP   10.1.142.189   <none>        80/TCP     2d3h    k8s-app=httpd01
service/tomcat-svc   ClusterIP   10.1.29.129    <none>        8080/TCP   2d18h   k8s-app=tomcat01

NAME                              CLASS                HOSTS          ADDRESS                         PORTS     AGE
ingress.extensions/test-ingress   test-ingress-class   *.test04.com   172.16.201.135,172.16.201.136   80, 443   18m


[root@master-1 https]# cat /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 master-1
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

172.16.201.135 www.test01.com
172.16.201.135 www.test02.com
172.16.201.135 www.test03.com
172.16.201.135 www.test04.com
172.16.201.135 i.test04.com


#####*.test04.com下面所有域名都可以访问
#####i.test04.com 和 www.test04.com 浏览器都可以访问


测试正常：
[root@master-1 https]# curl -k --cert tls.key -I https://i.test04.com/tomcat-tls
HTTP/1.1 200 
Date: Sun, 26 Sep 2021 06:43:18 GMT
Content-Type: text/html;charset=UTF-8
Connection: keep-alive
Set-Cookie: JSESSIONID=E72F11F2F54E0CD9B5FE2CF31915E4CE; Path=/; HttpOnly
Strict-Transport-Security: max-age=15724800; includeSubDomains


[root@master-1 https]# curl -k --cert tls.key -I https://www.test04.com/tomcat-tls
HTTP/1.1 200 
Date: Sun, 26 Sep 2021 06:43:41 GMT
Content-Type: text/html;charset=UTF-8
Connection: keep-alive
Set-Cookie: JSESSIONID=5C242F6A414CFF91907A431D86074053; Path=/; HttpOnly
Strict-Transport-Security: max-age=15724800; includeSubDomains


lex@lexliudeMacBook-Pro Downloads %curl  -I https://www.test04.com/tomcat-tls
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.



lex@lexliudeMacBook-Pro Downloads %curl  -I https://i.test04.com/tomcat-tls
curl: (60) SSL certificate problem: unable to get local issuer certificate
More details here: https://curl.haxx.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.


[root@master-1 ~]# kubectl get pod -A | grep 'ingre'
ingress-nginx   ingress-nginx-admission-create-4wp6d   0/1     Completed   0          46h
ingress-nginx   ingress-nginx-admission-patch-glk59    0/1     Completed   0          46h
ingress-nginx   ingress-nginx-controller-s7x2z         1/1     Running     0          46h
ingress-nginx   ingress-nginx-controller-xf68j         1/1     Running     0          46h


[root@master-1 ~]# kubectl exec -it -n ingress-nginx ingress-nginx-controller-s7x2z bash
bash-5.1$ cat /etc/nginx/nginx.conf|grep test
        ## start server *.test04.com
                server_name ~^(?<subdomain>[\w-]+)\.test04\.com$ ;
                        set $namespace      "test-ns";
                        set $ingress_name   "test-ingress";
                        set $proxy_upstream_name "test-ns-tomcat-svc-8080";
                        set $namespace      "test-ns";
                        set $ingress_name   "test-ingress";
        ## end server *.test04.com
bash-5.1$ 

###### 可见*.test04.com的配置


##(四)、部署 MySQL



###（一）NFS安装
######master节点安装nfs
[root@master-1 mysql]# yum -y install rpcbind nfs-utils
######创建nfs目录
[root@master-1 mysql]# mkdir /root/nfs_data
[root@master-1 mysql]# chmod -R 777 /root/nfs_data
######编辑export文件
[root@master-1 mysql]# vim /etc/exports
/root/nfs_data *(rw,no_root_squash,sync)
######配置生效
[root@master-1 mysql]# exportfs -r
######查看生效
[root@master-1 mysql]# exportfs
/root/nfs_data  <world>

######启动rpcbind、nfs服务
[root@master-1 mysql]# systemctl restart rpcbind && systemctl enable rpcbind
[root@master-1 mysql]# systemctl restart nfs && systemctl enable nfs
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
[root@master-1 mysql]# rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100024    1   udp  52230  status
    100024    1   tcp  37745  status
    100005    1   udp  20048  mountd
    100005    1   tcp  20048  mountd
    100005    2   udp  20048  mountd
    100005    2   tcp  20048  mountd
    100005    3   udp  20048  mountd
    100005    3   tcp  20048  mountd
    100003    3   tcp   2049  nfs
    100003    4   tcp   2049  nfs
    100227    3   tcp   2049  nfs_acl
    100003    3   udp   2049  nfs
    100003    4   udp   2049  nfs
    100227    3   udp   2049  nfs_acl
    100021    1   udp  38776  nlockmgr
    100021    3   udp  38776  nlockmgr
    100021    4   udp  38776  nlockmgr
    100021    1   tcp  35404  nlockmgr
    100021    3   tcp  35404  nlockmgr
    100021    4   tcp  35404  nlockmgr


######showmount测试
[root@master-1 mysql]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/root/nfs_data *

[root@node-2 ~]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/root/nfs_data *

[root@node-1 ~]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/root/nfs_data *


######所有节点安装客户端
[root@node-1 ~]# yum -y install nfs-utils;systemctl start nfs && systemctl enable nfs
[root@node-2 ~]# yum -y install nfs-utils;systemctl start nfs && systemctl enable nfs


######节点1挂上
[root@node-1 ~]# mkdir /root/nfs_data
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/nfs_data  /root/nfs_data
[root@node-1 nfs_data]# df -h|grep 172.16.201.134
172.16.201.134:/root/nfs_data   50G  4.7G   46G  10% /root/nfs_data

######节点2挂上
[root@node-2 ~]# mkdir /root/nfs_data
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/nfs_data  /root/nfs_data
[root@node-2 ~]# df -h|grep 172.16.201.134
172.16.201.134:/root/nfs_data   50G  4.7G   46G  10% /root/nfs_data

######节点测试写
[root@master-1 nfs_data]#  echo "test" > /root/nfs_data/1
[root@node-2 nfs_data]# cat 1
test
[root@node-1 nfs_data]# cat 1
test

###（二）部署PVC

####1、建立配置文件
######建PV
[root@master-1 mysql]# vim pv-mysql.yaml              
apiVersion: v1
kind: PersistentVolume
metadata:
  name: model-db-pv
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 5Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: model-db-pv-claim
    namespace: default
  persistentVolumeReclaimPolicy: Retain
  storageClassName: mysql-nfs-storage
  nfs:
    path: /root/nfs_data
    server: 172.16.201.134
  volumeMode: Filesystem

[root@master-1 mysql]# kubectl apply -f pv-mysql.yaml 
persistentvolume/model-db-pv created



######建SC
[root@master-1 mysql]# vim sc-mysql.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mysql-nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
[root@master-1 mysql]# kubectl apply -f sc-mysql.yaml
storageclass.storage.k8s.io/mysql-nfs-storage created

确保
[root@master-1 mysql]# kubectl get StorageClass
NAME                          PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
mysql-nfs-storage (default)   kubernetes.io/nfs   Retain          Immediate           false                  19s



######建PVC
[root@master-1 mysql]# vim pvc-mysql.yaml             
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-db-pv-claim
  namespace: default
  labels:
    app: model-mysql
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

[root@master-1 mysql]# kubectl apply -f pvc-mysql.yaml
persistentvolumeclaim/model-db-pv-claim created


######建ConfigMap
[root@master-1 mysql]# vim config-mysql.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: model-db-config
  namespace: default
  labels:
    app: model-db
data:
  my.cnf: |-
    [client]
    default-character-set=utf8mb4
    [mysql]
    default-character-set=utf8mb4
    [mysqld]
    character-set-server = utf8mb4
    collation-server = utf8mb4_unicode_ci
    init_connect='SET NAMES utf8mb4'
    skip-character-set-client-handshake = true
    max_connections=2000
    secure_file_priv=/var/lib/mysql
    bind-address=0.0.0.0
    symbolic-links=0
    sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'

[root@master-1 mysql]# kubectl apply -f config-mysql.yaml 
configmap/model-db-config created

######建Deployment
[root@master-1 mysql]# vim dep-mysql.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-db
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: model-mysql
  template:
    metadata:
      labels:
        app: model-mysql
    spec:
      containers:
      - args:
        - --datadir
        - /var/lib/mysql/datadir
        env:
          - name: MYSQL_ROOT_PASSWORD
            value: root
          - name: MYSQL_USER
            value: user
          - name: MYSQL_PASSWORD
            value: user
        image: mysql:5.6
        name: model-db-container
        ports:
        - containerPort: 3306
          name: dbapi
        volumeMounts:
        - mountPath: /var/lib/mysql
          name: model-db-storage
        - name: config
          mountPath: /etc/mysql/conf.d/my.cnf
          subPath: my.cnf
      volumes:
      - name: model-db-storage
        persistentVolumeClaim:
          claimName: model-db-pv-claim
      - name: config
        configMap:
          name: model-db-config
      - name: localtime
        hostPath:
          type: File
          path: /etc/localtime
                                                                                                  
[root@master-1 mysql]# kubectl apply -f dep-mysql.yaml 
deployment.apps/model-db created

######建SVC
[root@master-1 mysql]# vim svc-mysql.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: model-mysql
  name: model-db-svc
  namespace: default
spec:
  type: NodePort
  ports:
  - name: http
    port: 3306
    nodePort: 32306
    protocol: TCP
    targetPort: 3306
  selector:
    app: model-mysql

[root@master-1 mysql]# kubectl apply -f svc-mysql.yaml
service/model-db-svc created



######查看：
[root@master-1 mysql]# kubectl get pv,pvc,configmap -o wide        
NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/model-db-pv   5Gi        RWO            Retain           Bound    default/model-db-pv-claim   nfs                     79s   Filesystem

NAME                                      STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE   VOLUMEMODE
persistentvolumeclaim/model-db-pv-claim   Bound    model-db-pv   5Gi        RWO            mysql-nfs-storage   62s   Filesystem

NAME                        DATA   AGE
configmap/model-db-config   1      18s


######看日志，查问题
[root@master-1 mysql]# kubectl describe pvc 
Name:          data-mysql-0
Namespace:     default
StorageClass:  
Status:        Pending
Volume:        
Labels:        app=mysql
Annotations:   <none>
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      
Access Modes:  
VolumeMode:    Filesystem
Mounted By:    <none>
Events:
  Type    Reason         Age                   From                         Message
  ----    ------         ----                  ----                         -------
  Normal  FailedBinding  48s (x2216 over 19h)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set


Name:          model-db-pv-claim
Namespace:     default
StorageClass:  
Status:        Bound
Volume:        model-db-pv
Labels:        app=model-mysql
Annotations:   pv.kubernetes.io/bind-completed: yes
               pv.kubernetes.io/bound-by-controller: yes
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:      5Gi
Access Modes:  RWO
VolumeMode:    Filesystem
Mounted By:    model-db-6969cd69d9-9w97r
Events:        <none>




#####问题处理：网络上的文档缺少storageclass相关配置
问题no persistent volumes available for this claim and no storage class is set
[root@master-1 mysql]# vim sc-mysql.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mysql-nfs-storage
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
[root@master-1 mysql]# kubectl apply -f sc-mysql.yaml
storageclass.storage.k8s.io/mysql-nfs-storage created

######PVC配置引用上面的：storageClassName: mysql-nfs-storage
[root@master-1 mysql]# cat pvc-mysql.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-db-pv-claim
  namespace: default
  labels:
    app: model-mysql
spec:
  storageClassName: mysql-nfs-storage
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
######PVC配置引用上面的：storageClassName: mysql-nfs-storage


手动删除Pending的pvc，重新建立：
[root@master-1 mysql]# kubectl get pvc
NAME           STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-mysql-0   Pending                                                     20h
[root@master-1 mysql]# kubectl delete pvc data-mysql-0
persistentvolumeclaim "data-mysql-0" deleted

[root@master-1 mysql]# kubectl get pvc
No resources found in default namespace.
#####问题处理：网络上的文档缺少storageclass相关配置


######调试：
kubectl apply -f pv-mysql.yaml
kubectl apply -f pvc-mysql.yaml
kubectl apply -f config-mysql.yaml
kubectl apply -f dep-mysql.yaml
kubectl apply -f svc-mysql.yaml

kubectl delete -f pv-mysql.yaml
kubectl delete -f pvc-mysql.yaml
kubectl delete -f config-mysql.yaml
kubectl delete -f dep-mysql.yaml
kubectl delete -f svc-mysql.yaml




######全部执行完，查看状态
[root@master-1 mysql]# kubectl get pod,svc,pv,pvc,configmap -o wide  
NAME                            READY   STATUS    RESTARTS   AGE    IP            NODE     NOMINATED NODE   READINESS GATES
pod/model-db-6969cd69d9-5wlcc   1/1     Running   0          101s   10.244.2.37   node-2   <none>           <none>

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE   SELECTOR
service/kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          5d    <none>
service/model-db-svc   NodePort    10.1.155.128   <none>        3306:32306/TCP   8s    app=model-mysql

NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE     VOLUMEMODE
persistentvolume/model-db-pv   5Gi        RWO            Retain           Bound    default/model-db-pv-claim   nfs                     3m22s   Filesystem

NAME                                      STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE    VOLUMEMODE
persistentvolumeclaim/model-db-pv-claim   Bound    model-db-pv   5Gi        RWO            mysql-nfs-storage   3m5s   Filesystem

NAME                        DATA   AGE
configmap/model-db-config   1      2m21s



[root@master-1 mysql]# kubectl get all -n default
NAME                            READY   STATUS    RESTARTS   AGE
pod/model-db-6969cd69d9-5wlcc   1/1     Running   0          2m16s

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          5d
service/model-db-svc   NodePort    10.1.155.128   <none>        3306:32306/TCP   43s

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/model-db   1/1     1            1           2m16s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/model-db-6969cd69d9   1         1         1       2m16s
[root@master-1 mysql]# 

######所有节点端口已开
[root@master-1 mysql]#  netstat -antup|grep 32306
tcp        0      0 0.0.0.0:32306           0.0.0.0:*               LISTEN      2855/kube-proxy 


[root@node-1 datadir]# netstat -antup|grep 32306
tcp        0      0 0.0.0.0:32306           0.0.0.0:*               LISTEN      2585/kube-proxy    

[root@node-2 ~]# netstat -antup|grep 32306
tcp        0      0 0.0.0.0:32306           0.0.0.0:*               LISTEN      2551/kube-proxy     


######数据已有：
[root@node-1 datadir]# ll /root/nfs_data/datadir
total 110604
-rw-rw---- 1 polkitd input       56 Sep 27 12:08 auto.cnf
-rw-rw---- 1 polkitd input 12582912 Sep 27 12:08 ibdata1
-rw-rw---- 1 polkitd input 50331648 Sep 27 12:08 ib_logfile0
-rw-rw---- 1 polkitd input 50331648 Sep 27 12:08 ib_logfile1
drwx------ 2 polkitd input     4096 Sep 27 12:08 mysql
drwx------ 2 polkitd input     4096 Sep 27 12:08 performance_schema




#####测试mysql 随便哪个node都可以


安装客户端：
[root@master-1 mysql]#  rpm -ivh https://repo.mysql.com//mysql57-community-release-el7-11.noarch.rpm
Retrieving https://repo.mysql.com//mysql57-community-release-el7-11.noarch.rpm
warning: /var/tmp/rpm-tmp.IIXkn0: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:mysql57-community-release-el7-11 ################################# [100%]

[root@master-1 mysql]# yum install mysql-community-client


[root@master-1 mysql]# mysql -h172.16.201.134 -uroot -proot --port=32306 -e"show databases"
mysql: [Warning] Using a password on the command line interface can be insecure.
+--------------------+
| Database           |
+--------------------+
| information_schema |
| mysql              |
| performance_schema |
+--------------------+
[root@master-1 mysql]# 


数据库联通正常。






##(四)、部署 MySQL同步
#####1、StateFulSet简介
Deployment 实际上并不足以覆盖所有的应用编排问题。 如果一个应用的所有 Pod是完全一样的，它们之间没有启动顺序要求，也无所谓运行在哪台宿主机上。对于这样的应用，Deployment 可以随时通过 Pod 模板创建新的 Pod；不需要的时候，可以“杀掉”任意一个 Pod。

在实际的场景中，并不是所有的应用都可以满足这样的要求。

例如 Mysql集群、MongoDB集群、ZooKeeper集群等，它的多个实例之间，可能有依赖关系 (比如：主从关系); 还有就是数据存储，它的多个实例，可能会在本地磁盘上保存一份自己独有的数据。如果还使用RC或Deployment这类控制器进行管理, 那么这些实例一旦被杀掉，再次重建出来后，实例与数据之间的对应关系也已经丢失，从而导致应用失败。这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，被称为“有状态应用”。

StatefulSet 有如下一些特性：

StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设 StatefulSet 的名字叫 kafka，那么第1个Pod叫 kafka-0，第2个叫kafka-1，以此类推。
StatefulSet控制的Pod副本的启停顺序是受控的，创建一个Pod时，它的前1个Pod已经是运行且准备好的状态。
StatefulSet 里的 Pod 采用稳定的持久化存储卷，通过 PV/PVC 来实现，删除 Pod 时默认不会删除与StatefulSet 相关的存储卷（为了保证数据的安全）。
StatefulSet 除了要与PV卷捆绑使用以存储 Pod 的状态数据，还要与 Headless Service(无头服务) 配合使用。

Headless Service 与普通 Service 的区别在于，它没有 Cluster IP，如果解析 Headless Service 的 DNS 域名，则返回的是该 Service 对应的全部 Pod 的 Endpoint列表。

StatefulSet 在 Headless Service 的基础上又为 StatefulSet 控制的每个 Pod 实例创建了一个 DNS 域名，这个域名的格式为：<pod-name>.<service-name>

比如一个 3 节点的 kafka 的 StatefulSet 集群，对应的 Headless Service 的名字为 kafkasvc，StatefulSet 的名字为kafka，则 StatefulSet 里面的 3 个 Pod 的 DNS 名称分别为kafka-0.kafkasvc、kafka-1.kafkasvc、kafka-2.kafkasvc，这些DNS 名称可以直接在集群的配置文件中固定下来。

#####2、StateFulSet设计思路
StatefulSet 把应用状态抽象为了两种情况：

拓扑状态
应用的多个实例必须按照一定的顺序启动。

比如: 应用的主节点 A 要先于从节点 B 启动，如果把 A 和 B 两个 Pod 删除掉，它们再次被创建出来时也必须严格按照这个顺序才行; 同时新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。
存储状态

应用的多个实例分别绑定了不同的存储数据。
对于这些应用实例来说，Pod A 第一次读取到的数据，和重建后读取的数据应该是同一份
StatefulSet 的核心功能就是通过某种方式记录这些状态，在 Pod 被重建时，能够为新 Pod 恢复这些状态。



StatefulSet 对存储状态的管理，主要使用的是 PersistentVolume 的功能。

#####3、PV/PVC介绍
PersistentVolume（PV）是集群中由管理员配置的一段网络存储。 它是集群中的资源，就像节点是集群资源一样。 PV是容量插件，如Volumes，但其生命周期独立于使用PV的任何单个pod。 此API对象捕获存储实现的详细信息，包括NFS，iSCSI或特定于云提供程序的存储系统。

PersistentVolumeClaim（PVC）是由用户进行存储的请求。 它类似于pod。 Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以一次读/写 或多次只读）。

虽然PersistentVolumeClaims允许用户使用抽象存储资源，但是PersistentVolumes对于不同的问题，用户通常需要具有不同属性（例如性能）。群集管理员需要能够提供各种PersistentVolumes不同的方式，而不仅仅是大小和访问模式，而不会让用户了解这些卷的实现方式。对于这些需求，有StorageClass 资源。

StorageClass为管理员提供了一种描述他们提供的存储的“类”的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。

PVC和PV是一一对应的。

#####4、PV/PVC生命周期
PV是群集中的资源。PVC是对这些资源的请求，并且还充当对资源的检查。PV和PVC之间的相互作用遵循以下生命周期：Provisioning--> Binding--> Using--> Releasing--> Recycling

######4.1供应准备Provisioning --- 通过集群外的存储系统或者云平台来提供存储持久化支持。

静态提供Static：集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们 存在于Kubernetes API中，可用于消费
动态提供Dynamic：当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会 尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并 配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置。
######4.2绑定Binding---用户创建pvc并指定需要的资源和访问模式。

在找到可用pv之前，pvc会保持未绑定状态。

######4.3 使用Using---用户可在pod中像volume一样使用pvc。

######4.4 释放Releasing---用户删除pvc来回收存储资源

pv将变成“released”状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。

######4.5 回收Recycling---pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。

保留策略：允许人工处理保留的数据。
删除策略：将删除pv和外部关联的存储资源，需要插件支持。
回收策略：将执行清除操作，之后可以被新的pvc使用，需要插件支持。
目前 NFS和 HostPath类型卷支持回收策略，AWS EBS,GCE PD,Azure Disk和 Cinder支持删除(Delete)策略。

#####5、PV类型
- GCEPersistentDisk
- AWSElasticBlockStore
- AzureFile
- AzureDisk
- FC (Fibre Channel)
- Flexvolume
- Flocker
- NFS
- iSCSI
- RBD (Ceph Block Device)
- CephFS
- Cinder (OpenStack block storage)
- Glusterfs
- VsphereVolume
- Quobyte Volumes
- HostPath (Single node testing only – local storage is not supported in any way and WILL NOT WORK in a multi-node cluster)
- Portworx Volumes
- ScaleIO Volumes
- StorageOS
我们前面学习的 Volume 是定义在 Pod 上的，属于“计算资源”的一部分.

而“网络存储”则是相对独立于“计算资源”而存在的一种实体资源。比如在使用云主机的情况下，我们通常会先创建一个网络存储，然后从中划出一个“网盘”并挂载到云主机上。Persistent Volume（简称PV）和与之相关联的 Persistent Volume Claim（简称PVC）实现了类似的功能。

PV 与 Volume 的区别如下：

- PV 只能是网络存储，不属于任何 Node，但可以在每个 Node 上访问。
- PV 并不是定义在 Pod 上的，而是独立于 Pod 之外定义。

#####6、在Pod中使用PVC
有了PV/PVC这两个API, 我们就可以保证有状态应用的存储状态了. 上面的StatefulSet由两个Pod组成, 所以我们需要两份存储, 分别用于存储不同Pod上各自的数据.



部署详细说明：
######有状态服务抽取配置为ConfigMap
在之前我们使用Docker部署MySQL的时候也会将conf、logs、data等数据挂载到宿主机上，那么在k8s里面的话专门有一个空间是管理配置文件的也就是上面提到的ConfigMap，可以将一些常用的配置抽离出来做成ConfigMap配置，后来不管是MySQL挂了重启还是创建新的MySQL都可已使用同一个ConfigMap中的配置，这也就是第一点将有状态服的配置抽离到ConfigMap中来，这样后期修改配置就只需要更改ConfigMap就行了
######有状态服务必须使用PVC持久化数据
每个MySQL都会用自己的数据存储，那么在k8s中存在一个专门存储数据的空间，也就是上面提到的PVC，每一个MySQL都会分配一个PVC数据存储空间，或者共享一个PVC空间，也就是想Docker挂载出来的data目录一样，在Docker中的MySQL容器重启后MySQL中的数据还存在，那么k8s中的MySQL挂掉后重启后也会在PVC中找持久化的数据，那么这样就不会存在在其他节点拉起MySQL存在数据丢失的问题了
######服务集群内访问使用DNS提供稳定的域名
在上面图中存在一个主节点MySQL和两个从节点MySQL，在这个MySQL集群中个节点间是要相互通信访问的，这里实现各节点间通信访问的话就需要使用HeadlessService服务，这个服务就是集群中间相互访问的，在k8s中最小的部署单元是pod，如MySQL0就是一个pod，那么我们将这个pod包装成一个Service，同时让k8s为这个Service生成一个域名，DNS为这个服务提供一个稳定域名，如图上为MySQL0这个服务提供了mysql-0.test域名，其他的pod对外暴露服务后也会提供相应的域名，那么各节点通信就可以使用域名访问，这里提供域名的好处就是防止某台对外暴露的服务突然挂掉了，在别的机器上拉起，那么这时的ip就会发生改变，那么集群内的所有应用，包括服务自己都可以使用域名来访问.



#####开始部署：
######1、单独准备nfs：/root/web1
[root@node-1 web1]# df -h|grep 172.16.201.134
172.16.201.134:/root/nfs_data   50G  5.1G   45G  11% /root/nfs_data
172.16.201.134:/root/web1       50G  5.1G   45G  11% /root/web1


[root@master-1 ~]# exportfs
/root/nfs_data  <world>
/root/web1      <world>


######2、建立pv
[root@master-1 mysql]# vim r-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
spec:
  storageClassName: nfs      
  capacity:                  # PV容量
    storage: 1Gi
  accessModes:               # 访问模式
  - ReadWriteOnce
  nfs:
    path: /root/web1
    server: "172.16.201.134"
  volumeMode: Filesystem

#####==================================
PV 的 accessModes 属性有以下类型：
- ReadWriteOnce：读写权限、并且只能被单个 Node 挂载。
- ReadOnlyMany：只读权限、允许被多个 Node 挂载。
- ReadWriteMany：读写权限、允许被多个 Node 挂载。


kubectl apply -f r-pv.yaml
kubectl delete -f r-pv.yaml



[root@master-1 mysql]# kubectl get pv
NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                       STORAGECLASS   REASON   AGE
model-db-pv   5Gi        RWO            Retain           Bound       default/model-db-pv-claim   nfs                     5h21m
pv01          1Gi        RWO            Retain           Available                               nfs                     10s


######3、建立pvc
[root@master-1 mysql]# vim r-pvc.yaml 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc01
spec:
  storageClassName: nfs   # 只能绑定nfs类的PV
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi  

[root@master-1 mysql]# kubectl apply -f r-pvc.yaml
[root@master-1 mysql]# kubectl delete -f r-pvc.yaml

[root@master-1 mysql]# kubectl get pvc
NAME                STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE
model-db-pv-claim   Bound    model-db-pv   5Gi        RWO            mysql-nfs-storage   5h22m
pvc01               Bound    pv01          1Gi        RWO            nfs                 6s


kubectl delete pvc  mysqldata-mysql-0

######4、建立StatefulSet配置
kubectl delete -f r.yaml 
[root@master-1 mysql]# kubectl apply -f r.yaml 
secret/mysql created
configmap/mysql created
service/mysqlsvc created
statefulset.apps/mysql created



[root@master-1 mysql]# kubectl get pod,svc,pv,pvc,configmap -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/model-db-6969cd69d9-5wlcc   1/1     Running   8          5h25m   10.244.2.37   node-2   <none>           <none>
pod/mysql-0                     0/1     Pending   0          30s     <none>        <none>   <none>           <none>

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
service/kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          5d5h    <none>
service/model-db-svc   NodePort    10.1.155.128   <none>        3306:32306/TCP   5h23m   app=model-mysql
service/mysqlsvc       ClusterIP   None           <none>        3306/TCP         30s     app=mysql

NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE     VOLUMEMODE
persistentvolume/model-db-pv   5Gi        RWO            Retain           Bound    default/model-db-pv-claim   nfs                     5h26m   Filesystem
persistentvolume/pv01          1Gi        RWO            Retain           Bound    default/pvc01               nfs                     5m13s   Filesystem

NAME                                      STATUS    VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE     VOLUMEMODE
persistentvolumeclaim/model-db-pv-claim   Bound     model-db-pv   5Gi        RWO            mysql-nfs-storage   5h26m   Filesystem
persistentvolumeclaim/mysqldata-mysql-0   Pending                                           nfs                 30s     Filesystem
persistentvolumeclaim/pvc01               Bound     pv01          1Gi        RWO            nfs                 4m17s   Filesystem

NAME                        DATA   AGE
configmap/model-db-config   1      5h25m
configmap/mysql             2      30s


[root@master-1 mysql]# kubectl get events
LAST SEEN   TYPE      REASON               OBJECT                                    MESSAGE
82s         Warning   FailedScheduling     pod/mysql-0                               0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.
49m         Warning   FailedCreate         statefulset/mysql                         create Pod mysql-0 in StatefulSet mysql failed error: Pod "mysql-0" is invalid: spec.containers[0].volumeMounts[0].name: Not found: "mysqldata"
4m3s        Normal    SuccessfulCreate     statefulset/mysql                         create Claim mysqldata-mysql-0 Pod mysql-0 in StatefulSet mysql success
4m3s        Normal    SuccessfulCreate     statefulset/mysql                         create Pod mysql-0 in StatefulSet mysql successful
14s         Warning   ProvisioningFailed   persistentvolumeclaim/mysqldata-mysql-0   no provisionable volume plugin matched




[root@master-1 mysql]# cat r.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: mysql
  namespace: default
type: Opaque
data:
  root_pass: bGVlZG9u               # 用base64转码后的字符
  rep_user: c2xhdmU=
  rep_pass: c2xhdmVwYXNz
  
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  namespace: default
data:
  master.cnf: |                     # 配置文件自行编写，这里我用于测试只写了最简单的
    [mysqld]
    datadir = /var/lib/mysql
    server_id = 0
    log_bin = binlog
  slave.cnf: |
    [mysqld]
    datadir = /var/lib/mysql
    server_id = 0

---

apiVersion: v1
kind: Service
metadata:
  name: mysqlsvc
  labels:
    app: mysql
spec:
  selector:
    app: mysql
  clusterIP: None                  # Headless Service
  ports:
  - name: mysql
    port: 3306

---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysqlsvc 
  replicas: 2                     # 3副本就是一主二从
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: registry.cn-shenzhen.aliyuncs.com/leedon/mysql-5.6
        imagePullPolicy: IfNotPresent
        env:
        - name: SVC_NAME           # 这里一定要跟service名称一致
          value: mysqlsvc               
        - name: SLAVE_HOSTS
          value: mysql-_.mysqlsvc%
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: root_pass
        - name: REPLIC_USER
          valueFrom:
            secretKeyRef:
              name: mysql
              key: rep_user
        - name: REPLIC_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: rep_pass
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
          - name: mysqldata               #使用pvc：mysqldata
            mountPath: /var/lib/mysql
          - name: configfile
            mountPath: /etc/mysql
     volumes:
      - name: configfile
        configMap:
          name: mysql
          items:
          - key: master.cnf
            path: master.cnf
          - key: slave.cnf
            path: slave.cnf
      - name: mysqldata                  #定义pvc名字，配置
        persistentVolumeClaim:  
           claimName: pvc01
[root@master-1 mysql]# 


######5、维护脚本
[root@master-1 mysql]# cat ra
kubectl apply -f r-sc.yaml
kubectl apply -f r-pv.yaml
kubectl apply -f r-pvc.yaml
kubectl apply -f r.yaml
sleep 2
echo "============== kubectl get pod,svc,pv,pvc,sc -o wide ================="
kubectl get pod,svc,pv,pvc,sc -o wide

[root@master-1 mysql]# kubectl get pod,svc,pv,pvc,sc -o wide
NAME          READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
pod/mysql-0   1/1     Running   0          75s   10.244.2.40   node-2   <none>           <none>
pod/mysql-1   1/1     Running   0          73s   10.244.1.36   node-1   <none>           <none>

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP    5d10h   <none>
service/mysqlsvc     ClusterIP   None         <none>        3306/TCP   6m4s    app=mysql

NAME                    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM           STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv01   1Gi        RWO            Retain           Bound    default/pvc01   nfs                     25m   Filesystem

NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc01   Bound    pv01     1Gi        RWO            nfs            25m   Filesystem

NAME                                        PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/nfs (default)   kubernetes.io/nfs   Retain          Immediate           false                  25m


#---------------------








######注：用之前pvc:model-db-pv-claim测试
[root@master-1 mysql]# cat r.yaml 
---
apiVersion: v1
kind: Secret
metadata:
  name: mysql
  namespace: default
type: Opaque
data:
  root_pass: bGVlZG9u               # 用base64转码后的字符
  rep_user: c2xhdmU=
  rep_pass: c2xhdmVwYXNz
  
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  namespace: default
data:
  master.cnf: |                     # 配置文件自行编写，这里我用于测试只写了最简单的
    [mysqld]
    datadir = /var/lib/mysql
    server_id = 0
    log_bin = binlog
  slave.cnf: |
    [mysqld]
    datadir = /var/lib/mysql
    server_id = 0

---

apiVersion: v1
kind: Service
metadata:
  name: mysqlsvc
  labels:
    app: mysql
spec:
  selector:
    app: mysql
  clusterIP: None                  # Headless Service
  ports:
  - name: mysql
    port: 3306

---

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysqlsvc 
  replicas: 2                     # 3副本就是一主二从
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: registry.cn-shenzhen.aliyuncs.com/leedon/mysql-5.6
        imagePullPolicy: IfNotPresent
        env:
        - name: SVC_NAME           # 这里一定要跟service名称一致
          value: mysqlsvc               
        - name: SLAVE_HOSTS
          value: mysql-_.mysqlsvc%
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: root_pass
        - name: REPLIC_USER
          valueFrom:
            secretKeyRef:
              name: mysql
              key: rep_user
        - name: REPLIC_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql
              key: rep_pass
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
          - name: model-db-storage
            mountPath: /var/lib/mysql
          - name: configfile
            mountPath: /etc/mysql
      volumes:
      - name: model-db-storage
        persistentVolumeClaim:
          claimName: model-db-pv-claim
      - name: configfile
        configMap:
          name: mysql
          items:
          - key: master.cnf
            path: master.cnf
          - key: slave.cnf
            path: slave.cnf

[root@master-1 mysql]# 

###考虑可以不能用nfs，因为多个库同步，不能共用一份数据，挂本地卷即可。



[root@master-1 mysql]# kubectl get pod,svc,pv,pvc,configmap -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/model-db-6969cd69d9-5wlcc   0/1     Error     1          4h48m   10.244.2.37   node-2   <none>           <none>
pod/mysql-0                     1/1     Running   0          3m35s   10.244.1.33   node-1   <none>           <none>
pod/mysql-1                     1/1     Running   1          3m32s   10.244.2.38   node-2   <none>           <none>
pod/mysql-2                     1/1     Running   0          67s     10.244.1.34   node-1   <none>           <none>

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
service/kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          5d4h    <none>
service/model-db-svc   NodePort    10.1.155.128   <none>        3306:32306/TCP   4h47m   app=model-mysql
service/mysqlsvc       ClusterIP   None           <none>        3306/TCP         3m35s   app=mysql

NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE     VOLUMEMODE
persistentvolume/model-db-pv   5Gi        RWO            Retain           Bound    default/model-db-pv-claim   nfs                     4h50m   Filesystem

NAME                                      STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE     VOLUMEMODE
persistentvolumeclaim/model-db-pv-claim   Bound    model-db-pv   5Gi        RWO            mysql-nfs-storage   4h50m   Filesystem

NAME                        DATA   AGE
configmap/model-db-config   1      4h49m
configmap/mysql             2      3m36s
[root@master-1 mysql]# 



[root@master-1 mysql]# kubectl get pod,svc,pv,pvc,configmap -o wide
NAME                            READY   STATUS             RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/model-db-6969cd69d9-5wlcc   0/1     CrashLoopBackOff   4          4h52m   10.244.2.37   node-2   <none>           <none>
pod/mysql-0                     1/1     Running            0          6m59s   10.244.1.33   node-1   <none>           <none>
pod/mysql-1                     1/1     Running            2          6m56s   10.244.2.38   node-2   <none>           <none>
pod/mysql-2                     1/1     Terminating        0          4m31s   10.244.1.34   node-1   <none>           <none>

NAME                   TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
service/kubernetes     ClusterIP   10.1.0.1       <none>        443/TCP          5d4h    <none>
service/model-db-svc   NodePort    10.1.155.128   <none>        3306:32306/TCP   4h50m   app=model-mysql
service/mysqlsvc       ClusterIP   None           <none>        3306/TCP         6m59s   app=mysql

NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                       STORAGECLASS   REASON   AGE     VOLUMEMODE
persistentvolume/model-db-pv   5Gi        RWO            Retain           Bound    default/model-db-pv-claim   nfs                     4h53m   Filesystem

NAME                                      STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS        AGE     VOLUMEMODE
persistentvolumeclaim/model-db-pv-claim   Bound    model-db-pv   5Gi        RWO            mysql-nfs-storage   4h53m   Filesystem

NAME                        DATA   AGE
configmap/model-db-config   1      4h52m
configmap/mysql             2      6m59s


查看新的 Pod 的运行情况，相当于tail
kubectl get pods -l app=mysql --watch


##其他部署版本一(未成功)：
1、nfs准备：
[root@master-1 mysql-replicationdata]# exportfs |grep mysql
/root/mysql-replicationdata

[root@node-1 ~]# df -h|grep mysql
172.16.201.134:/root/mysql-replicationdata                                                                                     50G  5.3G   45G  11% /root/mysql-replicationdata

[root@node-2 ~]# df -h|grep mysql
172.16.201.134:/root/mysql-replicationdata                                                                                     50G  5.3G   45G  11% /root/mysql-replicationdata


2、部署storageclass
[root@master-1 mysql-kubernetes]# kubectl apply -f storageclass.yaml
storageclass.storage.k8s.io/nfs created
[root@master-1 mysql-kubernetes]# kubectl get storageclass
NAME          PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
mongodb-nfs   asd                      Delete          Immediate           false                  20h
nfs           nfs-client-provisioner   Delete          Immediate           false                  4s

3、部署nfs-client-provisioner
[root@master-1 mysql-kubernetes]# kubectl apply -f nfs-client-provisioner.yaml
serviceaccount/nfs-client-provisioner unchanged
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner configured
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner unchanged
deployment.apps/nfs-client-provisioner created
storageclass.storage.k8s.io/nfs-client-provisioner created


[root@master-1 mysql-kubernetes]# kubectl get pod -n kube-system              
NAME                                      READY   STATUS    RESTARTS   AGE
coredns-6d56c8448f-9dr27                  1/1     Running   2          16d
coredns-6d56c8448f-mfn9z                  1/1     Running   2          16d
etcd-master-1                             1/1     Running   4          23d
kube-apiserver-master-1                   1/1     Running   0          20h
kube-controller-manager-master-1          1/1     Running   11         23d
kube-flannel-ds-mmhsm                     1/1     Running   1          23d
kube-flannel-ds-rz2xj                     1/1     Running   4          23d
kube-flannel-ds-ts9fm                     1/1     Running   1          23d
kube-proxy-bkck2                          1/1     Running   1          23d
kube-proxy-c6fdx                          1/1     Running   4          23d
kube-proxy-phjdh                          1/1     Running   1          23d
kube-scheduler-master-1                   1/1     Running   13         23d
mongodb-nfs-5f6fd65ff9-dm7wv              1/1     Running   0          20h
nfs-client-provisioner-6f4769697d-dc9hc   1/1     Running   0          2s


kubectl delete -f nfs-client-provisioner.yaml     

无法下载景象：
[root@master-1 mysql-kubernetes]#kubectl describe pod nfs-client-provisioner-665468796-jnrz5  -n kube-system
Events:
  Type     Reason     Age   From               Message
  ----     ------     ----  ----               -------
  Normal   Scheduled  37s   default-scheduler  Successfully assigned kube-system/nfs-client-provisioner-665468796-jnrz5 to node-2
  Normal   Pulling    35s   kubelet            Pulling image "nfs-client-provisioner"
  Warning  Failed     9s    kubelet            Failed to pull image "nfs-client-provisioner": rpc error: code = Unknown desc = Error response from daemon: pull access denied for nfs-client-provisioner, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
  Warning  Failed     9s    kubelet            Error: ErrImagePull
  Normal   BackOff    9s    kubelet            Back-off pulling image "nfs-client-provisioner"
  Warning  Failed     9s    kubelet            Error: ImagePullBackOff


解决：手动下载，然后填写地址：image: docker.io/vbouchaud/nfs-client-provisioner:latest
[root@node-2 ~]#  docker pull vbouchaud/nfs-client-provisioner 
Using default tag: latest
latest: Pulling from vbouchaud/nfs-client-provisioner
a0d0a0d46f8b: Already exists 
08e4f7172a4c: Pull complete 
b369a37ae378: Pull complete 
Digest: sha256:fc6b7f5d62463ff0d0a09487fd822b25adbfb2dc7d2b67f01025db30f38560c0
Status: Downloaded newer image for vbouchaud/nfs-client-provisioner:latest
docker.io/vbouchaud/nfs-client-provisioner:latest #此处是景象地址
[root@node-2 ~]# 


[root@master-1 mysql-kubernetes]# vim nfs-client-provisioner.yaml 
.....
        - name: nfs-client-provisioner
          image: docker.io/vbouchaud/nfs-client-provisioner:latest
          imagePullPolicy: IfNotPresent
.....

再看，已经running了
[root@master-1 mysql-kubernetes]# kubectl get pod -n kube-system              
NAME                                      READY   STATUS    RESTARTS   AGE
coredns-6d56c8448f-9dr27                  1/1     Running   2          16d
coredns-6d56c8448f-mfn9z                  1/1     Running   2          16d
etcd-master-1                             1/1     Running   4          23d
kube-apiserver-master-1                   1/1     Running   0          20h
kube-controller-manager-master-1          1/1     Running   11         23d
kube-flannel-ds-mmhsm                     1/1     Running   1          23d
kube-flannel-ds-rz2xj                     1/1     Running   4          23d
kube-flannel-ds-ts9fm                     1/1     Running   1          23d
kube-proxy-bkck2                          1/1     Running   1          23d
kube-proxy-c6fdx                          1/1     Running   4          23d
kube-proxy-phjdh                          1/1     Running   1          23d
kube-scheduler-master-1                   1/1     Running   13         23d
mongodb-nfs-5f6fd65ff9-dm7wv              1/1     Running   0          20h
nfs-client-provisioner-6f4769697d-dc9hc   1/1     Running   0          2s

4、创建用于Mysql数据持久化的pvc
[root@master-1 mysql-kubernetes]# kubectl apply -f mysql-pvc.yaml
persistentvolumeclaim/data-mysql-0 created
persistentvolumeclaim/data-mysql-1 created
persistentvolumeclaim/data-mysql-2 created
[root@master-1 mysql-kubernetes]#

[root@master-1 mysql-kubernetes]# kubectl get pv,pvc|grep mysql
persistentvolume/pvc-33eae2dd-292f-41a2-9720-d86a57b3b6c0   500Mi      RWX            Delete           Bound    default/data-mysql-0                                                      nfs-client-provisioner                  90s
persistentvolume/pvc-88ac9409-2cb7-4aa0-a752-a4bf6c02993a   500Mi      RWX            Delete           Bound    default/data-mysql-2                                                      nfs-client-provisioner                  90s
persistentvolume/pvc-d2de1f89-408a-4e0c-973b-7c0745098500   500Mi      RWX            Delete           Bound    default/data-mysql-1                                                      nfs-client-provisioner                  90s
persistentvolumeclaim/data-mysql-0                               Bound    pvc-33eae2dd-292f-41a2-9720-d86a57b3b6c0   500Mi      RWX            nfs-client-provisioner   90s
persistentvolumeclaim/data-mysql-1                               Bound    pvc-d2de1f89-408a-4e0c-973b-7c0745098500   500Mi      RWX            nfs-client-provisioner   90s
persistentvolumeclaim/data-mysql-2                               Bound    pvc-88ac9409-2cb7-4aa0-a752-a4bf6c02993a   500Mi      RWX            nfs-client-provisioner   90s


5、创建configmap
[root@master-1 mysql-kubernetes]# kubectl apply -f mysql-configmap.yaml
configmap/mysql created
[root@master-1 mysql-kubernetes]# kubectl get cm
NAME            DATA   AGE
mysql           2      12s
redis-cluster   2      14d
[root@master-1 mysql-kubernetes]# 

6、创建service
[root@master-1 mysql-kubernetes]# kubectl apply -f mysql-services.yaml
service/mysql created
service/mysql-read created
[root@master-1 mysql-kubernetes]# kubectl get svc
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
kubernetes      ClusterIP   10.1.0.1       <none>        443/TCP                          23d
mongo           ClusterIP   None           <none>        27017/TCP                        20h
mongo-service   NodePort    10.1.98.245    <none>        27017:27017/TCP                  20h
mysql           NodePort    10.1.87.119    <none>        3306:27567/TCP                   3s
mysql-read      NodePort    10.1.111.206   <none>        3306:15454/TCP                   3s
redis-cluster   NodePort    10.1.231.82    <none>        6379:31000/TCP,16379:30701/TCP   7d3h

7、创建mysql-statefulset
[root@master-1 mysql-kubernetes]# kubectl apply -f mysql-statefulset.yaml
statefulset.apps/mysql created
[root@master-1 mysql-kubernetes]#

[root@master-1 mysql-kubernetes]# kubectl get sts
NAME            READY   AGE
mongo           6/6     18h
mysql           0/3     9s
redis-cluster   6/6     14d


kubectl delete -f mysql-statefulset.yaml;kubectl get po -o wide   
kubectl describe pod mysql-0  -n kube-system
docker pull xtrabackup:1.0



找个景象：xtrabackup
gcr.io/google-samples/xtrabackup:1.0 

保证本机有xtrabackup
[root@node-1 mysql-replicationdata]# docker pull gcr.io/google-samples/xtrabackup:1.0
1.0: Pulling from google-samples/xtrabackup
386a066cd84a: Pull complete 
40f175e652e1: Pull complete 
5eb6b5905d55: Pull complete 
Digest: sha256:29354f70c9d9207e757a1bae6a4cbf2f57a56b18fe5c2b0acc1198a053b24b38
Status: Downloaded newer image for gcr.io/google-samples/xtrabackup:1.0
gcr.io/google-samples/xtrabackup:1.0

[root@node-1 mysql-replicationdata]# docker images|grep xtrabackup
gcr.io/google-samples/xtrabackup                         1.0                 c415dbd7af07        4 years ago         265MB
[root@node-1 mysql-replicationdata]# 


[root@node-2 ~]# docker pull gcr.io/google-samples/xtrabackup:1.0 
1.0: Pulling from google-samples/xtrabackup
386a066cd84a: Pull complete 
40f175e652e1: Pull complete 
5eb6b5905d55: Pull complete 
Digest: sha256:29354f70c9d9207e757a1bae6a4cbf2f57a56b18fe5c2b0acc1198a053b24b38
Status: Downloaded newer image for gcr.io/google-samples/xtrabackup:1.0
gcr.io/google-samples/xtrabackup:1.0

[root@node-2 ~]# docker images|grep xtrabackup
gcr.io/google-samples/xtrabackup                                    1.0                 c415dbd7af07        4 years ago         265MB
[root@node-2 ~]# 


kubectl get po -o wide --watch
kubectl logs -f mysql-rs-0
kubectl get events
kubectl logs --tail 100 -p mysql-rs-0
kubectl describe pod mysql-rs-0



必须为pod mysql-0指定一个容器名称，选择[mysql xtrabackup]中的一个，或选择：中的一个或一个init容器：[init mysql clone mysql]
a container name must be specified for pod mysql-0, choose one of: [mysql xtrabackup]，choose one of: or one of the init containers：[init-mysql clone-mysql]


PV的访问模式（accessModes <[]string>）
ReadWriteOnce --- 单路读写，显示简写：RWO
ReadOnlyMany --- 多路只读，显示简写：ROX
ReadWriteMany --- 多路读写，显示简写：RWX



  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
[root@master-1 mysql-kubernetes]# kubectl get  pv|grep mysql
pvc-33eae2dd-292f-41a2-9720-d86a57b3b6c0   500Mi      RWX            Delete           Bound    default/data-mysql-0                                                      nfs-client-provisioner                  81m
pvc-88ac9409-2cb7-4aa0-a752-a4bf6c02993a   500Mi      RWX            Delete           Bound    default/data-mysql-2                                                      nfs-client-provisioner                  81m
pvc-d2de1f89-408a-4e0c-973b-7c0745098500   500Mi      RWX            Delete           Bound    default/data-mysql-1                                                      nfs-client-provisioner                  81m
[root@master-1 mysql-kubernetes]# kubectl get  pvc|grep mysql
data-mysql-0                               Bound     pvc-33eae2dd-292f-41a2-9720-d86a57b3b6c0   500Mi      RWX            nfs-client-provisioner   81m
data-mysql-1                               Bound     pvc-d2de1f89-408a-4e0c-973b-7c0745098500   500Mi      RWX            nfs-client-provisioner   81m
data-mysql-2                               Bound     pvc-88ac9409-2cb7-4aa0-a752-a4bf6c02993a   500Mi      RWX            nfs-client-provisioner   81m
data-mysql-rs-0                            Pending                                                                                                 50m

PV的回收策略（persistentVolumeReclaimPolicy：
如果某个已经绑定PV的PVC被删除了，那么这个PV将被回收，回收策略有以下三种：
Retain --- 默认，表示保留PV中存在的数据，在PV下次被PVC绑定时，数据还存在；通常都使用这种回收策略。
Recycle --- 回收，表示将删除PV里面的数据，并将其状态置为空闲状态，供其他PVC去绑定；危险！
Delete --- 删除，表示如果PVC删除了，那么PV也随之删除，数据会被清除。危险！



[root@master-1 mysql-kubernetes]# kubectl delete pvc  data-mysql-rs-0
persistentvolumeclaim "data-mysql-rs-0" deleted

kubectl delete pvc data-mysql-rs-1
kubectl delete pvc data-mysql-rs-0
kubectl delete pv pvc-8f258bef-fe94-43bd-aa70-8de2007a251b
kubectl delete pv pvc-a8fc2e49-f33a-4f87-9508-6c8d6ca8f13a
kubectl get pod,pv,pvc -o wide|grep mysql



#MySQL 主从同步-其他部署版本二(成功)：
参考：
https://blog.csdn.net/qq_38900565/article/details/102486100
https://www.jianshu.com/p/a5e184188ccd


###1.前提：起码得有个已经可以部署简单pod的k8s单机或集群
PS: 本文使用静态存储卷实现，非使用存储类
###2.制作拉取gcr.io镜像源脚本并拉取镜像
[root@master-1 k8s-mysql-replication]# ls
pull-google.com.sh
[root@master-1 k8s-mysql-replication]# ./pull-google.com.sh gcr.io/google-samples/xtrabackup:1.0
gcr.io/google-samples/xtrabackup:1.0
docker pull anjia0532/google-samples.xtrabackup:1.0
1.0: Pulling from anjia0532/google-samples.xtrabackup
Digest: sha256:39f106eb400e18dcb4bded651a7ab308b39c305578ce228ae35f3c76bc715510
Status: Downloaded newer image for anjia0532/google-samples.xtrabackup:1.0
docker.io/anjia0532/google-samples.xtrabackup:1.0
docker tag anjia0532/google-samples.xtrabackup:1.0 gcr.io/google-samples/xtrabackup:1.0
[root@master-1 k8s-mysql-replication]# 



###3.准备NFS服务,查看NFS服务器IP为172.16.201.134，准备三个持久化磁盘
[root@master-1 k8s-mysql-replication]# mkdir -p /net/mysql-0 /net/mysql-1 /net/mysql-2

[root@master-1 k8s-mysql-replication]# ifconfig |grep 172
        inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255
        inet 172.16.201.134  netmask 255.255.255.0  broadcast 172.16.2

[root@master-1 mysql-kubernetes]# echo '/net/mysql-0 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 mysql-kubernetes]# echo '/net/mysql-1 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 mysql-kubernetes]# echo '/net/mysql-2 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 mysql-kubernetes]# systemctl restart nfs-server

[root@master-1 ~]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/net/mysql-2                *
/net/mysql-1                *
/net/mysql-0                *
/root/mysql-replicationdata *
/root/mongodb-share         *
/root/redis-cluster/pv6     *
/root/redis-cluster/pv5     *
/root/redis-cluster/pv4     *
/root/redis-cluster/pv3     *
/root/redis-cluster/pv2     *
/root/redis-cluster/pv1     *
/root/redis-sentinel/2      *
/root/redis-sentinel/1      *
/root/redis-sentinel/0      *
/root/web1                  *
/root/nfs_data              *
[root@master-1 ~]# 

[root@node-1 ~]# mount -t nfs 172.16.201.134:/net/mysql-0 /net/mysql-0 
[root@node-1 ~]# mount -t nfs 172.16.201.134:/net/mysql-1 /net/mysql-1
[root@node-1 ~]# mount -t nfs 172.16.201.134:/net/mysql-2 /net/mysql-2
[root@node-1 ~]# df -h|grep 134|grep mysql                              
172.16.201.134:/net/mysql-0                                                                                                    50G  6.6G   44G  14% /net/mysql-0
172.16.201.134:/net/mysql-1                                                                                                    50G  6.6G   44G  14% /net/mysql-1
172.16.201.134:/net/mysql-2                                                                                                    50G  6.6G   44G  14% /net/mysql-2
[root@node-1 ~]#

[root@node-2 ~]#  mount -t nfs 172.16.201.134:/net/mysql-0 /net/mysql-0 
[root@node-2 ~]#  mount -t nfs 172.16.201.134:/net/mysql-1 /net/mysql-1
[root@node-2 ~]#  mount -t nfs 172.16.201.134:/net/mysql-2 /net/mysql-2
[root@node-2 ~]# df -h|grep 134|grep mysql        
172.16.201.134:/net/mysql-0                                                                                                    50G  6.6G   44G  14% /net/mysql-0
172.16.201.134:/net/mysql-1                                                                                                    50G  6.6G   44G  14% /net/mysql-1
172.16.201.134:/net/mysql-2                                                                                                    50G  6.6G   44G  14% /net/mysql-2
[root@node-2 ~]# 


###4.创建三个持久卷
 [root@master-1 k8s-mysql-replication]# cat pv.yaml 
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-a
   spec:
     capacity:
       storage: 1Gi
     accessModes: 
     - ReadWriteOnce
     - ReadOnlyMany
     persistentVolumeReclaimPolicy: Recycle # 当声明被释放，空间将回收再利用
     nfs:
       server: 172.16.201.134
       path: /net/mysql-0

 ---

   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-b
   spec:
     capacity:
       storage: 1Gi
     accessModes: 
     - ReadWriteOnce
     - ReadOnlyMany
     persistentVolumeReclaimPolicy: Recycle # 当声明被释放，空间将回收再利用
     nfs:
       server: 172.16.201.134
       path: /net/mysql-1

 ---

   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: pv-c
   spec:
     capacity:
       storage: 1Gi
     accessModes: 
     - ReadWriteOnce
     - ReadOnlyMany
     persistentVolumeReclaimPolicy: Recycle # 当声明被释放，空间将回收再利用
     nfs:
       server: 172.16.201.134
       path: /net/mysql-2
 [root@master-1 k8s-mysql-replication]#

[root@master-1 k8s-mysql-replication]# kubectl apply -f pv.yaml
persistentvolume/pv-a created
persistentvolume/pv-b created
persistentvolume/pv-c created

[root@master-1 k8s-mysql-replication]#  kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                     STORAGECLASS                   REASON   AGE
pv-a                                       1Gi        RWO,ROX        Recycle          Available                                                                                                                     13s
pv-b                                       1Gi        RWO,ROX        Recycle          Available                                                                                                                     13s
pv-c                                       1Gi        RWO,ROX        Recycle          Available                                                                                                                     13s


pv一直处于terminating

[root@master-1 k8s-mysql-replication]# kubectl patch pvc data-mysql-ss-0 -p '{"metadata":{"finalizers":null}}'
persistentvolumeclaim/data-mysql-ss-0 patched
[root@master-1 k8s-mysql-replication]#  kubectl delete pvc data-mysql-ss-0
persistentvolumeclaim "data-mysql-ss-0" deleted

kubectl delete -f mysql-statefulset.yaml;kubectl get po -o wide   
kubectl describe pod mysql-0  -n kube-system




###5.创建configMap配置字典
[root@master-1 k8s-mysql-replication]# cat mp
cat: mp: No such file or directory
[root@master-1 k8s-mysql-replication]# cat cm.yaml 
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    [mysqld]
    log-bin
  slave.cnf: |
    [mysqld]
    super-read-only
[root@master-1 k8s-mysql-replication]# 
[root@master-1 k8s-mysql-replication]# kubectl apply -f cm.yaml
[root@master-1 k8s-mysql-replication]# kubectl get cm
NAME            DATA   AGE
mysql           2      10s


###6.部署headless服务，有状态服务都需要，让服务旗下的Pod彼此发现
[root@master-1 k8s-mysql-replication]# cat service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
[root@master-1 k8s-mysql-replication]# 

[root@master-1 k8s-mysql-replication]# kubectl apply -f service.yaml
service/mysql-headless created
[root@master-1 k8s-mysql-replication]# kubectl get svc
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                          AGE
kubernetes       ClusterIP   10.1.0.1      <none>        443/TCP                          32d
mongo            ClusterIP   None          <none>        27017/TCP                        10d
mongo-service    NodePort    10.1.98.245   <none>        27017:27017/TCP                  10d
mysql-headless   ClusterIP   None          <none>        3306/TCP                         5s
redis-cluster    NodePort    10.1.231.82   <none>        6379:31000/TCP,16379:30701/TCP   17d


###7. 部署SatefulSet应用
[root@master-1 k8s-mysql-replication]# cat sfs.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-ss
spec: 
  selector: 
    matchLabels: 
      app: mysql 
  serviceName: mysql-headless
  replicas: 3
  template: 
    metadata:
      labels:
        app: mysql 
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set ex
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          ncat --recv-only mysql-ss-$(($ordinal-1)).mysql-headless 3307 | xbstream -x -C /var/lib/mysql
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          if [[ -s xtrabackup_slave_info ]]; then
            mv xtrabackup_slave_info change_master_to.sql.in
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then         
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi     
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
            echo "Initializing replication from clone position"
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-ss-0.mysql-headless',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 0.1Gi

[root@master-1 k8s-mysql-replication]#
[root@master-1 k8s-mysql-replication]# kubectl apply -f sfs.yaml
SatefulSet created

查看状态
[root@master-1 k8s-mysql-replication]# kubectl get po -o wide --watch   |grep mysql
mysql-ss-0        2/2     Running    0          2m21s   10.244.0.14   master-1   <none>           <none>
mysql-ss-1        2/2     Running    1          118s    10.244.1.77   node-1     <none>           <none>
mysql-ss-2        0/2     Init:0/2   0          87s     <none>        node-2     <none>           <none>
mysql-ss-2        0/2     Init:0/2   0          92s     10.244.2.79   node-2     <none>           <none>
mysql-ss-2        0/2     Init:1/2   0          93s     10.244.2.79   node-2     <none>           <none>
mysql-ss-2        0/2     Init:1/2   0          94s     10.244.2.79   node-2     <none>           <none>
mysql-ss-2        0/2     PodInitializing   0          113s    10.244.2.79   node-2     <none>           <none>
mysql-ss-2        1/2     Running           0          118s    10.244.2.79   node-2     <none>           <none>
mysql-ss-2        2/2     Running           0          2m7s    10.244.2.79   node-2     <none>           <none>

###8.查看状态
[root@master-1 k8s-mysql-replication]# kubectl get po -o wide  |grep mysql         
mysql-ss-0        2/2     Running   0          4m39s   10.244.0.14   master-1   <none>           <none>
mysql-ss-1        2/2     Running   1          4m16s   10.244.1.77   node-1     <none>           <none>
mysql-ss-2        2/2     Running   0          3m45s   10.244.2.79   node-2     <none>           <none>


[root@master-1 k8s-mysql-replication]# kubectl get all | grep mysql
pod/mysql-ss-0        2/2     Running   0          3m29s
pod/mysql-ss-1        2/2     Running   1          3m6s
pod/mysql-ss-2        2/2     Running   0          2m35s
service/mysql-headless   ClusterIP   None          <none>        3306/TCP                         3m42s
statefulset.apps/mysql-ss        3/3     3m29s

查看节点数据：
[root@node-1 net]# pwd
/net
[root@node-1 net]# ll 
total 0
drwxr-xr-x 3 root root 19 Oct 25 10:57 mysql-0
drwxr-xr-x 3 root root 19 Oct 25 10:57 mysql-1
drwxr-xr-x 3 root root 19 Oct 25 10:59 mysql-2
[root@node-1 net]# ll *
mysql-0:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql

mysql-1:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql

mysql-2:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql




######附件配置说明：
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-ss
spec: 
  selector: 
    matchLabels: 
      app: mysql 
  serviceName: mysql-headless
  replicas: 3
  template: 
    metadata:
      labels:
        app: mysql 
    spec:
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set ex
          # 从hostname中获取索引，比如(mysql-1)会获取(1)
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # 为了不让server-id相同而增加偏移量
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # 拷贝对应的文件到/mnt/conf.d/文件夹中
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 整体意思:
          # 1.如果是主mysql中的xtrabackup,就不需要克隆自己了,直接退出
          # 2.如果是从mysql中的xtrabackup,先判断是否是第一次创建，因为第二次重启本地就有数据库，无需克隆。若是第一次创建(通过/var/lib/mysql/mysql文件是否存在判断),就需要克隆数据库到本地。
          # 如果有数据不必克隆数据，直接退出()
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # 如果是master数据也不必克隆
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # 从序列号比自己小一的数据库克隆数据，比如mysql-2会从mysql-1处克隆数据
          ncat --recv-only mysql-ss-$(($ordinal-1)).mysql-headless 3307 | xbstream -x -C /var/lib/mysql
          # 比较数据
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 50m
            memory: 50Mi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 确定binlog 克隆数据位置(如果binlog存在的话).
          cd /var/lib/mysql
          # 如果存在该文件，则该xrabackup是从现有的从节点克隆出来的。
          if [[ -s xtrabackup_slave_info ]]; then
            mv xtrabackup_slave_info change_master_to.sql.in
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then         
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi     
          if [[ -f change_master_to.sql.in ]]; then
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
            echo "Initializing replication from clone position"
            mv change_master_to.sql.in change_master_to.sql.orig
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-ss-0.mysql-headless',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 10m
            memory: 10Mi
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 0.1Gi



[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-0 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-0 -n default' to see all of the containers in this pod.
root@mysql-ss-0:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 392
Server version: 5.7.35-log MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
mysql> show databases;
+------------------------+
| Database               |
+------------------------+
| information_schema     |
| mysql                  |
| performance_schema     |
| sys                    |
| xtrabackup_backupfiles |
+------------------------+
5 rows in set (0.06 sec)

mysql> 

[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-1 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-1 -n default' to see all of the containers in this pod.
root@mysql-ss-1:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 396
Server version: 5.7.35 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+------------------------+
| Database               |
+------------------------+
| information_schema     |
| mysql                  |
| performance_schema     |
| sys                    |
| xtrabackup_backupfiles |
+------------------------+
5 rows in set (0.04 sec)

mysql> 

[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-2 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-2 -n default' to see all of the containers in this pod.
root@mysql-ss-2:/# 
root@mysql-ss-2:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 349
Server version: 5.7.36 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

######在salve(mysql-ss-2或mysql-ss-2)上操作:
######只有master(mysql-0)才有一切操作（读写删....）权限，而salve（mysql-1,mysql-2）只有读的权限
mysql> CREATE DATABASE demo; 
ERROR 1290 (HY000): The MySQL server is running with the --super-read-only option so it cannot execute this statement
mysql> 



###9、测试同步：
#####主库建表：
[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-0 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-0 -n default' to see all of the containers in this pod.
root@mysql-ss-0:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 493
Server version: 5.7.35-log MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show master status;
+-----------------------+----------+--------------+------------------+-------------------+
| File                  | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+-----------------------+----------+--------------+------------------+-------------------+
| mysql-ss-0-bin.000003 |      756 |              |                  |                   |
+-----------------------+----------+--------------+------------------+-------------------+
1 row in set (0.00 sec)

mysql> 


mysql>  CREATE DATABASE demo; 
Query OK, 1 row affected (0.05 sec)

mysql> CREATE TABLE demo.messages (message VARCHAR(250)); 
Query OK, 0 rows affected (0.05 sec)

mysql> INSERT INTO demo.messages VALUES ('hello');
Query OK, 1 row affected (0.03 sec)

mysql> 


#####从库查表：
[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-2 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-2 -n default' to see all of the containers in this pod.
root@mysql-ss-2:/# 
root@mysql-ss-2:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 425
Server version: 5.7.36 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 
mysql> 
mysql> show databases;
+------------------------+
| Database               |
+------------------------+
| information_schema     |
| demo                   |
| mysql                  |
| performance_schema     |
| sys                    |
| xtrabackup_backupfiles |
+------------------------+
6 rows in set (0.02 sec)

mysql>  use demo;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> select * from messages;
+---------+
| message |
+---------+
| hello   |
+---------+
1 row in set (0.00 sec)

mysql> 



mysql> show slave status\G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: mysql-ss-0.mysql-headless
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 10
              Master_Log_File: mysql-ss-0-bin.000003
          Read_Master_Log_Pos: 756
               Relay_Log_File: mysql-ss-2-relay-bin.000002
                Relay_Log_Pos: 927
        Relay_Master_Log_File: mysql-ss-0-bin.000003
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 756
              Relay_Log_Space: 1139
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 100
                  Master_UUID: 49b8f260-353f-11ec-a092-5a94cfd92711
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.00 sec)

ERROR: 
No query specified

mysql> 
ok,可以发现同步实现成功。


[root@master-1 k8s-mysql-replication]# kubectl get pods --show-labels 
NAME              READY   STATUS    RESTARTS   AGE   LABELS
mongo-0           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-0
mongo-1           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-1
mongo-2           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-2
mongo-3           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-3
mongo-4           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-4
mongo-5           2/2     Running   2          10d   app=mongo,controller-revision-hash=mongo-7845cf5bc9,environment=test,role=mongo,statefulset.kubernetes.io/pod-name=mongo-5
mysql-ss-0        2/2     Running   0          20m   app=mysql,controller-revision-hash=mysql-ss-7d977f6c98,statefulset.kubernetes.io/pod-name=mysql-ss-0
mysql-ss-1        2/2     Running   1          20m   app=mysql,controller-revision-hash=mysql-ss-7d977f6c98,statefulset.kubernetes.io/pod-name=mysql-ss-1
mysql-ss-2        2/2     Running   0          19m   app=mysql,controller-revision-hash=mysql-ss-7d977f6c98,statefulset.kubernetes.io/pod-name=mysql-ss-2
redis-cluster-0   1/1     Running   1          24d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-0
redis-cluster-1   1/1     Running   1          16d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-1
redis-cluster-2   1/1     Running   1          24d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-2
redis-cluster-3   1/1     Running   1          24d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-3
redis-cluster-4   1/1     Running   1          24d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-4
redis-cluster-5   1/1     Running   1          24d   app=redis-cluster,controller-revision-hash=redis-cluster-7dfd545b6d,statefulset.kubernetes.io/pod-name=redis-cluster-5
[root@master-1 k8s-mysql-replication]# 


###10、部署对外读写服务
[root@master-1 k8s-mysql-replication]# kubectl get pods --show-labels | grep mysql-ss-0 | awk '{print $6}' | awk -F, '{print $3}'
statefulset.kubernetes.io/pod-name=mysql-ss-0
[root@master-1 k8s-mysql-replication]#


得到该标签后写入如下selector字段:
[root@master-1 k8s-mysql-replication]# cat mysql-readwrite.yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-readwrite
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
    nodePort: 30006
  selector:
    statefulset.kubernetes.io/pod-name: mysql-ss-0
  type: NodePort
[root@master-1 k8s-mysql-replication]# kubectl apply -f  mysql-readwrite.yaml
service/mysql-readwrite created
[root@master-1 k8s-mysql-replication]# kubectl get svc|grep mysql
mysql-headless    ClusterIP   None          <none>        3306/TCP                         32m
mysql-readwrite   NodePort    10.1.24.219   <none>        3306:30006/TCP                   18s


读写连接测试(使用集群任意节点IP:nodePort进行连接)
[root@master-1 k8s-mysql-replication]# mysql -h172.16.201.134 -uroot -p -P30006
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1226
Server version: 5.7.35-log MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show databases;
+------------------------+
| Database               |
+------------------------+
| information_schema     |
| demo                   |
| mysql                  |
| performance_schema     |
| sys                    |
| xtrabackup_backupfiles |
+------------------------+
6 rows in set (0.00 sec)

mysql> 

[root@master-1 k8s-mysql-replication]# mysql -h172.16.201.135 -uroot -p -P30006 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1243
Server version: 5.7.35-log MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.


mysql> show master status;
+-----------------------+----------+--------------+------------------+-------------------+
| File                  | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |
+-----------------------+----------+--------------+------------------+-------------------+
| mysql-ss-0-bin.000003 |      756 |              |                  |                   |
+-----------------------+----------+--------------+------------------+-------------------+
1 row in set (0.01 sec)


###11、部署对外读服务
[root@master-1 k8s-mysql-replication]# cat mysql-read.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysqlread
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
    targetPort: 3306
    nodePort: 30036
  type: NodePort
  selector:
    app: mysql
[root@master-1 k8s-mysql-replication]# 

[root@master-1 k8s-mysql-replication]# kubectl apply -f mysql-read.yaml 
service/mysqlread created
[root@master-1 k8s-mysql-replication]# kubectl get svc
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
kubernetes        ClusterIP   10.1.0.1       <none>        443/TCP                          32d
mongo             ClusterIP   None           <none>        27017/TCP                        10d
mongo-service     NodePort    10.1.98.245    <none>        27017:27017/TCP                  10d
mysql-headless    ClusterIP   None           <none>        3306/TCP                         50m
mysql-readwrite   NodePort    10.1.24.219    <none>        3306:30006/TCP                   18m
mysqlread         NodePort    10.1.138.131   <none>        3306:30036/TCP                   13s
redis-cluster     NodePort    10.1.231.82    <none>        6379:31000/TCP,16379:30701/TCP   17d
[root@master-1 k8s-mysql-replication]# kubectl get svc|grep mysql
mysql-headless    ClusterIP   None           <none>        3306/TCP                         50m
mysql-readwrite   NodePort    10.1.24.219    <none>        3306:30006/TCP                   18m
mysqlread         NodePort    10.1.138.131   <none>        3306:30036/TCP                   20s
[root@master-1 k8s-mysql-replication]# 


[root@master-1 k8s-mysql-replication]# mysql -h172.16.201.134 -uroot -p -P30036 
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1789
Server version: 5.7.36 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use demo
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> 
mysql> 
mysql> CREATE TABLE demo.js(name VARCHAR(250)); 
ERROR 1290 (HY000): The MySQL server is running with the --super-read-only option so it cannot execute this statement
mysql> 




####12、扩展副本节点数量 
1、增加pv
[root@master-1 k8s-mysql-replication]# mkdir -p /net/mysql-3
[root@master-1 k8s-mysql-replication]# echo '/net/mysql-3 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 k8s-mysql-replication]# exportfs -r
[root@master-1 k8s-mysql-replication]# exportfs|grep mysql
/root/mysql-replicationdata
/net/mysql-0    <world>
/net/mysql-1    <world>
/net/mysql-2    <world>
/net/mysql-3    <world>

[root@master-1 k8s-mysql-replication]# showmount -e 172.16.201.134|grep mysql
/net/mysql-3                *
/net/mysql-2                *
/net/mysql-1                *
/net/mysql-0                *

[root@node-2 ~]# df -h|grep 134|grep mysql        
172.16.201.134:/net/mysql-0            50G  7.2G   43G  15% /net/mysql-0
172.16.201.134:/net/mysql-1            50G  7.2G   43G  15% /net/mysql-1
172.16.201.134:/net/mysql-2            50G  7.2G   43G  15% /net/mysql-2
172.16.201.134:/net/mysql-3            50G  7.2G   43G  15% /net/mysql-3

添加pv 名字是pv-d
[root@master-1 k8s-mysql-replication]# vim pv.yaml 
---

  apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv-d
  spec:
    capacity:
      storage: 1Gi
    accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
    persistentVolumeReclaimPolicy: Recycle # 当声明被释放，空间将回收再利用
    nfs:
      server: 172.16.201.134
      path: /net/mysql-3

更新pv配置：
[root@master-1 k8s-mysql-replication]# kubectl apply -f pv.yaml
persistentvolume/pv-a unchanged
persistentvolume/pv-b unchanged
persistentvolume/pv-c unchanged
persistentvolume/pv-d created

[root@master-1 k8s-mysql-replication]#  kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                     STORAGECLASS                   REASON   AGE
pv-a                                       1Gi        RWO,ROX        Recycle          Bound       default/data-mysql-ss-0                                                                                           118m
pv-b                                       1Gi        RWO,ROX        Recycle          Bound       default/data-mysql-ss-1                                                                                           118m
pv-c                                       1Gi        RWO,ROX        Recycle          Bound       default/data-mysql-ss-2                                                                                           118m
pv-d                                       1Gi        RWO,ROX        Recycle          Available                                                                                                                     8s
pv-d是Available状态。


[root@master-1 k8s-mysql-replication]# kubectl get statefulset
NAME            READY   AGE
mongo           6/6     10d
mysql-ss        3/3     102m
redis-cluster   6/6     24d


[root@master-1 k8s-mysql-replication]# kubectl scale statefulset mysql-ss --replicas=4
statefulset.apps/mysql-ss scaled
[root@master-1 k8s-mysql-replication]# 


[root@master-1 k8s-mysql-replication]#  kubectl get pods|grep mysql
mysql-ss-0        2/2     Running   0          124m
mysql-ss-1        2/2     Running   1          124m
mysql-ss-2        2/2     Running   0          123m
mysql-ss-3        1/2     Running   1          3m50s



[root@master-1 k8s-mysql-replication]# kubectl get pv|grep mysql
pv-a                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-0                                                                                           126m
pv-b                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-1                                                                                           126m
pv-c                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-2                                                                                           126m
pv-d                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-3                                                                                           7m56s


[root@master-1 k8s-mysql-replication]# kubectl get pvc
NAME                                       STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS    AGE
data-mysql-ss-0                            Bound    pv-a                                       1Gi        RWO,ROX                        124m
data-mysql-ss-1                            Bound    pv-b                                       1Gi        RWO,ROX                        124m
data-mysql-ss-2                            Bound    pv-c                                       1Gi        RWO,ROX                        123m
data-mysql-ss-3                            Bound    pv-d                                       1Gi        RWO,ROX                        4m10s

pv-d已经挂上了。

[root@node-1 net]# ll *
mysql-0:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql

mysql-1:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql

mysql-2:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 11:12 mysql

mysql-3:
total 4
drwxr-xr-x 7 polkitd root 4096 Oct 25 12:59 mysql
You have new mail in /var/spool/mail/root
[root@node-1 net]# 


[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-3 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-3 -n default' to see all of the containers in this pod.
root@mysql-ss-3:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 183
Server version: 5.7.35 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show slave status\G;
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: mysql-ss-0.mysql-headless
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 10
              Master_Log_File: mysql-ss-0-bin.000003
          Read_Master_Log_Pos: 756
               Relay_Log_File: mysql-ss-3-relay-bin.000002
                Relay_Log_Pos: 325
        Relay_Master_Log_File: mysql-ss-0-bin.000003
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 756
              Relay_Log_Space: 537
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 100
                  Master_UUID: 49b8f260-353f-11ec-a092-5a94cfd92711
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.03 sec)

ERROR: 
No query specified

mysql> 
第四个节点同步正常，

mysql> show databases;
+------------------------+
| Database               |
+------------------------+
| information_schema     |
| demo                   |
| mysql                  |
| performance_schema     |
| sys                    |
| xtrabackup_backupfiles |
+------------------------+
6 rows in set (0.22 sec)

mysql> use demp
ERROR 1049 (42000): Unknown database 'demp'
mysql> use demo
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> select * from messages;
+---------+
| message |
+---------+
| hello   |
+---------+
1 row in set (0.13 sec)

mysql> 
数据可读。


创建过程：
[root@master-1 k8s-mysql-replication]# kubectl get event
11m         Normal    Scheduled                 pod/mysql-ss-3                          Successfully assigned default/mysql-ss-3 to master-1
11m         Normal    Pulled                    pod/mysql-ss-3                          Container image "mysql:5.7" already present on machine
11m         Normal    Created                   pod/mysql-ss-3                          Created container init-mysql
11m         Normal    Started                   pod/mysql-ss-3                          Started container init-mysql
11m         Normal    Pulled                    pod/mysql-ss-3                          Container image "gcr.io/google-samples/xtrabackup:1.0" already present on machine
11m         Normal    Created                   pod/mysql-ss-3                          Created container clone-mysql
11m         Normal    Started                   pod/mysql-ss-3                          Started container clone-mysql
11m         Normal    Pulled                    pod/mysql-ss-3                          Container image "mysql:5.7" already present on machine
11m         Normal    Created                   pod/mysql-ss-3                          Created container mysql
11m         Normal    Started                   pod/mysql-ss-3                          Started container mysql
11m         Normal    Pulled                    pod/mysql-ss-3                          Container image "gcr.io/google-samples/xtrabackup:1.0" already present on machine
11m         Normal    Created                   pod/mysql-ss-3                          Created container xtrabackup
11m         Normal    Started                   pod/mysql-ss-3                          Started container xtrabackup
11m         Normal    SuccessfulCreate          statefulset/mysql-ss                    create Claim data-mysql-ss-3 Pod mysql-ss-3 in StatefulSet mysql-ss success
11m         Normal    SuccessfulCreate          statefulset/mysql-ss                    create Pod mysql-ss-3 in StatefulSet mysql-ss successful
24m         Normal    SuccessfulDelete          statefulset/mysql-ss                    delete Pod mysql-ss-3 in StatefulSet mysql-ss successful



缩容操作：
[root@master-1 k8s-mysql-replication]#  kubectl scale statefulset mysql-ss --replicas=3
statefulset.apps/mysql-ss scaled
[root@master-1 k8s-mysql-replication]#  kubectl get pods|grep mysql
mysql-ss-0        2/2     Running   0          133m
mysql-ss-1        2/2     Running   1          133m
mysql-ss-2        2/2     Running   0          132m


######但是请注意，按比例扩大会自动创建新的 PersistentVolumeClaims，而按比例缩小不会自动删除这些 PVC。 这使你可以选择保留那些初始化的 PVC，以更快地进行缩放，或者在删除它们之前提取数据。这表明，尽管将 StatefulSet 缩小为3，所有5个 PVC 仍然存在：
[root@master-1 k8s-mysql-replication]# kubectl get pv,pvc|grep mysql
persistentvolume/pv-a                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-0                                                                                           134m
persistentvolume/pv-b                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-1                                                                                           134m
persistentvolume/pv-c                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-2                                                                                           134m
persistentvolume/pv-d                                       1Gi        RWO,ROX        Recycle          Bound    default/data-mysql-ss-3                                                                                           16m
persistentvolumeclaim/data-mysql-ss-0                            Bound    pv-a                                       1Gi        RWO,ROX                        134m
persistentvolumeclaim/data-mysql-ss-1                            Bound    pv-b                                       1Gi        RWO,ROX                        133m
persistentvolumeclaim/data-mysql-ss-2                            Bound    pv-c                                       1Gi        RWO,ROX                        133m
persistentvolumeclaim/data-mysql-ss-3                            Bound    pv-d                                       1Gi        RWO,ROX                        13m


如果你不打算重复使用多余的 PVC，则可以删除它：
kubectl delete pvc pv-d

#####操作大概步骤：1、建nfs（建/net/mysql-3目录），建pv（更新pv.yaml配置），扩容（replicas=4）。


####13、模拟 Pod 和 Node 的宕机

######1、当前状态：
[root@master-1 k8s-mysql-replication]# kubectl get pod
NAME              READY   STATUS    RESTARTS   AGE
mongo-0           2/2     Running   2          10d
mongo-1           2/2     Running   2          10d
mongo-2           2/2     Running   2          10d
mongo-3           2/2     Running   2          10d
mongo-4           2/2     Running   2          10d
mongo-5           2/2     Running   2          10d
mysql-ss-0        2/2     Running   0          137m
mysql-ss-1        2/2     Running   1          136m
mysql-ss-2        2/2     Running   0          136m
redis-cluster-0   1/1     Running   1          24d
redis-cluster-1   1/1     Running   1          17d
redis-cluster-2   1/1     Running   1          24d
redis-cluster-3   1/1     Running   1          24d
redis-cluster-4   1/1     Running   1          24d
redis-cluster-5   1/1     Running   1          24d
[root@master-1 k8s-mysql-replication]# 


######2、破坏就绪态探测，模拟故障：
[root@master-1 k8s-mysql-replication]# kubectl exec mysql-ss-2 -c mysql -- mv /usr/bin/mysql /usr/bin/mysql.off

在 READY 列中查找 1/2 ：
[root@master-1 k8s-mysql-replication]# kubectl get pod mysql-ss-2
NAME         READY   STATUS    RESTARTS   AGE
mysql-ss-2   1/2     Running   0          141m


######3、修复：
[root@master-1 k8s-mysql-replication]# kubectl exec mysql-ss-2 -c mysql -- mv /usr/bin/mysql.off /usr/bin/mysql
[root@master-1 k8s-mysql-replication]# kubectl get pod mysql-ss-2
NAME         READY   STATUS    RESTARTS   AGE
mysql-ss-2   2/2     Running   0          142m

[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-2 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-2 -n default' to see all of the containers in this pod.
root@mysql-ss-2:/# mysql 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 5050
Server version: 5.7.36 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> show slave status\G;    
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: mysql-ss-0.mysql-headless
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 10
              Master_Log_File: mysql-ss-0-bin.000003
          Read_Master_Log_Pos: 756
               Relay_Log_File: mysql-ss-2-relay-bin.000002
                Relay_Log_Pos: 927
        Relay_Master_Log_File: mysql-ss-0-bin.000003
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 756
              Relay_Log_Space: 1139
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 100
                  Master_UUID: 49b8f260-353f-11ec-a092-5a94cfd92711
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.00 sec)

ERROR: 
No query specified

mysql> 
恢复正常。


######4、删除 Pods
如果删除了 Pod，则 StatefulSet 还会重新创建 Pod，类似于 ReplicaSet 对无状态 Pod 所做的操作。


[root@master-1 k8s-mysql-replication]# kubectl delete pod mysql-ss-2
pod "mysql-ss-2" deleted
[root@master-1 k8s-mysql-replication]# 
需要等一会，清理结束

状态：
[root@master-1 k8s-mysql-replication]# kubectl get pod|grep mysql
mysql-ss-0        2/2     Running       0          149m
mysql-ss-1        2/2     Running       1          149m
mysql-ss-2        2/2     Terminating   0          148m

立即新建mysql-ss-2
[root@master-1 k8s-mysql-replication]# kubectl get pod|grep mysql
mysql-ss-0        2/2     Running   0          149m
mysql-ss-1        2/2     Running   1          149m
mysql-ss-2        1/2     Running   0          9s


[root@master-1 k8s-mysql-replication]# kubectl exec -it mysql-ss-2 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
Defaulting container name to mysql.
Use 'kubectl describe pod/mysql-ss-2 -n default' to see all of the containers in this pod.
root@mysql-ss-2:/# 
root@mysql-ss-2:/# mysql
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 26
Server version: 5.7.36 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql>  show slave status\G;    
*************************** 1. row ***************************
               Slave_IO_State: Waiting for master to send event
                  Master_Host: mysql-ss-0.mysql-headless
                  Master_User: root
                  Master_Port: 3306
                Connect_Retry: 10
              Master_Log_File: mysql-ss-0-bin.000003
          Read_Master_Log_Pos: 756
               Relay_Log_File: mysql-ss-2-relay-bin.000004
                Relay_Log_Pos: 325
        Relay_Master_Log_File: mysql-ss-0-bin.000003
             Slave_IO_Running: Yes
            Slave_SQL_Running: Yes
              Replicate_Do_DB: 
          Replicate_Ignore_DB: 
           Replicate_Do_Table: 
       Replicate_Ignore_Table: 
      Replicate_Wild_Do_Table: 
  Replicate_Wild_Ignore_Table: 
                   Last_Errno: 0
                   Last_Error: 
                 Skip_Counter: 0
          Exec_Master_Log_Pos: 756
              Relay_Log_Space: 537
              Until_Condition: None
               Until_Log_File: 
                Until_Log_Pos: 0
           Master_SSL_Allowed: No
           Master_SSL_CA_File: 
           Master_SSL_CA_Path: 
              Master_SSL_Cert: 
            Master_SSL_Cipher: 
               Master_SSL_Key: 
        Seconds_Behind_Master: 0
Master_SSL_Verify_Server_Cert: No
                Last_IO_Errno: 0
                Last_IO_Error: 
               Last_SQL_Errno: 0
               Last_SQL_Error: 
  Replicate_Ignore_Server_Ids: 
             Master_Server_Id: 100
                  Master_UUID: 49b8f260-353f-11ec-a092-5a94cfd92711
             Master_Info_File: /var/lib/mysql/master.info
                    SQL_Delay: 0
          SQL_Remaining_Delay: NULL
      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates
           Master_Retry_Count: 86400
                  Master_Bind: 
      Last_IO_Error_Timestamp: 
     Last_SQL_Error_Timestamp: 
               Master_SSL_Crl: 
           Master_SSL_Crlpath: 
           Retrieved_Gtid_Set: 
            Executed_Gtid_Set: 
                Auto_Position: 0
         Replicate_Rewrite_DB: 
                 Channel_Name: 
           Master_TLS_Version: 
1 row in set (0.00 sec)

ERROR: 
No query specified

mysql> 

StatefulSet 控制器注意到不再存在 mysql-ss-2 Pod，于是创建一个具有相同名称并链接到相同 PersistentVolumeClaim 的新 Pod。 
新的 mysql-ss-2同步正常。



####14、腾空节点 （未做测试）
如果你的 Kubernetes 集群具有多个节点，则可以通过发出以下 drain 命令来模拟节点停机（就好像节点在被升级）。

首先确定 MySQL Pod 之一在哪个节点上：

kubectl get pod mysql-2 -o wide
节点名称应显示在最后一列中：

NAME      READY     STATUS    RESTARTS   AGE       IP            NODE
mysql-2   2/2       Running   0          15m       10.244.5.27   kubernetes-node-9l2t
然后通过运行以下命令腾空节点，该命令将其保护起来，以使新的 Pod 不能调度到该节点， 然后逐出所有现有的 Pod。将 <节点名称> 替换为在上一步中找到的节点名称。

这可能会影响节点上的其他应用程序，因此最好 仅在测试集群中执行此操作。

kubectl drain <节点名称> --force --delete-local-data --ignore-daemonsets
现在，你可以看到 Pod 被重新调度到其他节点上：

kubectl get pod mysql-2 -o wide --watch
它看起来应该像这样：

NAME      READY   STATUS          RESTARTS   AGE       IP            NODE
mysql-2   2/2     Terminating     0          15m       10.244.1.56   kubernetes-node-9l2t
[...]
mysql-2   0/2     Pending         0          0s        <none>        kubernetes-node-fjlm
mysql-2   0/2     Init:0/2        0          0s        <none>        kubernetes-node-fjlm
mysql-2   0/2     Init:1/2        0          20s       10.244.5.32   kubernetes-node-fjlm
mysql-2   0/2     PodInitializing 0          21s       10.244.5.32   kubernetes-node-fjlm
mysql-2   1/2     Running         0          22s       10.244.5.32   kubernetes-node-fjlm
mysql-2   2/2     Running         0          30s       10.244.5.32   kubernetes-node-fjlm
再次，你应该看到服务器 ID 102 从 SELECT @@server_id 循环输出 中消失一段时间，然后自行出现。

现在去掉节点保护（Uncordon），使其恢复为正常模式:

kubectl uncordon <节点名称>







##(五)、部署 Redis单点




[root@master-1 mysql]# cat ra
kubectl apply -f r-sc.yaml
kubectl apply -f r-pv.yaml
kubectl apply -f r-pvc.yaml

kubectl apply -f redis-config.yaml
kubectl apply -f redis-deployment.yaml

sleep 2
echo "============== kubectl get ns,cm,pod,svc,pv,pvc,sc -o wide ================="
kubectl get ns,cm,pod,svc,pv,pvc,sc -o wide


[root@master-1 redis]# cat rd
kubectl delete -f r-pvc.yaml
kubectl delete -f r-pv.yaml
kubectl delete -f r-sc.yaml

kubectl delete -f redis-deployment.yaml
kubectl delete -f redis-config.yaml

sleep 2
echo "============== kubectl get pod,svc,pv,pvc,sc -o wide ================="
kubectl get pod,svc,pv,pvc,sc -o wide



[root@master-1 redis]# cat r-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"


[root@master-1 redis]# cat r-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv01
spec:
  storageClassName: nfs      
  capacity:                  # PV容量
    storage: 1Gi
  accessModes:               # 访问模式
  - ReadWriteOnce
  nfs:
    path: /root/web1
    server: "172.16.201.134"
  volumeMode: Filesystem


[root@master-1 redis]# cat r-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc01
spec:
  storageClassName: nfs   # 只能绑定nfs类的PV
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi







[root@master-1 redis]# cat redis-config.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: redis-config
  labels:
    app: redis
data:
  redis.conf: |-
    dir /data
    port 6379
    bind 0.0.0.0
    appendonly yes
    protected-mode no
    requirepass zisefeizhu
    pidfile /data/redis-6379.pid


[root@master-1 redis]# cat redis-deployment.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: redis
spec:
  type: ClusterIP
  ports:
    - name: redis
      port: 6379
  selector:
    app: redis

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis

  labels:
    app: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      initContainers:
        - name: system-init
          image: busybox:1.32
          imagePullPolicy: IfNotPresent
          command:
            - "sh"
            - "-c"
            - "echo 2048 > /proc/sys/net/core/somaxconn && echo never > /sys/kernel/mm/transparent_hugepage/enabled"
          securityContext:
            privileged: true
            runAsUser: 0
          volumeMounts:
          - name: sys
            mountPath: /sys
      containers:
        - name: redis
          image: redis:5.0.8
          command:
            - "sh"
            - "-c"
            - "redis-server /usr/local/etc/redis/redis.conf"
          ports:
            - containerPort: 6379
          resources:
            limits:
              cpu: 1000m
              memory: 1024Mi
            requests:
              cpu: 1000m
              memory: 1024Mi
          livenessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 300
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: 6379
            initialDelaySeconds: 5
            timeoutSeconds: 1
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: rdata
              mountPath: /data
            - name: config
              mountPath: /usr/local/etc/redis/redis.conf
              subPath: redis.conf
      volumes:
        - name: rdata
          persistentVolumeClaim:
            claimName: pvc01
        - name: config
          configMap:
            name: redis-config
        - name: sys
          hostPath:
            path: /sys
[root@master-1 redis]# 





[root@master-1 redis]# kubectl get ns,cm,pod,svc,pv,pvc,sc -o wide
NAME                        STATUS   AGE
namespace/default           Active   7d2h
namespace/ingress-nginx     Active   4d21h
namespace/kube-node-lease   Active   7d2h
namespace/kube-public       Active   7d2h
namespace/kube-system       Active   7d2h
namespace/ng                Active   40h
namespace/test-ns           Active   5d19h

NAME                     DATA   AGE
configmap/redis-config   1      18h

NAME                         READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
pod/redis-5bffdfbd94-fsv8x   1/1     Running   0          8h    10.244.2.43   node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE    SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP    7d2h   <none>
service/redis        ClusterIP   10.1.89.238   <none>        6379/TCP   18h    app=redis

NAME                    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM           STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv01   1Gi        RWO            Retain           Bound    default/pvc01   nfs                     18h   Filesystem

NAME                          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/pvc01   Bound    pv01     1Gi        RWO            nfs            18h   Filesystem

NAME                                        PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/nfs (default)   kubernetes.io/nfs   Retain          Immediate           false                  18h





######测试redis是否可以正常使用
[root@master-1 redis]# kubectl exec -it pod/redis-5bffdfbd94-fsv8x -- /bin/sh
######redis-cli

127.0.0.1:6379> auth zisefeizhu
OK
127.0.0.1:6379> config get requirepass
1) "requirepass"
2) "zisefeizhu"
127.0.0.1:6379> 



问题：
[root@master-1 redis]# ./rd
service "redis" deleted
deployment.apps "redis" deleted
configmap "redis-config" deleted
persistentvolumeclaim "pvc01" deleted
hang在这里不动了，无法清理
解决：
[root@master-1 mysql]# kubectl patch pvc pvc01 -p '{"metadata":{"finalizers": []}}' --type=merge         
persistentvolumeclaim/pvc01 patched



问题：无法清理pod
清理Docker占用的磁盘空间
// 可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络
[root@master-1 redis]#  docker system prune -a
WARNING! This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all images without at least one container associated to them
  - all build cache

Are you sure you want to continue? [y/N] y
Deleted Containers:
。。。。。。。。。。。
Total reclaimed space: 1.462GB

pod还在
[root@master-1 redis]# kubectl get pods -o wide                      
NAME                     READY   STATUS        RESTARTS   AGE   IP       NODE     NOMINATED NODE   READINESS GATES
redis-5bffdfbd94-9r6rm   0/1     Terminating   0          53m   <none>   node-1   <none>           <none>

去node-1上，手动删除


[root@node-1 ~]# docker ps -a|grep redis
8025db9ac01f        busybox                                              "sh -c 'echo 2048 > …"   53 minutes ago      Created                                           k8s_system-init_redis-5bffdfbd94-9r6rm_default_9305e987-644d-480b-a19e-ed2d64262d2b_0
9642213831f5        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 53 minutes ago      Exited (137) 52 minutes ago                       k8s_POD_redis-5bffdfbd94-9r6rm_default_9305e987-644d-480b-a19e-ed2d64262d2b_0
1ae1bdb2a6d0        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 14 hours ago        Exited (255) 8 hours ago                          k8s_POD_redis-5bffdfbd94-ssnhr_default_4fc56a08-a3fe-47ef-b371-88ddd4af6d13_0
[root@node-1 ~]# 
[root@node-1 ~]# 
[root@node-1 ~]# 
[root@node-1 ~]# docker rm 8025db9ac01f 
8025db9ac01f
[root@node-1 ~]# docker rm 9642213831f5
9642213831f5
[root@node-1 ~]# docker rm 1ae1bdb2a6d0
1ae1bdb2a6d0

简单粗暴，pod被干掉了：
[root@master-1 ~]# kubectl get pods -o wide                      
No resources found in default namespace.
[root@master-1 ~]# 


##(六)、部署 Redis-Sentinel集群
参考：
https://www.cnblogs.com/dukuan/p/9913420.html

YAML配置：
https://github.com/dotbalo/k8s/blob/master/redis/k8s-redis-sentinel/redis-sentinel-pv.yaml



###1、PV创建
[root@master-1 redis]# mkdir -p /root/redis-sentinel/0
[root@master-1 redis]# mkdir -p /root/redis-sentinel/1
[root@master-1 redis]# mkdir -p /root/redis-sentinel/2


[root@master-1 redis]# vim /etc/exports
/root/redis-sentinel/0 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-sentinel/1 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-sentinel/2 *(rw,sync,no_subtree_check,no_root_squash)

######配置生效
[root@master-1 mysql]# exportfs -r
######查看生效
[root@master-1 mysql]# exportfs
/root/nfs_data  <world>


[root@master-1 redis]# exportfs
/root/nfs_data  <world>
/root/web1      <world>
/root/redis-sentinel/0 <world>
/root/redis-sentinel/1 <world>
/root/redis-sentinel/2 <world>

[root@node-1 ~]# mkdir -p /root/redis-sentinel/0
[root@node-1 ~]# mkdir -p /root/redis-sentinel/1
[root@node-1 ~]# mkdir -p /root/redis-sentinel/2
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/0  /root/redis-sentinel/0
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/1  /root/redis-sentinel/1
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/2  /root/redis-sentinel/2

[root@node-2 ~]# mkdir -p /root/redis-sentinel/0
[root@node-2 ~]# mkdir -p /root/redis-sentinel/1
[root@node-2 ~]# mkdir -p /root/redis-sentinel/2
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/0  /root/redis-sentinel/0
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/1  /root/redis-sentinel/1
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-sentinel/2  /root/redis-sentinel/2

umount -t nfs 172.16.201.134:/k8s/redis-sentinel/0
umount -t nfs 172.16.201.134:/k8s/redis-sentinel/1
umount -t nfs 172.16.201.134:/k8s/redis-sentinel/2


[root@node-1 ~]# df -h|grep redis
172.16.201.134:/root/redis-sentinel/0   50G  4.9G   46G  10% /root/redis-sentinel/0
172.16.201.134:/root/redis-sentinel/1   50G  4.9G   46G  10% /root/redis-sentinel/1
172.16.201.134:/root/redis-sentinel/2   50G  4.9G   46G  10% /root/redis-sentinel/2

[root@node-2 ~]#  df -h|grep redis
172.16.201.134:/root/redis-sentinel/0   50G  4.9G   46G  10% /root/redis-sentinel/0
172.16.201.134:/root/redis-sentinel/1   50G  4.9G   46G  10% /root/redis-sentinel/1
172.16.201.134:/root/redis-sentinel/2   50G  4.9G   46G  10% /root/redis-sentinel/2


######创建pv，注意Redis的空间大小按需修改
[root@k8s-master01 redis-sentinel]#vim redis-sentinel-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-redis-sentinel-0
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-sentinel-storage-class"
  nfs:
    path: /root/redis-sentinel/0
    server: 172.16.201.134

---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-redis-sentinel-1
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-sentinel-storage-class"
  nfs:
    path: /root/redis-sentinel/1
    server: 172.16.201.134

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-redis-sentinel-2
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-sentinel-storage-class"
  nfs:
    path: /root/redis-sentinel/2
    server: 172.16.201.134


[root@master-1 sentinel]# kubectl create -f redis-sentinel-pv.yaml
persistentvolume/pv-redis-sentinel-0 created
persistentvolume/pv-redis-sentinel-1 created
persistentvolume/pv-redis-sentinel-2 created

[root@master-1 sentinel]# kubectl get pv
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS                   REASON   AGE
pv-redis-sentinel-0   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            6s
pv-redis-sentinel-1   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            6s
pv-redis-sentinel-2   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            6s
[root@master-1 sentinel]# 

###2、创建namespace
[root@master-1 sentinel]# kubectl create namespace public-service
namespace/public-service created

[root@master-1 ~]# kubectl get ns,cm,pod,svc,pv,pvc,sc -o wide

NAME                        STATUS   AGE
namespace/default           Active   7d6h
namespace/ingress-nginx     Active   5d2h
namespace/kube-node-lease   Active   7d6h
namespace/kube-public       Active   7d6h
namespace/kube-system       Active   7d6h
namespace/ng                Active   44h
namespace/public-service    Active   33s
namespace/test-ns           Active   6d

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   7d6h   <none>

NAME                                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS                   REASON   AGE     VOLUMEMODE
persistentvolume/pv-redis-sentinel-0   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            2m50s   Filesystem
persistentvolume/pv-redis-sentinel-1   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            2m50s   Filesystem
persistentvolume/pv-redis-sentinel-2   1Gi        RWX            Recycle          Available           redis-sentinel-storage-class            2m50s   Filesystem
[root@master-1 ~]# 

###3、创建ConfigMap
[root@master-1 sentinel]# vim redis-sentinel-configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: redis-sentinel-config
  namespace: public-service
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
data:
    redis-master.conf: |
      port 6379
      tcp-backlog 511
      timeout 0
      tcp-keepalive 0
      loglevel notice
      databases 16
      save 900 1
      save 300 10
      save 60 10000
      stop-writes-on-bgsave-error yes
      rdbcompression yes
      rdbchecksum yes
      dbfilename dump.rdb
      dir /data/
      slave-serve-stale-data yes
      repl-diskless-sync no
      repl-diskless-sync-delay 5
      repl-disable-tcp-nodelay no
      slave-priority 100
      appendonly no
      appendfilename "appendonly.aof"
      appendfsync everysec
      no-appendfsync-on-rewrite no
      auto-aof-rewrite-percentage 100
      auto-aof-rewrite-min-size 64mb
      aof-load-truncated yes
      lua-time-limit 5000
      slowlog-log-slower-than 10000
      slowlog-max-len 128
      latency-monitor-threshold 0
      notify-keyspace-events ""
      hash-max-ziplist-entries 512
      hash-max-ziplist-value 64
      list-max-ziplist-entries 512
      list-max-ziplist-value 64
      set-max-intset-entries 512
      zset-max-ziplist-entries 128
      zset-max-ziplist-value 64
      hll-sparse-max-bytes 3000
      activerehashing yes
      client-output-buffer-limit normal 0 0 0
      client-output-buffer-limit slave 256mb 64mb 60
      client-output-buffer-limit pubsub 64mb 16mb 60
      hz 10
      aof-rewrite-incremental-fsync yes
    redis-slave.conf: |
      port 6379
      slaveof redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local 6379
      tcp-backlog 511
      timeout 0
      tcp-keepalive 0
      loglevel notice
      databases 16
      save 900 1
      save 300 10
      save 60 10000
      stop-writes-on-bgsave-error yes
      rdbcompression yes
      rdbchecksum yes
      dbfilename dump.rdb
      dir /data/
      slave-serve-stale-data yes
      slave-read-only yes
      repl-diskless-sync no
      repl-diskless-sync-delay 5
      repl-disable-tcp-nodelay no
      slave-priority 100
      appendonly no
      appendfilename "appendonly.aof"
      appendfsync everysec
      no-appendfsync-on-rewrite no
      auto-aof-rewrite-percentage 100
      auto-aof-rewrite-min-size 64mb
      aof-load-truncated yes
      lua-time-limit 5000
      slowlog-log-slower-than 10000
      slowlog-max-len 128
      latency-monitor-threshold 0
      notify-keyspace-events ""
      hash-max-ziplist-entries 512
      hash-max-ziplist-value 64
      list-max-ziplist-entries 512
      list-max-ziplist-value 64
      set-max-intset-entries 512
      zset-max-ziplist-entries 128
      zset-max-ziplist-value 64
      hll-sparse-max-bytes 3000
      activerehashing yes
      client-output-buffer-limit normal 0 0 0
      client-output-buffer-limit slave 256mb 64mb 60
      client-output-buffer-limit pubsub 64mb 16mb 60
      hz 10
      aof-rewrite-incremental-fsync yes
    redis-sentinel.conf: |
      port 26379
      dir /data
      sentinel monitor mymaster redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local 6379 2
      sentinel down-after-milliseconds mymaster 30000
      sentinel parallel-syncs mymaster 1
      sentinel failover-timeout mymaster 180000


[root@master-1 sentinel]# kubectl create -f redis-sentinel-configmap.yaml
configmap/redis-sentinel-config created
[root@master-1 sentinel]# kubectl get configmap -n public-service
NAME                    DATA   AGE
redis-sentinel-config   3      5s
[root@master-1 sentinel]# 

注意，此时configmap中redis-slave.conf的slaveof的master地址为ss里面的Headless Service地址。




###4、创建service
　　service主要提供pods之间的互访，StatefulSet主要用Headless Service通讯，格式：statefulSetName-{0..N-1}.serviceName.namespace.svc.cluster.local

　　- serviceName为Headless Service的名字
　　- 0..N-1为Pod所在的序号，从0开始到N-1
　　- statefulSetName为StatefulSet的名字
　　- namespace为服务所在的namespace，Headless Servic和StatefulSet必须在相同的namespace
　　- .cluster.local为Cluster Domain

　　如本集群的HS为：
　　　　Master：
　　　　　　redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local:6379
　　　　Slave：
　　　　　　redis-sentinel-slave-ss-0.redis-sentinel-slave-ss.public-service.svc.cluster.local:6379
　　　　　　redis-sentinel-slave-ss-1.redis-sentinel-slave-ss.public-service.svc.cluster.local:6379


[root@master-1 sentinel]# vim redis-sentinel-service-master.yaml
kind: Service
apiVersion: v1
metadata:
  labels:
    app: redis-sentinel-master-ss
  name: redis-sentinel-master-ss
  namespace: public-service
spec:
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
  selector:
    app: redis-sentinel-master-ss
[root@master-1 sentinel]# vim redis-sentinel-service-slave.yaml
kind: Service
apiVersion: v1
metadata:
  labels:
    app: redis-sentinel-slave-ss
  name: redis-sentinel-slave-ss
  namespace: public-service
spec:
  clusterIP: None
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
  selector:
    app: redis-sentinel-slave-ss
[root@master-1 sentinel]# 


[root@master-1 sentinel]# kubectl create -f redis-sentinel-service-master.yaml -f redis-sentinel-service-slave.yaml
service/redis-sentinel-master-ss created
service/redis-sentinel-slave-ss created

[root@master-1 sentinel]# kubectl get service -n public-service
NAME                       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
redis-sentinel-master-ss   ClusterIP   None         <none>        6379/TCP   6s
redis-sentinel-slave-ss    ClusterIP   None         <none>        6379/TCP   6s



###5、创建StatefulSet
[root@master-1 sentinel]# vim redis-sentinel-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: redis-sentinel
  namespace: public-service
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: redis-sentinel
  namespace: public-service
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: redis-sentinel
  namespace: public-service
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: redis-sentinel
subjects:
- kind: ServiceAccount
  name: redis-sentinel
  namespace: public-service
[root@master-1 sentinel]# 



[root@master-1 sentinel]# vim redis-sentinel-ss-master.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  labels:
    app: redis-sentinel-master-ss
  name: redis-sentinel-master-ss
  namespace: public-service
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis-sentinel-master-ss
  serviceName: redis-sentinel-master-ss
  template:
    metadata:
      labels:
        app: redis-sentinel-master-ss
    spec:
      containers:
      - args:
        - -c
        - cp /mnt/redis-master.conf /data/ ; redis-server /data/redis-master.conf
        command:
        - sh
        image: redis
        imagePullPolicy: IfNotPresent
        name: redis-master
        ports:
        - containerPort: 6379
          name: masterport
          protocol: TCP
        volumeMounts:
        - mountPath: /mnt/
          name: config-volume
          readOnly: false
        - mountPath: /data/
          name: redis-sentinel-master-storage
          readOnly: false
      serviceAccountName: redis-sentinel
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          items:
          - key: redis-master.conf
            path: redis-master.conf
          name: redis-sentinel-config 
        name: config-volume
c
[root@master-1 sentinel]# 

[root@master-1 sentinel]# vim redis-sentinel-ss-slave.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  labels:
    app: redis-sentinel-slave-ss
  name: redis-sentinel-slave-ss
  namespace: public-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis-sentinel-slave-ss
  serviceName: redis-sentinel-slave-ss
  template:
    metadata:
      labels:
        app: redis-sentinel-slave-ss
    spec:
      containers:
      - args:
        - -c
        - cp /mnt/redis-slave.conf /data/ ; redis-server /data/redis-slave.conf
        command:
        - sh
        image: redis
        imagePullPolicy: IfNotPresent
        name: redis-slave
        ports:
        - containerPort: 6379
          name: slaveport
          protocol: TCP
        volumeMounts:
        - mountPath: /mnt/
          name: config-volume
          readOnly: false
        - mountPath: /data/
          name: redis-sentinel-slave-storage
          readOnly: false
      serviceAccountName: redis-sentinel
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          items:
          - key: redis-slave.conf
            path: redis-slave.conf
          name: redis-sentinel-config 
        name: config-volume
  volumeClaimTemplates:
  - metadata:
      name: redis-sentinel-slave-storage
    spec:
      accessModes:
      - ReadWriteMany
      storageClassName: "redis-sentinel-storage-class"
      resources:
        requests:
          storage: 1Gi
[root@master-1 sentinel]# 




kubectl delete -f redis-sentinel-ss-slave.yaml


kubectl delete -f redis-sentinel-rbac.yaml -f redis-sentinel-ss-master.yaml -f redis-sentinel-ss-slave.yaml


[root@master-1 sentinel]# kubectl create -f redis-sentinel-rbac.yaml -f redis-sentinel-ss-master.yaml -f redis-sentinel-ss-slave.yaml
serviceaccount/redis-sentinel created
 rbac.authorization.k8s.io/v1 Role
role.rbac.authorization.k8s.io/redis-sentinel created
 rbac.authorization.k8s.io/v1 RoleBinding
rolebinding.rbac.authorization.k8s.io/redis-sentinel created
statefulset.apps/redis-sentinel-master-ss created
statefulset.apps/redis-sentinel-slave-ss created


[root@master-1 sentinel]# kubectl get statefulset -n public-service
NAME                       READY   AGE
redis-sentinel-master-ss   1/1     2m1s
redis-sentinel-slave-ss    2/2     2m1s


[root@master-1 sentinel]# kubectl get pods -n public-service
NAME                         READY   STATUS    RESTARTS   AGE
redis-sentinel-master-ss-0   1/1     Running   0          2m6s
redis-sentinel-slave-ss-0    1/1     Running   0          2m6s
redis-sentinel-slave-ss-1    1/1     Running   0          87s
[root@master-1 sentinel]# 


此时相当于已经在k8s上创建了Redis的主从模式。


###6、测试：

####master连接slave测试
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-master-ss-0 -n public-service -- redis-cli -h redis-sentinel-slave-ss-0.redis-sentinel-slave-ss.public-service.svc.cluster.local  ping
PONG
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-master-ss-0 -n public-service -- redis-cli -h redis-sentinel-slave-ss-1.redis-sentinel-slave-ss.public-service.svc.cluster.local  ping
PONG
[root@master-1 sentinel]# 

####slave连接master测试
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-0 -n public-service -- redis-cli -h redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local  ping
PONG
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli -h redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local  ping
PONG
[root@master-1 sentinel]# 


####同步状态查看
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli -h redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local  info replication
##### Replication
role:master
connected_slaves:2
slave0:ip=10.244.2.46,port=6379,state=online,offset=546,lag=0
slave1:ip=10.244.1.43,port=6379,state=online,offset=546,lag=1
master_failover_state:no-failover
master_replid:9b1bce324a94b3b68f829b63532d73bb59ec4553
master_replid2:0000000000000000000000000000000000000000
master_repl_offset:546
second_repl_offset:-1
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:546
[root@master-1 sentinel]# 




####同步测试
######master写入数据
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli -h redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local  set test test_data
OK

 
[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli -h redis-sentinel-master-ss-0.redis-sentinel-master-ss.public-service.svc.cluster.local  get test
"test_data"


[root@master-1 sentinel]#  kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli   get test
"test_data"
[root@master-1 sentinel]# 


####从节点无法写入数据

[root@master-1 sentinel]# kubectl exec -ti redis-sentinel-slave-ss-1 -n public-service -- redis-cli   set k v
(error) READONLY You can't write against a read only replica.
[root@master-1 sentinel]# 


[root@node-1 redis-sentinel]# pwd
/root/redis-sentinel
[root@node-1 redis-sentinel]# ll *
0:
total 8
-rw-r--r-- 1 root root  175 Sep 29 18:50 dump.rdb
-rw-r--r-- 1 root root 1057 Sep 29 18:50 redis-master.conf

1:
total 8
-rw-r--r-- 1 root root  175 Sep 29 18:50 dump.rdb
-rw-r--r-- 1 root root 1175 Sep 29 18:50 redis-slave.conf

2:
total 8
-rw-r--r-- 1 root root  175 Sep 29 18:50 dump.rdb
-rw-r--r-- 1 root root 1175 Sep 29 18:50 redis-slave.conf


#####说明：个人认为在k8s上搭建Redissentinel完全没有意义，经过测试，当master节点宕机后，sentinel选择新的节点当主节点，当原master恢复后，此时无法再次成为集群节点。因为在物理机上部署时，sentinel探测以及更改配置文件都是以IP的形式，集群复制也是以IP的形式，但是在容器中，虽然采用的StatefulSet的HeadlessService来建立的主从，但是主从建立后，master、slave、sentinel记录还是解析后的IP，但是pod的IP每次重启都会改变，所有sentinel无法识别宕机后又重新启动的master节点，所以一直无法加入集群，虽然可以通过固定podIP或者使用NodePort的方式来固定，或者通过sentinel获取当前master的IP来修改配置文件，但是个人觉得也是没有必要的，sentinel实现的是高可用Redis主从，检测Redis

#####Master的状态，进行主从切换等操作，但是在k8s中，无论是dc或者ss，都会保证pod以期望的值进行运行，再加上k8s自带的活性检测，当端口不可用或者服务不可用时会自动重启pod或者pod的中的服务，所以当在k8s中建立了Redis主从同步后，相当于已经成为了高可用状态，并且sentinel进行主从切换的时间不一定有k8s重建pod的时间快，所以个人认为在k8s上搭建sentinel没有意义。所以下面搭建sentinel的步骤无需在看。 


PS：Redis Cluster：https://github.com/dotbalo/k8s/tree/master/redis/k8s-redis-cluster



###7、创建sentinel

[root@master-1 sentinel]# vim redis-sentinel-ss-sentinel.yaml
kind: StatefulSet
apiVersion: apps/v1
metadata:
  labels:
    app: redis-sentinel-sentinel-ss
  name: redis-sentinel-sentinel-ss
  namespace: public-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis-sentinel-sentinel-ss
  serviceName: redis-sentinel-sentinel-ss
  template:
    metadata:
      labels:
        app: redis-sentinel-sentinel-ss
    spec:
      containers:
      - args:
        - -c
        - cp /mnt/redis-sentinel.conf /data/ ; redis-sentinel /data/redis-sentinel.conf
        command:
        - sh
        image: redis
        imagePullPolicy: IfNotPresent
        name: redis-sentinel
        ports:
        - containerPort: 26379
          name: sentinel-port
          protocol: TCP
        volumeMounts:
        - mountPath: /mnt/
          name: config-volume
          readOnly: false
      serviceAccountName: redis-sentinel
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          items:
          - key: redis-sentinel.conf
            path: redis-sentinel.conf
          name: redis-sentinel-config 
        name: config-volume


[root@master-1 sentinel]# vim redis-sentinel-service-sentinel.yaml
kind: Service
apiVersion: v1
metadata:
  labels:
    app: redis-sentinel-sentinel-ss
  name: redis-sentinel-sentinel-ss
  namespace: public-service
spec:
  clusterIP: None
  ports:
  - name: redis
    port: 26379
    targetPort: 26379
  selector:
    app: redis-sentinel-sentinel-ss
[root@master-1 sentinel]# 


[root@master-1 sentinel]#  kubectl create -f redis-sentinel-ss-sentinel.yaml -f redis-sentinel-service-sentinel.yaml
statefulset.apps/redis-sentinel-sentinel-ss created
service/redis-sentinel-sentinel-ss created


[root@master-1 sentinel]# kubectl get service -n public-service
NAME                         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE
redis-sentinel-master-ss     ClusterIP   None         <none>        6379/TCP    17m
redis-sentinel-sentinel-ss   ClusterIP   None         <none>        26379/TCP   53s
redis-sentinel-slave-ss      ClusterIP   None         <none>        6379/TCP    17m
[root@master-1 sentinel]# 

[root@master-1 sentinel]#  kubectl get statefulset -n public-service
NAME                         READY   AGE
redis-sentinel-master-ss     1/1     14m
redis-sentinel-sentinel-ss   0/3     68s
redis-sentinel-slave-ss      2/2     14m


[root@master-1 sentinel]# kubectl get pods -n public-service 
NAME                           READY   STATUS             RESTARTS   AGE
redis-sentinel-master-ss-0     1/1     Running            0          14m
redis-sentinel-sentinel-ss-0   0/1     CrashLoopBackOff   3          89s
redis-sentinel-slave-ss-0      1/1     Running            0          14m
redis-sentinel-slave-ss-1      1/1     Running            0          13m


[root@master-1 sentinel]#  kubectl create -f redis-sentinel-ss-sentinel.yaml -f redis-sentinel-service-sentinel.yaml
[root@master-1 sentinel]#  kubectl delete -f redis-sentinel-ss-sentinel.yaml -f redis-sentinel-service-sentinel.yaml


kubectl describe pod  redis-sentinel-sentinel-ss-0   -n public-service     
kubectl get event -n public-service 

0/3 nodes are available: 3 pod has unbound immediate PersistentVolumeClaims.

CrashLoopBackOff



kubectl create -f redis-sentinel-ss-sentinel.yaml
kubectl delete -f redis-sentinel-ss-sentinel.yaml







##后面未测试：
###8、查看哨兵状态
[root@k8s-master01 ~]# kubectl exec -ti redis-sentinel-sentinel-ss-0 -n public-service -- redis-cli -h 127.0.0.1 -p 26379 info Sentinel
######Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=mymaster,status=ok,address=172.168.6.111:6379,slaves=2,sentinels=3



###9、容灾测试

# 查看当前数据
[root@k8s-master01 ~]# kubectl exec -ti redis-sentinel-master-ss-0 -n public-service -- redis-cli -h 127.0.0.1 -p 6379 get test
"test_data"


查看状态
[root@k8s-master01 ~]# kubectl get pods -n public-service
NAME                           READY     STATUS    RESTARTS   AGE
redis-sentinel-sentinel-ss-0   1/1       Running   0          22m
redis-sentinel-sentinel-ss-1   1/1       Running   0          22m
redis-sentinel-sentinel-ss-2   1/1       Running   0          22m
redis-sentinel-slave-ss-0      1/1       Running   0          17h
redis-sentinel-slave-ss-1      1/1       Running   0          17h


查看sentinel状态
[root@k8s-master01 redis]# kubectl exec -ti redis-sentinel-sentinel-ss-2 -n public-service -- redis-cli -h 127.0.0.1 -p 26379 info Sentinel
######Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
sentinel_simulate_failure_flags:0
master0:name=mymaster,status=ok,address=172.168.6.116:6379,slaves=2,sentinels=3

[root@k8s-master01 redis]# kubectl exec -ti redis-sentinel-slave-ss-0 -n public-service -- redis-cli -h 127.0.0.1 -p 6379 info replication
###### Replication
role:slave
master_host:172.168.6.116
master_port:6379
master_link_status:up
master_last_io_seconds_ago:0
master_sync_in_progress:0
slave_repl_offset:82961
slave_priority:100
slave_read_only:1
connected_slaves:0
master_replid:4097ccd725a7ffc6f3767f7c726fc883baf3d7ef
master_replid2:603280e5266e0a6b0f299d2b33384c1fd8c3ee64
master_repl_offset:82961
second_repl_offset:68647
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:1
repl_backlog_histlen:82961


清理PV Terminating
kubectl patch pv pv-redis-sentinel-0 -p '{"metadata": {"finalizers": null}}'
kubectl patch pv pv-redis-sentinel-1 -p '{"metadata": {"finalizers": null}}'
kubectl patch pv pv-redis-sentinel-2 -p '{"metadata": {"finalizers": null}}'



##(七)、部署 Redis-Cluster集群

本篇将介绍如何在K8S上部署Redis集群。注意，这里所说的Redis 集群，指Redis Cluster而非Sentinel模式集群。
下图为Redis集群的架构图，每个Master都可以拥有多个Slave。当Master下线后，Redis集群会从多个Slave中选举出一个新的Master作为替代，而旧Master重新上线后变成新Master的Slave。


其包含了两种部署Redis集群的方式：
StatefulSet
Service&Deployment
两种方式各有优劣，对于像Redis、Mongodb、Zookeeper等有状态的服务，使用StatefulSet是首选方式。本文将主要介绍如何使用StatefulSet进行Redis集群的部署。

StatefulSet的概念非常重要，简单来说，其就是为了解决Pod重启、迁移后，Pod的IP、主机名等网络标识会改变而带来的问题。IP变化对于有状态的服务是难以接受的，如在Zookeeper集群的配置文件中，每个ZK节点都会记录其他节点的地址信息
server.1=192.168.229.160:2888:3888
server.2=192.168.229.161:2888:3888
server.3=192.168.229.162:2888:3888

但若某个ZK节点的Pod重启后改变了IP，那么就会导致该节点脱离集群，而如果该配置文件中不使用IP而使用IP对应的域名，则可避免该问题：
server.1=zk-node1:2888:3888
server.2=zk-node2:2888:3888
server.3=zk-node3:2888:3888

也即是说，对于有状态服务，我们最好使用固定的网络标识（如域名信息）来标记节点，当然这也需要应用程序的支持（如Zookeeper就支持在配置文件中写入主机域名）。

StatefulSet基于Headless Service（即没有Cluster IP的Service）为Pod实现了稳定的网络标志（包括Pod的hostname和DNS Records），在Pod重新调度后也保持不变。同时，结合PV/PVC，StatefulSet可以实现稳定的持久化存储，就算Pod重新调度后，还是能访问到原先的持久化数据。

下图为使用StatefulSet部署Redis的架构，无论是Master还是Slave，都作为StatefulSet的一个副本，并且数据通过PV进行持久化，对外暴露为一个Service，接受客户端请求。

本文参考项目的README中，简要介绍了基于StatefulSet的Redis创建步骤：
创建NFS存储
创建PV
创建Configmap
创建Redis StatefulSet
创建SVC
初始化Redis集群
这里，我们将参考如上步骤，实践操作并详细介绍Redis集群的部署过程。文中会涉及到很多K8S的概念，希望大家能提前了解学习。




###1、PV创建,NFS存储创建

####1)、创建NFS
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv1
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv2
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv3
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv4
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv5
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv6

其他方法：
[root@master-1 redis]# mkdir -p /root/redis-cluster/pv{1..6}

[root@master-1 redis]# vim /etc/exports
/root/redis-cluster/pv1 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-cluster/pv2 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-cluster/pv3 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-cluster/pv4 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-cluster/pv5 *(rw,sync,no_subtree_check,no_root_squash)
/root/redis-cluster/pv6 *(rw,sync,no_subtree_check,no_root_squash)


######配置生效
[root@master-1 mysql]# exportfs -r
######查看生效
[root@master-1 redis]# exportfs
/root/nfs_data  <world>
/root/web1      <world>
/root/redis-sentinel/0 <world>
/root/redis-sentinel/1 <world>
/root/redis-sentinel/2 <world>
/root/redis-cluster/pv1 <world>
/root/redis-cluster/pv2 <world>
/root/redis-cluster/pv3 <world>
/root/redis-cluster/pv4 <world>
/root/redis-cluster/pv5 <world>
/root/redis-cluster/pv6 <world>

[root@node-1 ~]# mkdir -p /root/redis-cluster/pv1
[root@node-1 ~]# mkdir -p /root/redis-cluster/pv2
[root@node-1 ~]# mkdir -p /root/redis-cluster/pv3
[root@node-1 ~]# mkdir -p /root/redis-cluster/pv4
[root@node-1 ~]# mkdir -p /root/redis-cluster/pv5
[root@node-1 ~]# mkdir -p /root/redis-cluster/pv6
[root@node-1 ~]# chmod -R 777 redis-cluster/
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv1  /root/redis-cluster/pv1
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv2  /root/redis-cluster/pv2
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv3  /root/redis-cluster/pv3
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv4  /root/redis-cluster/pv4
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv5  /root/redis-cluster/pv5
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv6  /root/redis-cluster/pv6



[root@node-2 ~]# mkdir -p /root/redis-cluster/pv1
[root@node-2 ~]# mkdir -p /root/redis-cluster/pv2
[root@node-2 ~]# mkdir -p /root/redis-cluster/pv3
[root@node-2 ~]# mkdir -p /root/redis-cluster/pv4
[root@node-2 ~]# mkdir -p /root/redis-cluster/pv5
[root@node-2 ~]# mkdir -p /root/redis-cluster/pv6
[root@node-2 ~]# chmod -R 777 redis-cluster/
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv1  /root/redis-cluster/pv1
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv2  /root/redis-cluster/pv2
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv3  /root/redis-cluster/pv3
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv4  /root/redis-cluster/pv4
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv5  /root/redis-cluster/pv5
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/redis-cluster/pv6  /root/redis-cluster/pv6



umount -t nfs 172.16.201.134:/root/redis-cluster/pv1



[root@node-1 ~]# df -h|grep 172.16.201.134
172.16.201.134:/root/web1                50G  3.5G   47G   7% /root/web1
172.16.201.134:/root/nfs_data            50G  3.5G   47G   7% /root/nfs_data
172.16.201.134:/root/redis-sentinel/0    50G  3.5G   47G   7% /root/redis-sentinel/0
172.16.201.134:/root/redis-sentinel/1    50G  3.5G   47G   7% /root/redis-sentinel/1
172.16.201.134:/root/redis-sentinel/2    50G  3.5G   47G   7% /root/redis-sentinel/2
172.16.201.134:/root/redis-sentinel/0    50G  3.5G   47G   7% /var/lib/kubelet/pods/1044079a-3090-4189-83e7-b1496f7bae63/volumes/kubernetes.io~nfs/pv-redis-sentinel-0
172.16.201.134:/root/redis-sentinel/2    50G  3.5G   47G   7% /var/lib/kubelet/pods/5472358e-c6b7-4ea3-8b60-5435f2e9136b/volumes/kubernetes.io~nfs/pv-redis-sentinel-2
172.16.201.134:/root/redis-cluster/pv1   50G  3.5G   47G   7% /root/redis-cluster/pv1
172.16.201.134:/root/redis-cluster/pv2   50G  3.5G   47G   7% /root/redis-cluster/pv2
172.16.201.134:/root/redis-cluster/pv3   50G  3.5G   47G   7% /root/redis-cluster/pv3
172.16.201.134:/root/redis-cluster/pv4   50G  3.5G   47G   7% /root/redis-cluster/pv4
172.16.201.134:/root/redis-cluster/pv5   50G  3.5G   47G   7% /root/redis-cluster/pv5
172.16.201.134:/root/redis-cluster/pv6   50G  3.5G   47G   7% /root/redis-cluster/pv6


[root@node-2 ~]#  df -h|grep 172.16.201.134
172.16.201.134:/root/nfs_data            50G  3.5G   47G   7% /root/nfs_data
172.16.201.134:/root/web1                50G  3.5G   47G   7% /root/web1
172.16.201.134:/root/redis-sentinel/0    50G  3.5G   47G   7% /root/redis-sentinel/0
172.16.201.134:/root/redis-sentinel/1    50G  3.5G   47G   7% /root/redis-sentinel/1
172.16.201.134:/root/redis-sentinel/2    50G  3.5G   47G   7% /root/redis-sentinel/2
172.16.201.134:/root/redis-sentinel/1    50G  3.5G   47G   7% /var/lib/kubelet/pods/86129264-759a-41e2-aa96-fed0c6ac7b85/volumes/kubernetes.io~nfs/pv-redis-sentinel-1
172.16.201.134:/root/redis-cluster/pv1   50G  3.5G   47G   7% /root/redis-cluster/pv1
172.16.201.134:/root/redis-cluster/pv2   50G  3.5G   47G   7% /root/redis-cluster/pv2
172.16.201.134:/root/redis-cluster/pv3   50G  3.5G   47G   7% /root/redis-cluster/pv3
172.16.201.134:/root/redis-cluster/pv4   50G  3.5G   47G   7% /root/redis-cluster/pv4
172.16.201.134:/root/redis-cluster/pv5   50G  3.5G   47G   7% /root/redis-cluster/pv5
172.16.201.134:/root/redis-cluster/pv6   50G  3.5G   47G   7% /root/redis-cluster/pv6
[root@node-2 ~]# 


####2)、创建PV
[root@master-1 cluster]# vim redis-pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv1
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv1
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv2
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv2
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv3
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv3
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv4
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv4
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv5
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv5
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: redis-pv6
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: "redis-cluster"
  nfs:
    path: /root/redis-cluster/pv6
    server: 172.16.201.134



[root@master-1 cluster]#  kubectl create -f  redis-pv.yml
persistentvolume/redis-pv1 created
persistentvolume/redis-pv2 created
persistentvolume/redis-pv3 created
persistentvolume/redis-pv4 created
persistentvolume/redis-pv5 created
persistentvolume/redis-pv6 created


######Pv都是Available状态：
[root@master-1 cluster]# kubectl get pv
NAME                  CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                     STORAGECLASS                   REASON   AGE
pv-redis-sentinel-0   1Gi        RWX            Recycle          Bound       public-service/redis-sentinel-master-storage-redis-sentinel-master-ss-0   redis-sentinel-storage-class            19h
pv-redis-sentinel-1   1Gi        RWX            Recycle          Bound       public-service/redis-sentinel-slave-storage-redis-sentinel-slave-ss-0     redis-sentinel-storage-class            19h
pv-redis-sentinel-2   1Gi        RWX            Recycle          Bound       public-service/redis-sentinel-slave-storage-redis-sentinel-slave-ss-1     redis-sentinel-storage-class            19h
redis-pv1             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s
redis-pv2             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s
redis-pv3             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s
redis-pv4             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s
redis-pv5             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s
redis-pv6             5Gi        RWO            Recycle          Available                                                                             redis-cluster                           7s

###2、创建服务相关
####1)、创建ConfigMap
[root@master-1 cluster]# vim cm.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: redis-cluster
data:
  update-node.sh: |
    #!/bin/sh
    REDIS_NODES="/data/nodes.conf"
    sed -i -e "/myself/ s/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/${POD_IP}/" ${REDIS_NODES}
    exec "$@"
  redis.conf: |+
    cluster-enabled yes
    cluster-require-full-coverage no
    cluster-node-timeout 15000
    cluster-config-file /data/nodes.conf
    cluster-migration-barrier 1
    dir /data
    appendonly yes
    protected-mode no

[root@master-1 cluster]# kubectl apply -f redis-cm.yml
configmap/redis-cluster created
[root@master-1 cluster]# kubectl get cm
NAME            DATA   AGE
redis-cluster   2      7s



[root@master-1 ~]# kubectl describe cm redis-cluster
Name:         redis-cluster
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
update-node.sh:
----
######!/bin/sh
REDIS_NODES="/data/nodes.conf"
sed -i -e "/myself/ s/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/${POD_IP}/" ${REDIS_NODES}
exec "$@"

redis.conf:
----
cluster-enabled yes
cluster-require-full-coverage no
cluster-node-timeout 15000
cluster-config-file /data/nodes.conf
cluster-migration-barrier 1
dir /data
appendonly yes
protected-mode no

Events:  <none>
[root@master-1 ~]# 



####2)、创建 StatefulSet-Redis 集群节点

[root@master-1 cluster]# vim redis-sts.yml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  serviceName: redis-cluster
  replicas: 6
  selector:
    matchLabels:
      app: redis-cluster
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      containers:
      - name: redis
        image: redis:6.2-alpine
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        command: ["/conf/update-node.sh", "redis-server", "/conf/redis.conf"]
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        volumeMounts:
        - name: conf
          mountPath: /conf
          readOnly: false
        - name: data
          mountPath: /data
          readOnly: false
      volumes:
      - name: conf
        configMap:
          name: redis-cluster
          defaultMode: 0755
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 5Gi
      storageClassName: redis-cluster

[root@master-1 cluster]# kubectl apply -f redis-sts.yml
statefulset.apps/redis-cluster created

######Running一个之后 再创建下一个：
[root@master-1 cluster]# kubectl get pods -l app=redis-cluster
NAME              READY   STATUS              RESTARTS   AGE
redis-cluster-0   1/1     Running             0          61s
redis-cluster-1   1/1     Running             0          37s
redis-cluster-2   1/1     Running             0          12s
redis-cluster-3   1/1     Running             0          9s
redis-cluster-4   1/1     Running             0          6s
redis-cluster-5   0/1     ContainerCreating   0          1s

[root@master-1 cluster]# kubectl get pods -l app=redis-cluster
NAME              READY   STATUS    RESTARTS   AGE
redis-cluster-0   1/1     Running   0          69s
redis-cluster-1   1/1     Running   0          45s
redis-cluster-2   1/1     Running   0          20s
redis-cluster-3   1/1     Running   0          17s
redis-cluster-4   1/1     Running   0          14s
redis-cluster-5   1/1     Running   0          9s


redis:6.2-alpine真快
[root@node-1 pv3]# docker images
REPOSITORY                                               TAG                 IMAGE ID            CREATED             SIZE
redis                                                    latest              5d89766432d0        34 hours ago        105MB


####3)、创建 Svc
Headless service是StatefulSet实现稳定网络标识的基础，我们需要提前创建。准备文件redis-svc.yml如下
[root@master-1 cluster]# vim redis-svc.yml
---
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
spec:
  type: ClusterIP
  ports:
  - port: 6379
    targetPort: 6379
    name: client
  - port: 16379
    targetPort: 16379
    name: gossip
  selector:
    app: redis-cluster
[root@master-1 cluster]# kubectl apply -f redis-svc.yml
service/redis-cluster created

[root@master-1 cluster]# kubectl get svc redis-cluster
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE
redis-cluster   ClusterIP   10.1.188.158   <none>        6379/TCP,16379/TCP   6s

我们建立的不是“无头”服务，是ClusterIP服务
可以看到，服务名称为redis-service，其CLUSTER-IP为None，表示这是一个“无头”服务。

######全部状态：
[root@master-1 cluster]# kubectl get pods,svc,pv
NAME                  READY   STATUS    RESTARTS   AGE
pod/redis-cluster-0   1/1     Running   0          3m59s
pod/redis-cluster-1   1/1     Running   0          3m35s
pod/redis-cluster-2   1/1     Running   0          3m10s
pod/redis-cluster-3   1/1     Running   0          3m7s
pod/redis-cluster-4   1/1     Running   0          3m4s
pod/redis-cluster-5   1/1     Running   0          2m59s

NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)              AGE
service/kubernetes      ClusterIP   10.1.0.1       <none>        443/TCP              8d
service/redis-cluster   ClusterIP   10.1.188.158   <none>        6379/TCP,16379/TCP   113s

NAME                                   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                                                                     STORAGECLASS                   REASON   AGE
persistentvolume/pv-redis-sentinel-0   1Gi        RWX            Recycle          Bound    public-service/redis-sentinel-master-storage-redis-sentinel-master-ss-0   redis-sentinel-storage-class            19h
persistentvolume/pv-redis-sentinel-1   1Gi        RWX            Recycle          Bound    public-service/redis-sentinel-slave-storage-redis-sentinel-slave-ss-0     redis-sentinel-storage-class            19h
persistentvolume/pv-redis-sentinel-2   1Gi        RWX            Recycle          Bound    public-service/redis-sentinel-slave-storage-redis-sentinel-slave-ss-1     redis-sentinel-storage-class            19h
persistentvolume/redis-pv1             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-0                                              redis-cluster                           5m49s
persistentvolume/redis-pv2             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-5                                              redis-cluster                           5m49s
persistentvolume/redis-pv3             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-4                                              redis-cluster                           5m49s
persistentvolume/redis-pv4             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-1                                              redis-cluster                           5m49s
persistentvolume/redis-pv5             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-3                                              redis-cluster                           5m49s
persistentvolume/redis-pv6             5Gi        RWO            Recycle          Bound    default/data-redis-cluster-2                                              redis-cluster                           5m49s
[root@master-1 cluster]# 


###3、初始化 Redis Cluster

####1)、初始化
[root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 {end}')


输出:
····
Can I set the above configuration? (type 'yes' to accept): yes
····



[root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli --cluster create --cluster-replicas 1 $(kubectl get pods -l app=redis-cluster -o jsonpath='{range.items[*]}{.status.podIP}:6379 {end}')
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 10.244.2.54:6379 to 10.244.2.52:6379
Adding replica 10.244.1.49:6379 to 10.244.1.47:6379
Adding replica 10.244.1.48:6379 to 10.244.2.53:6379
M: efa0c84caafca28004f2a14b045975c8baf5e385 10.244.2.52:6379
   slots:[0-5460] (5461 slots) master
M: 3976b6e6ac99de968259705c65956e39bcd2a0cc 10.244.1.47:6379
   slots:[5461-10922] (5462 slots) master
M: 6dfe05f87940c752418b97f1b5bc3d8114633661 10.244.2.53:6379
   slots:[10923-16383] (5461 slots) master
S: a3b51b598d3e7f948472ebcfe0aaa5a606f3378b 10.244.1.48:6379
   replicates 6dfe05f87940c752418b97f1b5bc3d8114633661
S: 0d212bbd8e0ca2933bdd64db7937fdd7b550b73c 10.244.2.54:6379
   replicates efa0c84caafca28004f2a14b045975c8baf5e385
S: 3b04c7eb9c2bd8fa82bf5f5808d62d49f09c6cf0 10.244.1.49:6379
   replicates 3976b6e6ac99de968259705c65956e39bcd2a0cc
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
..
>>> Performing Cluster Check (using node 10.244.2.52:6379)
M: efa0c84caafca28004f2a14b045975c8baf5e385 10.244.2.52:6379
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: 3b04c7eb9c2bd8fa82bf5f5808d62d49f09c6cf0 10.244.1.49:6379
   slots: (0 slots) slave
   replicates 3976b6e6ac99de968259705c65956e39bcd2a0cc
M: 6dfe05f87940c752418b97f1b5bc3d8114633661 10.244.2.53:6379
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
M: 3976b6e6ac99de968259705c65956e39bcd2a0cc 10.244.1.47:6379
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
S: 0d212bbd8e0ca2933bdd64db7937fdd7b550b73c 10.244.2.54:6379
   slots: (0 slots) slave
   replicates efa0c84caafca28004f2a14b045975c8baf5e385
S: a3b51b598d3e7f948472ebcfe0aaa5a606f3378b 10.244.1.48:6379
   slots: (0 slots) slave
   replicates 6dfe05f87940c752418b97f1b5bc3d8114633661
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
[root@master-1 cluster]# 

####2)、验证
######验证集群部署
[root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:26
cluster_stats_messages_pong_sent:31
cluster_stats_messages_sent:57
cluster_stats_messages_ping_received:26
cluster_stats_messages_pong_received:26
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:57
[root@master-1 cluster]# 



[root@master-1 ~]# kubectl exec -it redis-cluster-0 -- redis-cli ping
PONG


######查看各个redis的状态
[root@master-1 ~]#  for x in $(seq 0 5); do echo "redis-cluster-$x"; kubectl exec redis-cluster-$x -- redis-cli ping; echo; done
redis-cluster-0
PONG

redis-cluster-1
PONG

redis-cluster-2
PONG

redis-cluster-3
PONG

redis-cluster-4
PONG

redis-cluster-5
PONG

[root@master-1 ~]# 

######查看各个redis的主从状态
 [root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli role
 1) "master"
 2) (integer) 3486
 3) 1) 1) "10.244.2.54"
       2) "6379"
       3) "3486"

 如上可以看到，其为master，slave为10.244.2.54即redis-cluster-4`。

 [root@node-1 pv3]# kubectl get pods -o wide
 NAME              READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
 redis-cluster-0   1/1     Running   0          59m   10.244.2.52   node-2   <none>           <none>
 redis-cluster-1   1/1     Running   0          59m   10.244.1.47   node-1   <none>           <none>
 redis-cluster-2   1/1     Running   0          58m   10.244.2.53   node-2   <none>           <none>
 redis-cluster-3   1/1     Running   0          58m   10.244.1.48   node-1   <none>           <none>
 redis-cluster-4   1/1     Running   0          58m   10.244.2.54   node-2   <none>           <none>
 redis-cluster-5   1/1     Running   0          58m   10.244.1.49   node-1   <none>           <none>



查看全部
[root@master-1 cluster]# for x in $(seq 0 5); do echo "redis-cluster-$x"; kubectl exec redis-cluster-$x -- redis-cli role; echo; done
redis-cluster-0
master
70
10.244.2.54
6379
70

redis-cluster-1
master
56
10.244.1.49
6379
56

redis-cluster-2
master
70
10.244.1.48
6379
70

redis-cluster-3
slave
10.244.2.53
6379
connected
70

redis-cluster-4
slave
10.244.2.52
6379
connected
70

redis-cluster-5
slave
10.244.1.47
6379
connected
56

如上信息可确定：master,slave对应信息：
redis-cluster-0是master
redis-cluster-1是master
redis-cluster-2是master
redis-cluster-3是slave
redis-cluster-4是slave
redis-cluster-5是slave



######数据文件生成：
[root@node-1 redis-cluster]# ll *
pv1:
total 8
-rw-r--r-- 1 root root   0 Sep 30 13:46 1
-rw-r--r-- 1 root root   0 Sep 30 14:03 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf

pv2:
total 12
-rw-r--r-- 1 root root  92 Sep 30 14:31 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf

pv3:
total 12
-rw-r--r-- 1 root root  92 Sep 30 14:31 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf

pv4:
total 8
-rw-r--r-- 1 root root   0 Sep 30 14:03 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf

pv5:
total 12
-rw-r--r-- 1 root root  92 Sep 30 14:31 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf

pv6:
total 8
-rw-r--r-- 1 root root   0 Sep 30 14:03 appendonly.aof
-rw-r--r-- 1 root root 175 Sep 30 14:31 dump.rdb
-rw-r--r-- 1 root root 793 Sep 30 14:31 nodes.conf
[root@node-1 redis-cluster]# 



######内部域名相关：
同时，每个Pod都会得到集群内的一个DNS域名，格式为$(podname).$(service name).$(namespace).svc.cluster.local，也即是：

redis-app-0.redis-service.default.svc.cluster.local
redis-app-1.redis-service.default.svc.cluster.local
...以此类推...



[root@node-1 pv3]# kubectl get pods -o wide
NAME              READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
redis-cluster-0   1/1     Running   0          59m   10.244.2.52   node-2   <none>           <none>
redis-cluster-1   1/1     Running   0          59m   10.244.1.47   node-1   <none>           <none>
redis-cluster-2   1/1     Running   0          58m   10.244.2.53   node-2   <none>           <none>
redis-cluster-3   1/1     Running   0          58m   10.244.1.48   node-1   <none>           <none>
redis-cluster-4   1/1     Running   0          58m   10.244.2.54   node-2   <none>           <none>
redis-cluster-5   1/1     Running   0          58m   10.244.1.49   node-1   <none>           <none>


在K8S集群内部，这些Pod就可以利用该域名互相通信。我们可以使用busybox镜像的nslookup检验这些域名：
[root@k8s-node1 ~]# kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh 
If you don't see a command prompt, try pressing enter.
/ # nslookup redis-app-0.redis-service
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      redis-app-0.redis-service
Address 1: 192.168.169.207 redis-app-0.redis-service.default.svc.cluster.local

可以看到， redis-app-0的IP为192.168.169.207。当然，若Redis Pod迁移或是重启（我们可以手动删除掉一个Redis Pod来测试），则IP是会改变的，但Pod的域名、SRV records、A record都不会改变。

另外可以发现，我们之前创建的pv都被成功绑定了。

 

###4、管理 Redis Cluster（不用必须做）
常用的Redis-tribe工具进行集群的初始化。

创建Ubuntu容器
由于Redis集群必须在所有节点启动后才能进行初始化，而如果将初始化逻辑写入Statefulset中，则是一件非常复杂而且低效的行为。这里，本人不得不称赞一下原项目作者的思路，值得学习。也就是说，我们可以在K8S上创建一个额外的容器，专门用于进行K8S集群内部某些服务的管理控制。

这里，我们专门启动一个Ubuntu的容器，可以在该容器中安装Redis-tribe，进而初始化Redis集群，执行：
kubectl run -i --tty ubuntu --image=ubuntu --restart=Never /bin/bash

成功后，我们可以进入ubuntu容器中，原项目要求执行如下命令安装基本的软件环境：
apt-get update
apt-get install -y vim wget python2.7 python-pip redis-tools dnsutils

但是，需要注意的是，在我们天朝，执行上述命令前需要提前做一件必要的工作——换源，否则你懂得。我们使用阿里云的Ubuntu源，执行：


root@ubuntu:/# cat > /etc/apt/sources.list << EOF
> deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
> deb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse
> 
> deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
> deb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse
> 
> deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
> deb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse
> 
> deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
> deb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse
> 
> deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
> deb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse
> EOF
源修改完毕后，就可以执行上面的两个命令。

初始化集群
首先，我们需要安装redis-trib：
pip install redis-trib

然后，创建只有Master节点的集群：
redis-trib.py create \
  `dig +short redis-app-0.redis-service.default.svc.cluster.local`:6379 \
  `dig +short redis-app-1.redis-service.default.svc.cluster.local`:6379 \
  `dig +short redis-app-2.redis-service.default.svc.cluster.local`:6379



其次，为每个Master添加Slave：
redis-trib.py replicate \
  --master-addr `dig +short redis-app-0.redis-service.default.svc.cluster.local`:6379 \
  --slave-addr `dig +short redis-app-3.redis-service.default.svc.cluster.local`:6379

redis-trib.py replicate \
  --master-addr `dig +short redis-app-1.redis-service.default.svc.cluster.local`:6379 \
  --slave-addr `dig +short redis-app-4.redis-service.default.svc.cluster.local`:6379

redis-trib.py replicate \
  --master-addr `dig +short redis-app-2.redis-service.default.svc.cluster.local`:6379 \
  --slave-addr `dig +short redis-app-5.redis-service.default.svc.cluster.local`:6379

至此，我们的Redis集群就真正创建完毕了，连到任意一个Redis Pod中检验一下：


###5、Redis Cluster其他操作

[root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli cluster nodes
3b04c7eb9c2bd8fa82bf5f5808d62d49f09c6cf0 10.244.1.49:6379@16379 slave 3976b6e6ac99de968259705c65956e39bcd2a0cc 0 1632985874700 2 connected
6dfe05f87940c752418b97f1b5bc3d8114633661 10.244.2.53:6379@16379 master - 0 1632985875712 3 connected 10923-16383
3976b6e6ac99de968259705c65956e39bcd2a0cc 10.244.1.47:6379@16379 master - 0 1632985875000 2 connected 5461-10922
0d212bbd8e0ca2933bdd64db7937fdd7b550b73c 10.244.2.54:6379@16379 slave efa0c84caafca28004f2a14b045975c8baf5e385 0 1632985877729 1 connected
efa0c84caafca28004f2a14b045975c8baf5e385 10.244.2.52:6379@16379 myself,master - 0 1632985877000 1 connected 0-5460
a3b51b598d3e7f948472ebcfe0aaa5a606f3378b 10.244.1.48:6379@16379 slave 6dfe05f87940c752418b97f1b5bc3d8114633661 0 1632985877000 3 connected

[root@master-1 cluster]# kubectl exec -it redis-cluster-0 -- redis-cli cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:2382
cluster_stats_messages_pong_sent:2478
cluster_stats_messages_sent:4860
cluster_stats_messages_ping_received:2473
cluster_stats_messages_pong_received:2382
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:4860


[root@master-1 cluster]# for x in $(seq 0 5); do echo "redis-cluster-$x"; kubectl exec redis-cluster-$x -- redis-cli role; echo; done
redis-cluster-0
master
135366
10.244.2.54
6379
135366

redis-cluster-1
master
135338
10.244.1.49
6379
135338

redis-cluster-2
master
135366
10.244.1.48
6379
135366

redis-cluster-3
slave
10.244.2.53
6379
connected
135366

redis-cluster-4
slave
10.244.2.52
6379
connected
135366

redis-cluster-5
slave
10.244.1.47
6379
connected
135338



######问题：
[root@master-1 cluster]# kubectl exec -it pod/redis-cluster-0 -- /bin/bash
OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: "/bin/bash": stat /bin/bash: no such file or directory: unknown
command terminated with exit code 126


这个错误说明 镜像不包含适合bash的风格操作，没有这样的文件或目录
可能你的镜像基于busybox，它没有bash shell。但他在/bin/sh有一个shell
直接执行kubectl exec -ti redis-cluster-0 /bin/sh 就可以进入容器里面

解决：
[root@master-1 cluster]# kubectl exec -it pod/redis-cluster-0 -- /bin/sh
/data # ls
1               appendonly.aof  dump.rdb        nodes.conf

/data # netstat  -anp
Active Internet connections (servers and established)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 0.0.0.0:16379           0.0.0.0:*               LISTEN      1/redis-server *:63
tcp        0      0 0.0.0.0:6379            0.0.0.0:*               LISTEN      1/redis-server *:63
tcp        0      0 10.244.2.52:49614       10.244.1.48:16379       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:36172       10.244.1.47:16379       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:54804       10.244.2.54:16379       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:16379       10.244.1.48:53314       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:16379       10.244.1.47:42512       ESTABLISHED 1/redis-server *:63
tcp        0      0 127.0.0.1:49042         127.0.0.1:6379          TIME_WAIT   -
tcp        0      0 10.244.2.52:16379       10.244.2.54:36702       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:6379        10.244.2.54:51654       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:54670       10.244.1.49:16379       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:16379       10.244.2.53:48374       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:36410       10.244.2.53:16379       ESTABLISHED 1/redis-server *:63
tcp        0      0 10.244.2.52:16379       10.244.1.49:55204       ESTABLISHED 1/redis-server *:63
tcp        0      0 :::16379                :::*                    LISTEN      1/redis-server *:63
tcp        0      0 :::6379                 :::*                    LISTEN      1/redis-server *:63
Active UNIX domain sockets (servers and established)
Proto RefCnt Flags       Type       State         I-Node PID/Program name    Path
/data # 


/conf # ls -al
total 0
drwxrwxrwx    3 root     root            99 Sep 30 06:03 .
drwxr-xr-x    1 root     root            41 Sep 30 06:03 ..
drwxr-xr-x    2 root     root            46 Sep 30 06:03 ..2021_09_30_06_03_01.833951206
lrwxrwxrwx    1 root     root            31 Sep 30 06:03 ..data -> ..2021_09_30_06_03_01.833951206
lrwxrwxrwx    1 root     root            17 Sep 30 06:03 redis.conf -> ..data/redis.conf
lrwxrwxrwx    1 root     root            21 Sep 30 06:03 update-node.sh -> ..data/update-node.sh
/conf # cat update-node.sh 
######!/bin/sh
REDIS_NODES="/data/nodes.conf"
sed -i -e "/myself/ s/[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}/${POD_IP}/" ${REDIS_NODES}
exec "$@"
/conf # 

/conf # cat redis.conf 
cluster-enabled yes
cluster-require-full-coverage no
cluster-node-timeout 15000
cluster-config-file /data/nodes.conf
cluster-migration-barrier 1
dir /data
appendonly yes
protected-mode no
/conf # 

/data # df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                  50.0G      5.5G     44.5G  11% /
tmpfs                    64.0M         0     64.0M   0% /dev
tmpfs                   909.7M         0    909.7M   0% /sys/fs/cgroup
/dev/mapper/centos-root
                         50.0G      5.5G     44.5G  11% /conf
172.16.201.134:/root/redis-cluster/pv1
                         50.0G      3.4G     46.5G   7% /data
/dev/mapper/centos-root
                         50.0G      5.5G     44.5G  11% /dev/termination-log
/dev/mapper/centos-root
                         50.0G      5.5G     44.5G  11% /etc/resolv.conf
/dev/mapper/centos-root
                         50.0G      5.5G     44.5G  11% /etc/hostname
/dev/mapper/centos-root
                         50.0G      5.5G     44.5G  11% /etc/hosts
shm                      64.0M         0     64.0M   0% /dev/shm
tmpfs                   909.7M     12.0K    909.7M   0% /run/secrets/kubernetes.io/serviceaccount
tmpfs                   909.7M         0    909.7M   0% /proc/acpi
tmpfs                    64.0M         0     64.0M   0% /proc/kcore
tmpfs                    64.0M         0     64.0M   0% /proc/keys
tmpfs                    64.0M         0     64.0M   0% /proc/timer_list
tmpfs                    64.0M         0     64.0M   0% /proc/timer_stats
tmpfs                    64.0M         0     64.0M   0% /proc/sched_debug
tmpfs                   909.7M         0    909.7M   0% /proc/scsi
tmpfs                   909.7M         0    909.7M   0% /sys/firmware

######以外发现：
[root@master-1 ~]# docker images
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
busybox                                                           latest              16ea53ea7c65        3 weeks ago         1.24MB

busybox很小，下次做景象，用busybox



######改一下NodePort，做下测试
[root@master-1 cluster]# cat redis-svc.yml 
---
apiVersion: v1
kind: Service
metadata:
  name: redis-cluster
spec:
  type: NodePort
  ports:
  - port: 6379
    targetPort: 6379
    nodePort: 31000
    name: client
  - port: 16379
    targetPort: 16379
    name: gossip
  selector:
    app: redis-cluster

[root@master-1 cluster]# redis-cli -p 31000
127.0.0.1:31000> role
1) "master"
2) (integer) 140210
3) 1) 1) "10.244.2.54"
      2) "6379"
      3) "140210"
127.0.0.1:31000> 

连接成功


###6、主从切换测试
在K8S上搭建完好Redis集群后，我们最关心的就是其原有的高可用机制是否正常。这里，我们可以任意挑选一个Master的Pod来测试集群的主从切换机制
如pod/redis-cluster-1：

[root@master-1 cluster]# kubectl get pods,svc -o wide   
NAME                  READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/redis-cluster-0   1/1     Running   0          7d21h   10.244.2.52   node-2   <none>           <none>
pod/redis-cluster-1   1/1     Running   0          7d21h   10.244.1.47   node-1   <none>           <none>
pod/redis-cluster-2   1/1     Running   0          7d21h   10.244.2.53   node-2   <none>           <none>
pod/redis-cluster-3   1/1     Running   0          7d21h   10.244.1.48   node-1   <none>           <none>
pod/redis-cluster-4   1/1     Running   0          7d21h   10.244.2.54   node-2   <none>           <none>
pod/redis-cluster-5   1/1     Running   0          7d21h   10.244.1.49   node-1   <none>           <none>

NAME                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                          AGE   SELECTOR
service/kubernetes      ClusterIP   10.1.0.1      <none>        443/TCP                          15d   <none>
service/redis-cluster   NodePort    10.1.231.82   <none>        6379:31000/TCP,16379:30701/TCP   82m   app=redis-cluster

进入pod/redis-cluster-1
[root@master-1 cluster]# kubectl exec -it pod/redis-cluster-1 -- /bin/sh

/data # redis-cli 
127.0.0.1:6379> role
1) "master"
2) (integer) 140784
3) 1) 1) "10.244.1.49"
      2) "6379"
      3) "140784"
127.0.0.1:6379> 

如上可以看到，其为master，slave为10.244.1.49 即redis-cluster-5。
接着，我们手动删除pod/redis-cluster-1：
[root@master-1 cluster]# kubectl delete pods redis-cluster-1 -n default    
pod "redis-cluster-1" deleted
[root@master-1 cluster]# 



[root@master-1 cluster]#  kubectl get pods redis-cluster-1 -o wide           
NAME              READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
redis-cluster-1   1/1     Running   0          30s   10.244.1.50   node-1   <none>           <none>
如上，IP改变为10.244.1.50。我们再进入redis-cluster-1内部查看：

[root@master-1 cluster]#  kubectl exec -it pod/redis-cluster-1 -- /bin/sh
/data # redis-cli
127.0.0.1:6379> role
1) "master"
2) (integer) 98
3) 1) 1) "10.244.1.49"
      2) "6379"
      3) "98"
127.0.0.1:6379> 

如上，redis-cluster-1变成了slave，从属于它之前的从节点10.244.1.47即redis-cluster-5。





查看新的 Pod 的运行情况，相当于tail着 实时的 容器状态：
[root@master-1 ~]# kubectl get pods -l redis-cluster-1 --watch      





###7、疑问
至此，大家可能会疑惑，前面讲了这么多似乎并没有体现出StatefulSet的作用，其提供的稳定标志redis-cluster-*仅在初始化集群的时候用到，而后续Redis Pod的通信或配置文件中并没有使用该标志。我想说，是的，本文使用StatefulSet部署Redis确实没有体现出其优势，还不如介绍Zookeeper集群来的明显，不过没关系，学到知识就好。

那为什么没有使用稳定的标志，Redis Pod也能正常进行故障转移呢？这涉及了Redis本身的机制。因为，Redis集群中每个节点都有自己的NodeId（保存在自动生成的nodes.conf中），并且该NodeId不会随着IP的变化和变化，这其实也是一种固定的网络标志。也就是说，就算某个Redis Pod重启了，该Pod依然会加载保存的NodeId来维持自己的身份。我们可以在NFS上查看redis-cluster-1的nodes.conf文件：
[root@master-1 ~]# cat /root/redis-cluster/pv1/nodes.conf
3b04c7eb9c2bd8fa82bf5f5808d62d49f09c6cf0 10.244.1.49:6379@16379 slave 3976b6e6ac99de968259705c65956e39bcd2a0cc 0 1633663260487 2 connected
6dfe05f87940c752418b97f1b5bc3d8114633661 10.244.2.53:6379@16379 master - 0 1633663259458 3 connected 10923-16383
3976b6e6ac99de968259705c65956e39bcd2a0cc 10.244.1.50:6379@16379 master - 1633663254407 1633663251381 2 disconnected 5461-10922
0d212bbd8e0ca2933bdd64db7937fdd7b550b73c 10.244.2.54:6379@16379 slave efa0c84caafca28004f2a14b045975c8baf5e385 0 1633663260000 1 connected
efa0c84caafca28004f2a14b045975c8baf5e385 10.244.2.52:6379@16379 myself,master - 0 1633663256000 1 connected 0-5460
a3b51b598d3e7f948472ebcfe0aaa5a606f3378b 10.244.1.48:6379@16379 slave 6dfe05f87940c752418b97f1b5bc3d8114633661 0 1633663260000 3 connected
vars currentEpoch 6 lastVoteEpoch 0
[root@master-1 ~]# 

如上，第一列为NodeId，稳定不变；第二列为IP和端口信息，可能会改变。

这里，我们介绍NodeId的两种使用场景：
当某个Slave Pod断线重连后IP改变，但是Master发现其NodeId依旧， 就认为该Slave还是之前的Slave。
当某个Master Pod下线后，集群在其Slave中选举重新的Master。待旧Master上线后，集群发现其NodeId依旧，会让旧Master变成新Master的slave。
对于这两种场景，大家有兴趣的话还可以自行测试，注意要观察Redis的日志。






##(八)、k8s部署 Mongodb集群
###一、NFS建立
[root@master-1 ~]# mkdir mongodb-share/
[root@master-1 ~]# chmod 777 mongodb-share/
[root@master-1 ~]#  vim /etc/exports
/root/nfs_data *(rw,no_root_squash,sync) 
/root/web1 *(rw,no_root_squash,sync) 
/root/redis-sentinel/0 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-sentinel/1 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-sentinel/2 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv1 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv2 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv3 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv4 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv5 *(rw,sync,no_subtree_check,no_root_squash) 
/root/redis-cluster/pv6 *(rw,sync,no_subtree_check,no_root_squash) 
/root/mongodb-share *(rw,sync,no_root_squash,no_all_squash) 

[root@master-1 ~]# exportfs -r 
[root@master-1 ~]# exportfs
/root/nfs_data  <world>
/root/web1      <world>
/root/redis-sentinel/0 <world>
/root/redis-sentinel/1 <world>
/root/redis-sentinel/2 <world>
/root/redis-cluster/pv1 <world>
/root/redis-cluster/pv2 <world>
/root/redis-cluster/pv3 <world>
/root/redis-cluster/pv4 <world>
/root/redis-cluster/pv5 <world>
/root/redis-cluster/pv6 <world>
/root/mongodb-share <world>

节点mount上
[root@node-1 ~]# mkdir mongodb-share
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/mongodb-share mongodb-share

[root@node-2 ~]# mkdir mongodb-share
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/mongodb-share mongodb-share

[root@node-1 ~]# df -h|grep mongodb
172.16.201.134:/root/mongodb-share       50G  3.5G   47G   7% /root/mongodb-share
[root@node-2 ~]#  df -h|grep mongodb
172.16.201.134:/root/mongodb-share       50G  3.5G   47G   7% /root/mongodb-share

###二、创建service-rbac
1、创建serviceaccount
[root@master-1 mongodb]# vim serviceaccount.yaml
apiVersion: v1 
kind: ServiceAccount 
metadata: 
  name: nfs-provisioner 
[root@master-1 mongodb]# kubectl apply -f serviceaccount.yaml
serviceaccount/nfs-provisioner created

[root@master-1 mongodb]# kubectl get ServiceAccount -o wide           
NAME              SECRETS   AGE
default           1         22d
nfs-provisioner   1         49s



2、创建service-rbac
[root@master-1 mongodb]# vim service-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  namespace: kube-system
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  namespace: kube-system
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: kube-system
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
[root@master-1 mongodb]# 

[root@master-1 mongodb]# kubectl apply -f  service-rbac.yaml
serviceaccount/nfs-client-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created

[root@master-1 mongodb]# kubectl get RoleBinding -nkube-system  -o wide   
NAME                                                ROLE                                                  AGE   USERS                                                   GROUPS                                                          SERVICEACCOUNTS
kube-proxy                                          Role/kube-proxy                                       22d                                                           system:bootstrappers:kubeadm:default-node-token                 
kubeadm:kubelet-config-1.19                         Role/kubeadm:kubelet-config-1.19                      22d                                                           system:nodes, system:bootstrappers:kubeadm:default-node-token   
kubeadm:nodes-kubeadm-config                        Role/kubeadm:nodes-kubeadm-config                     22d                                                           system:bootstrappers:kubeadm:default-node-token, system:nodes   
leader-locking-nfs-client-provisioner               Role/leader-locking-nfs-client-provisioner            83s                                                                                                                           kube-system/nfs-client-provisioner
system::extension-apiserver-authentication-reader   Role/extension-apiserver-authentication-reader        22d   system:kube-controller-manager, system:kube-scheduler                                                                   
system::leader-locking-kube-controller-manager      Role/system::leader-locking-kube-controller-manager   22d   system:kube-controller-manager                                                                                          kube-system/kube-controller-manager
system::leader-locking-kube-scheduler               Role/system::leader-locking-kube-scheduler            22d   system:kube-scheduler                                                                                                   kube-system/kube-scheduler
system:controller:bootstrap-signer                  Role/system:controller:bootstrap-signer               22d                                                                                                                           kube-system/bootstrap-signer
system:controller:cloud-provider                    Role/system:controller:cloud-provider                 22d                                                                                                                           kube-system/cloud-provider
system:controller:token-cleaner                     Role/system:controller:token-cleaner                  22d                                                                                                                           kube-system/token-cleaner
[root@master-1 mongodb]# 



3、创建nfs-provisioner-deploy
[root@master-1 mongodb]# vim nfs-provisioner-deploy.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  name: mongodb-nfs
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: mongodb-nfs
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: mongodb-nfs
    spec:
      serviceAccount: nfs-client-provisioner
      imagePullSecrets:
      - name: regcred
      containers:
        - name: mongodb-nfs
          image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner:latest
          volumeMounts:
            - name: mongodb-nfs-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: asd
            - name: NFS_SERVER
              value: 172.16.201.134
            - name: NFS_PATH
              value: /root/mongodb-share
      volumes:
        - name: mongodb-nfs-root
          nfs:
            server: 172.16.201.134
            path: /root/mongodb-share

[root@master-1 mongodb]# kubectl apply -f nfs-provisioner-deploy.yaml
deployment.apps/mongodb-nfs created

[root@master-1 mongodb]#  kubectl get pod -l app=mongodb-nfs  -n kube-system 
NAME                           READY   STATUS    RESTARTS   AGE
mongodb-nfs-5f6fd65ff9-dm7wv   1/1     Running   0          56s

4、创建storageclass
[root@master-1 mongodb]# vim storageclass.yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: mongodb-nfs
provisioner: asd
[root@master-1 mongodb]# kubectl apply -f storageclass.yaml
storageclass.storage.k8s.io/mongodb-nfs created

[root@master-1 mongodb]# kubectl get storageclasses
NAME          PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
mongodb-nfs   asd           Delete          Immediate           false                  21s

到此NFS StorageClass存储类服务创建成功!!！

###三、部署Mongodb
####1、创建mongo-service
[root@master-1 mongodb]# vim mongo-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: mongo
spec:
  ports:
  - name: mongo
    port: 27017
    targetPort: 27017
  clusterIP: None
  selector:
    role: mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongo-service
  labels:
    app: mongo
spec:
  ports:
  - name: mongo-http
    port: 27017
    targetPort: 27017
    nodePort: 27017
  selector:
    role: mongo
  type: NodePort


[root@master-1 mongodb]# kubectl apply -f mongo-service.yaml                           
service/mongo created
service/mongo-service created

[root@master-1 mongodb]# kubectl get svc
NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                          AGE
kubernetes      ClusterIP   10.1.0.1      <none>        443/TCP                          22d
mongo           ClusterIP   None          <none>        27017/TCP                        81s
mongo-service   NodePort    10.1.98.245   <none>        27017:27017/TCP                  81s
redis-cluster   NodePort    10.1.231.82   <none>        6379:31000/TCP,16379:30701/TCP   6d7h


####2、创建mongo-statefulset
[root@master-1 mongodb]# vim mongo-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: mongo
  replicas: 3
  selector:
    matchLabels:
      role: mongo
      environment: test
  template:
    metadata:
      labels:
        app: mongo
        role: mongo
        environment: test
    spec:
      containers:
      - name: mongod-container
        image: mongo:3.4
        command:
          - mongod
          - "--bind_ip"
          - "0.0.0.0"
          - "--replSet"
          - "rs0"
        resources:
          requests:
            cpu: 0.2
            memory: 200Mi
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongodb-persistent-storage-claim
          mountPath: /data/db
      - name: mongo-sidecar
        image: cvallance/mongo-k8s-sidecar
        env:
          - name: MONGO_SIDECAR_POD_LABELS
            value: "role=mongo,environment=test"
      volumes:
      - name: secrets-volume
        secret:
          secretName: shared-bootstrap-data
          defaultMode: 256
  volumeClaimTemplates:
  - metadata:
      name: mongodb-persistent-storage-claim
      annotations:
        volume.beta.kubernetes.io/storage-class: mongodb-nfs
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
[root@master-1 mongodb]# 

[root@master-1 mongodb]# kubectl apply -f mongo-statefulset.yaml
statefulset.apps/mongo created
[root@master-1 mongodb]# kubectl delete -f mongo-statefulset.yaml


[root@master-1 mongodb]# kubectl get StatefulSet  -o wide            
NAME            READY   AGE    CONTAINERS         IMAGES
mongo           0/3     2m1s   mongod-container   mongo:3.4
redis-cluster   6/6     14d    redis              redis:6.2-alpine


[root@master-1 mongodb]#  kubectl get pod --watch  
NAME              READY   STATUS              RESTARTS   AGE
mongo-0           0/2     ContainerCreating   0          4s
redis-cluster-0   1/1     Running             0          14d
redis-cluster-1   1/1     Running             0          6d6h
redis-cluster-2   1/1     Running             0          14d
redis-cluster-3   1/1     Running             0          14d
redis-cluster-4   1/1     Running             0          14d
redis-cluster-5   1/1     Running             0          14d
mongo-0           2/2     Running             0          19s
mongo-1           0/2     Pending             0          0s
mongo-1           0/2     Pending             0          0s
mongo-1           0/2     ContainerCreating   0          0s

[root@master-1 mongodb]# kubectl get pod  -o wide 
NAME              READY   STATUS    RESTARTS   AGE    IP            NODE     NOMINATED NODE   READINESS GATES
mongo-0           2/2     Running   0          26s    10.244.2.61   node-2   <none>           <none>
mongo-1           2/2     Running   0          50s    10.244.1.56   node-1   <none>           <none>
mongo-2           2/2     Running   0          72s    10.244.1.55   node-1   <none>           <none>
redis-cluster-0   1/1     Running   0          14d    10.244.2.52   node-2   <none>           <none>
redis-cluster-1   1/1     Running   0          6d6h   10.244.1.50   node-1   <none>           <none>
redis-cluster-2   1/1     Running   0          14d    10.244.2.53   node-2   <none>           <none>
redis-cluster-3   1/1     Running   0          14d    10.244.1.48   node-1   <none>           <none>
redis-cluster-4   1/1     Running   0          14d    10.244.2.54   node-2   <none>           <none>
redis-cluster-5   1/1     Running   0          14d    10.244.1.49   node-1   <none>           <none>

加完app=mongo，舒服一些
[root@master-1 mongodb]# kubectl get pod,svc -l app=mongo -o wide
NAME          READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/mongo-0   2/2     Running   0          96s     10.244.2.61   node-2   <none>           <none>
pod/mongo-1   2/2     Running   0          2m      10.244.1.56   node-1   <none>           <none>
pod/mongo-2   2/2     Running   0          2m22s   10.244.1.55   node-1   <none>           <none>

NAME                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)           AGE   SELECTOR
service/mongo           ClusterIP   None          <none>        27017/TCP         57m   role=mongo
service/mongo-service   NodePort    10.1.98.245   <none>        27017:27017/TCP   57m   role=mongo



[root@node-1 mongodb-share]# ls
default-mongodb-persistent-storage-claim-mongo-0-pvc-590e3a5f-39af-4cb3-a005-0570264787d1
default-mongodb-persistent-storage-claim-mongo-1-pvc-a815ab9a-c2fe-4036-b7fc-4cac32dc3ad0
default-mongodb-persistent-storage-claim-mongo-2-pvc-00b6d3e1-5c2e-4a4d-8e0d-affad057464d


######验证http访问，172.16.201.134-136为我的nodeIP地址，27017为nodeport自动生成的端口号；
[root@master-1 mongodb]# curl http://172.16.201.134:27017
It looks like you are trying to access MongoDB over HTTP on the native driver port.
[root@master-1 mongodb]# curl http://172.16.201.135:27017
It looks like you are trying to access MongoDB over HTTP on the native driver port.
[root@master-1 mongodb]# curl http://172.16.201.136:27017
It looks like you are trying to access MongoDB over HTTP on the native driver port.
#####集群访问正常；


######节点启动进程正常：
[root@node-1 ~]# ps -ef|grep mongo
root      19485  19425  0 09:42 pts/0    00:00:00 grep --color=auto mongo
root      86814  86796  1 06:52 ?        00:01:52 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root      86958  86870  0 06:52 ?        00:00:01 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root      87008  86958  0 06:52 ?        00:00:17 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js
root      97913  97891  0 07:21 ?        00:00:59 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root      98067  97954  0 07:21 ?        00:00:02 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root      98122  98067  0 07:21 ?        00:00:15 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js
root      99783  99765  0 07:25 ?        00:00:56 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root     100016  99952  0 07:26 ?        00:00:01 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root     100033 100016  0 07:26 ?        00:00:13 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js
[root@node-1 ~]# 


[root@node-2 ~]#  ps -ef|grep mongo
root       3981   3955  0 07:25 ?        00:00:56 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root       4223   4146  0 07:25 ?        00:00:01 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root       4244   4223  0 07:25 ?        00:00:14 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js
root      52636  52600  0 09:42 pts/0    00:00:00 grep --color=auto mongo
root     121820 121803  1 06:51 ?        00:01:52 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root     122014 121970  0 06:52 ?        00:00:01 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root     122070 122014  0 06:52 ?        00:00:17 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js
root     122280 122263  1 06:52 ?        00:01:52 mongod --replSet rs0 --bind_ip 0.0.0.0 --smallfiles --noprealloc
root     122470 122438  0 06:52 ?        00:00:01 node /opt/cvallance/mongo-k8s-sidecar/node_modules/.bin/forever src/index.js
root     122524 122470  0 06:52 ?        00:00:17 /usr/local/bin/node /opt/cvallance/mongo-k8s-sidecar/src/index.js





######
[root@master-1 mongodb]# kubectl exec -it mongo-0 -- mongo
Defaulting container name to mongod-container.
Use 'kubectl describe pod/mongo-0 -n default' to see all of the containers in this pod.
MongoDB shell version v3.4.24
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 3.4.24
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
        http://docs.mongodb.org/
Questions? Try the support group
        http://groups.google.com/group/mongodb-user
Server has startup warnings: 
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] 
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] 
2021-10-14T09:56:05.089+0000 I CONTROL  [initandlisten] 
2021-10-14T09:56:05.090+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'.
2021-10-14T09:56:05.090+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2021-10-14T09:56:05.090+0000 I CONTROL  [initandlisten] 
> rs.status()
{
        "info" : "run rs.initiate(...) if not yet done for the set",
        "ok" : 0,
        "errmsg" : "no replset config has been received",
        "code" : 94,
        "codeName" : "NotYetInitialized"
}
> 

######未设置集群


####3、副本集群初始化

[root@master-1 mongodb]# kubectl get pod --all-namespaces
NAMESPACE        NAME                               READY   STATUS    RESTARTS   AGE
default          busybox                            1/1     Running   0          70s
default          mongo-0                            2/2     Running   0          12m
default          mongo-1                            2/2     Running   0          12m
default          mongo-2                            2/2     Running   0          12m
default          redis-cluster-0                    1/1     Running   0          14d
default          redis-cluster-1                    1/1     Running   0          6d6h
default          redis-cluster-2                    1/1     Running   0          14d
default          redis-cluster-3                    1/1     Running   0          14d
default          redis-cluster-4                    1/1     Running   0          14d
default          redis-cluster-5                    1/1     Running   0          14d
ingress-nginx    ingress-nginx-controller-s7x2z     1/1     Running   1          20d
ingress-nginx    ingress-nginx-controller-xf68j     1/1     Running   2          20d
kube-system      coredns-6d56c8448f-9dr27           1/1     Running   2          15d
kube-system      coredns-6d56c8448f-mfn9z           1/1     Running   2          15d
kube-system      etcd-master-1                      1/1     Running   4          22d
kube-system      kube-apiserver-master-1            1/1     Running   0          68m
kube-system      kube-controller-manager-master-1   1/1     Running   11         22d
kube-system      kube-flannel-ds-mmhsm              1/1     Running   1          22d
kube-system      kube-flannel-ds-rz2xj              1/1     Running   4          22d
kube-system      kube-flannel-ds-ts9fm              1/1     Running   1          22d
kube-system      kube-proxy-bkck2                   1/1     Running   1          22d
kube-system      kube-proxy-c6fdx                   1/1     Running   4          22d
kube-system      kube-proxy-phjdh                   1/1     Running   1          22d
kube-system      kube-scheduler-master-1            1/1     Running   12         22d
kube-system      mongodb-nfs-5f6fd65ff9-dm7wv       1/1     Running   0          82m
public-service   redis-sentinel-master-ss-0         1/1     Running   0          14d
public-service   redis-sentinel-slave-ss-0          1/1     Running   0          14d
public-service   redis-sentinel-slave-ss-1          1/1     Running   0          14d
test-ns          httpd01-699c8fcff4-wmwkv           1/1     Running   0          15d
test-ns          tomcat01-95fc6cd5d-jg2fm           1/1     Running   0          15d
[root@master-1 mongodb]# kubectl delete pod busybox
pod "busybox" deleted
[root@master-1 mongodb]# 


[root@master-1 mongodb]# kubectl get pod,svc -l app=mongo -o wide
NAME          READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
pod/mongo-0   2/2     Running   0          9m43s   10.244.2.68   node-2   <none>           <none>
pod/mongo-1   2/2     Running   0          9m24s   10.244.1.62   node-1   <none>           <none>
pod/mongo-2   2/2     Running   0          9m17s   10.244.2.69   node-2   <none>           <none>

NAME                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)           AGE    SELECTOR
service/mongo           ClusterIP   None          <none>        27017/TCP         128m   role=mongo
service/mongo-service   NodePort    10.1.98.245   <none>        27017:27017/TCP   128m   role=mongo

初始化
[root@master-1 mongodb]# kubectl exec -it mongo-0 -- /bin/bash
Defaulting container name to mongod-container.
Use 'kubectl describe pod/mongo-0 -n default' to see all of the containers in this pod.
root@mongo-0:/#     
root@mongo-0:/# 
root@mongo-0:/# mongo
> config = {_id:"rs0", members:[
... ... {_id:0,host:"10.244.2.68:27017", priority:1}
... ... ]
... ... };
{
        "_id" : "rs0",
        "members" : [
                {
                        "_id" : 0,
                        "host" : "10.244.2.68:27017",
                        "priority" : 1
                }
        ]
}
> rs.initiate(config);
{ "ok" : 1 }
rs0:OTHER> 
rs0:PRIMARY> 
rs0:PRIMARY> 


rs.remove("mongo-0.kube-dns.kube-system.svc.cluster.local:27017")
rs.remove("mongo-1.kube-dns.kube-system.svc.cluster.local:27017")
rs.remove("mongo-2.kube-dns.kube-system.svc.cluster.local:27017")

rs0:PRIMARY> rs.status() 
{
        "set" : "rs0",
        "date" : ISODate("2021-10-14T11:11:07.098Z"),
        "myState" : 1,
        "term" : NumberLong(1),
        "syncingTo" : "",
        "syncSourceHost" : "",
        "syncSourceId" : -1,
        "heartbeatIntervalMillis" : NumberLong(2000),
        "optimes" : {
                "lastCommittedOpTime" : {
                        "ts" : Timestamp(1634209862, 1),
                        "t" : NumberLong(1)
                },
                "appliedOpTime" : {
                        "ts" : Timestamp(1634209862, 1),
                        "t" : NumberLong(1)
                },
                "durableOpTime" : {
                        "ts" : Timestamp(1634209862, 1),
                        "t" : NumberLong(1)
                }
        },
        "members" : [
                {
                        "_id" : 0,
                        "name" : "10.244.2.68:27017",
                        "health" : 1,
                        "state" : 1,
                        "stateStr" : "PRIMARY",
                        "uptime" : 737,
                        "optime" : {
                                "ts" : Timestamp(1634209862, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDate" : ISODate("2021-10-14T11:11:02Z"),
                        "syncingTo" : "",
                        "syncSourceHost" : "",
                        "syncSourceId" : -1,
                        "infoMessage" : "could not find member to sync from",
                        "electionTime" : Timestamp(1634209780, 2),
                        "electionDate" : ISODate("2021-10-14T11:09:40Z"),
                        "configVersion" : 1,
                        "self" : true,
                        "lastHeartbeatMessage" : ""
                }
        ],
        "ok" : 1
}
rs0:PRIMARY> 


####4、添加节点：
rs0:PRIMARY> rs.add("10.244.1.62:27017")
{ "ok" : 1 }
rs0:PRIMARY> rs.add("10.244.2.69:27017")
{ "ok" : 1 }

在主节点查看各个从节点状态：
rs0:PRIMARY> db.printSlaveReplicationInfo()
source: 10.244.1.62:27017
        syncedTo: Thu Oct 14 2021 11:14:05 GMT+0000 (UTC)
        3 secs (0 hrs) behind the primary 
source: 10.244.2.69:27017
        syncedTo: Thu Jan 01 1970 00:00:00 GMT+0000 (UTC)
        1634210048 secs (453947.24 hrs) behind the primary 
rs0:PRIMARY> 

rs0:PRIMARY> rs.status() 
{
        "set" : "rs0",
        "date" : ISODate("2021-10-14T11:14:27.632Z"),
        "myState" : 1,
        "term" : NumberLong(1),
        "syncingTo" : "",
        "syncSourceHost" : "",
        "syncSourceId" : -1,
        "heartbeatIntervalMillis" : NumberLong(2000),
        "optimes" : {
                "lastCommittedOpTime" : {
                        "ts" : Timestamp(1634210062, 1),
                        "t" : NumberLong(1)
                },
                "appliedOpTime" : {
                        "ts" : Timestamp(1634210062, 1),
                        "t" : NumberLong(1)
                },
                "durableOpTime" : {
                        "ts" : Timestamp(1634210062, 1),
                        "t" : NumberLong(1)
                }
        },
        "members" : [
                {
                        "_id" : 0,
                        "name" : "10.244.2.68:27017",
                        "health" : 1,
                        "state" : 1,
                        "stateStr" : "PRIMARY",
                        "uptime" : 937,
                        "optime" : {
                                "ts" : Timestamp(1634210062, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDate" : ISODate("2021-10-14T11:14:22Z"),
                        "syncingTo" : "",
                        "syncSourceHost" : "",
                        "syncSourceId" : -1,
                        "infoMessage" : "",
                        "electionTime" : Timestamp(1634209780, 2),
                        "electionDate" : ISODate("2021-10-14T11:09:40Z"),
                        "configVersion" : 3,
                        "self" : true,
                        "lastHeartbeatMessage" : ""
                },
                {
                        "_id" : 1,
                        "name" : "10.244.1.62:27017",
                        "health" : 1,
                        "state" : 2,
                        "stateStr" : "SECONDARY",
                        "uptime" : 21,
                        "optime" : {
                                "ts" : Timestamp(1634210062, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDurable" : {
                                "ts" : Timestamp(1634210062, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDate" : ISODate("2021-10-14T11:14:22Z"),
                        "optimeDurableDate" : ISODate("2021-10-14T11:14:22Z"),
                        "lastHeartbeat" : ISODate("2021-10-14T11:14:26.664Z"),
                        "lastHeartbeatRecv" : ISODate("2021-10-14T11:14:27.629Z"),
                        "pingMs" : NumberLong(13),
                        "lastHeartbeatMessage" : "",
                        "syncingTo" : "10.244.2.68:27017",
                        "syncSourceHost" : "10.244.2.68:27017",
                        "syncSourceId" : 0,
                        "infoMessage" : "",
                        "configVersion" : 3
                },
                {
                        "_id" : 2,
                        "name" : "10.244.2.69:27017",
                        "health" : 1,
                        "state" : 2,
                        "stateStr" : "SECONDARY",
                        "uptime" : 19,
                        "optime" : {
                                "ts" : Timestamp(1634210062, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDurable" : {
                                "ts" : Timestamp(1634210062, 1),
                                "t" : NumberLong(1)
                        },
                        "optimeDate" : ISODate("2021-10-14T11:14:22Z"),
                        "optimeDurableDate" : ISODate("2021-10-14T11:14:22Z"),
                        "lastHeartbeat" : ISODate("2021-10-14T11:14:26.579Z"),
                        "lastHeartbeatRecv" : ISODate("2021-10-14T11:14:24.204Z"),
                        "pingMs" : NumberLong(0),
                        "lastHeartbeatMessage" : "",
                        "syncingTo" : "10.244.2.68:27017",
                        "syncSourceHost" : "10.244.2.68:27017",
                        "syncSourceId" : 0,
                        "infoMessage" : "",
                        "configVersion" : 3
                }
        ],
        "ok" : 1
}
rs0:PRIMARY> 



可以查看集群详细运行状态，在主节点执行：
rs0:PRIMARY> rs.conf()
{
        "_id" : "rs0",
        "version" : 3,
        "protocolVersion" : NumberLong(1),
        "members" : [
                {
                        "_id" : 0,
                        "host" : "10.244.2.68:27017",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                },
                {
                        "_id" : 1,
                        "host" : "10.244.1.62:27017",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                },
                {
                        "_id" : 2,
                        "host" : "10.244.2.69:27017",
                        "arbiterOnly" : false,
                        "buildIndexes" : true,
                        "hidden" : false,
                        "priority" : 1,
                        "tags" : {

                        },
                        "slaveDelay" : NumberLong(0),
                        "votes" : 1
                }
        ],
        "settings" : {
                "chainingAllowed" : true,
                "heartbeatIntervalMillis" : 2000,
                "heartbeatTimeoutSecs" : 10,
                "electionTimeoutMillis" : 10000,
                "catchUpTimeoutMillis" : 60000,
                "getLastErrorModes" : {

                },
                "getLastErrorDefaults" : {
                        "w" : 1,
                        "wtimeout" : 0
                },
                "replicaSetId" : ObjectId("61680ff4cfb3d35f05c12953")
        }
}
rs0:PRIMARY> 

删除节点：
rs0:PRIMARY> rs.remove("10.244.2.69:27017")
{ "ok" : 1 }
rs0:PRIMARY> db.printSlaveReplicationInfo()
source: 10.244.1.62:27017
        syncedTo: Thu Oct 14 2021 11:16:45 GMT+0000 (UTC)
        0 secs (0 hrs) behind the primary 
rs0:PRIMARY> 

再次加回来：
rs0:PRIMARY> rs.add("10.244.2.69:27017")
{ "ok" : 1 }
查看节点状态：
rs0:PRIMARY> db.printSlaveReplicationInfo()
source: 10.244.1.62:27017
        syncedTo: Thu Oct 14 2021 11:17:11 GMT+0000 (UTC)
        0 secs (0 hrs) behind the primary 
source: 10.244.2.69:27017
        syncedTo: Thu Jan 01 1970 00:00:00 GMT+0000 (UTC)
        1634210231 secs (453947.29 hrs) behind the primary 
rs0:PRIMARY> 


####5、添加数据测试
主节点操作：
rs0:PRIMARY> use testdb
switched to db testdb
rs0:PRIMARY> db.test1231.insert({"name":"test repl"});
WriteResult({ "nInserted" : 1 })
rs0:PRIMARY> rs.slaveOk();
rs0:PRIMARY> 


node1操作：
[root@master-1 mongodb]# kubectl exec -it mongo-1 -- /bin/bash
Defaulting container name to mongod-container.
Use 'kubectl describe pod/mongo-1 -n default' to see all of the containers in this pod.
root@mongo-1:/# mongo
MongoDB shell version v3.4.24
connecting to: mongodb://127.0.0.1:27017
MongoDB server version: 3.4.24
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
        http://docs.mongodb.org/
Questions? Try the support group
        http://groups.google.com/group/mongodb-user
Server has startup warnings: 
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] 
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] ** WARNING: You are running this process as the root user, which is not recommended.
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] 
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] 
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] 
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'.
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2021-10-14T10:59:19.770+0000 I CONTROL  [initandlisten] 
rs0:SECONDARY> 
rs0:SECONDARY> 
rs0:SECONDARY> rs.slaveOk();
rs0:SECONDARY> use testdb
switched to db testdb
rs0:SECONDARY> show collections;
test1231
rs0:SECONDARY> db.test1231.find();
{ "_id" : ObjectId("6168122f3438c0f4171d6185"), "name" : "test repl" }
rs0:SECONDARY> 

主节点插入数据都在。



####6、replication set的一些设置
设置优先级，是否隐藏及延时
cfg = rs.conf()
cfg.members[0].priority = 0
cfg.members[0].hidden = true
cfg.members[0].slaveDelay = 3600
rs.reconfig(cfg)
设置选举权：
cfg = rs.conf()
cfg.members[3].votes = 0
cfg.members[4].votes = 0
cfg.members[5].votes = 0
rs.reconfig(cfg)


####7、Mongo集群进行扩容
[root@master-1 mongodb]#  kubectl scale statefulset mongo --replicas=4
statefulset.apps/mongo scaled

[root@master-1 mongodb]#  kubectl get pod -l app=mongo -o wide 
NAME      READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
mongo-0   2/2     Running   0          29m   10.244.2.68   node-2   <none>           <none>
mongo-1   2/2     Running   0          29m   10.244.1.62   node-1   <none>           <none>
mongo-2   2/2     Running   0          29m   10.244.2.69   node-2   <none>           <none>
mongo-3   2/2     Running   0          22s   10.244.1.63   node-1   <none>           <none>

mongo内部添加节点略


[root@master-1 mongodb]# kubectl scale statefulset mongo --replicas=6
statefulset.apps/mongo scaled
[root@master-1 mongodb]# kubectl get pod  -o wide 
NAME              READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
mongo-0           2/2     Running   0          34m     10.244.2.68   node-2   <none>           <none>
mongo-1           2/2     Running   0          34m     10.244.1.62   node-1   <none>           <none>
mongo-2           2/2     Running   0          34m     10.244.2.69   node-2   <none>           <none>
mongo-3           2/2     Running   0          4m50s   10.244.1.63   node-1   <none>           <none>
mongo-4           2/2     Running   0          55s     10.244.2.70   node-2   <none>           <none>
mongo-5           2/2     Running   0          33s     10.244.1.64   node-1   <none>           <none>
redis-cluster-0   1/1     Running   0          14d     10.244.2.52   node-2   <none>           <none>
redis-cluster-1   1/1     Running   0          6d8h    10.244.1.50   node-1   <none>           <none>
redis-cluster-2   1/1     Running   0          14d     10.244.2.53   node-2   <none>           <none>
redis-cluster-3   1/1     Running   0          14d     10.244.1.48   node-1   <none>           <none>
redis-cluster-4   1/1     Running   0          14d     10.244.2.54   node-2   <none>           <none>
redis-cluster-5   1/1     Running   0          14d     10.244.1.49   node-1   <none>           <none>


[root@master-1 ~]#  kubectl get all
NAME                  READY   STATUS    RESTARTS   AGE
pod/mongo-0           2/2     Running   0          15h
pod/mongo-1           2/2     Running   0          15h
pod/mongo-2           2/2     Running   0          15h
pod/mongo-3           2/2     Running   0          15h
pod/mongo-4           2/2     Running   0          15h
pod/mongo-5           2/2     Running   0          15h
pod/redis-cluster-0   1/1     Running   0          14d
pod/redis-cluster-1   1/1     Running   0          6d23h
pod/redis-cluster-2   1/1     Running   0          14d
pod/redis-cluster-3   1/1     Running   0          14d
pod/redis-cluster-4   1/1     Running   0          14d
pod/redis-cluster-5   1/1     Running   0          14d

NAME                    TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                          AGE
service/kubernetes      ClusterIP   10.1.0.1      <none>        443/TCP                          22d
service/mongo           ClusterIP   None          <none>        27017/TCP                        17h
service/mongo-service   NodePort    10.1.98.245   <none>        27017:27017/TCP                  17h
service/redis-cluster   NodePort    10.1.231.82   <none>        6379:31000/TCP,16379:30701/TCP   7d

NAME                             READY   AGE
statefulset.apps/mongo           6/6     15h
statefulset.apps/redis-cluster   6/6     14d


####8、清理：
kubectl delete statefulset mongo
kubectl delete svc mongo
kubectl delete pvc -l role=mongo


##(九)、k8s部署 Kafka集群
###一、概念、功能，架构介绍
####（1）、介绍，
ApacheKafka®是一个分布式流媒体平台，它主要有3种功能：
1、发布和订阅消息流，这个功能类似于消息队列，这也是kafka归类为消息队列框架的原因
2、以容错的方式记录消息流，kafka以文件的方式来存储消息流
3、可以再消息发布的时候进行处理

####（2）、使用场景：
1、在系统或应用程序之间构建可靠的用于传输实时数据的管道，消息队列功能
2、构建实时的流数据处理程序来变换或处理数据流，数据处理功能

####（3）详细介绍：
Kafka目前主要作为一个分布式的发布订阅式的消息系统使用，下面简单介绍一下kafka的基本机制

Producer即生产者，向Kafka集群发送消息，在发送消息之前，会对消息进行分类，即Topic，上图展示了两个producer发送了分类为topic1的消息，另外一个发送了topic2的消息。
Topic即主题，通过对消息指定主题可以将消息分类，消费者可以只关注自己需要的Topic中的消息
Consumer即消费者，消费者通过与kafka集群建立长连接的方式，不断地从集群中拉取消息，然后可以对这些消息进行处理。

同一个Topic下的消费者和生产者的数量并不是对应的

谈到kafka的存储，就不得不提到分区，即partitions，创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。

在每个分区中，消息以顺序存储，最晚接收的的消息会最后被消费。

生产者在向kafka集群发送消息的时候，可以通过指定分区来发送到指定的分区中
也可以通过指定均衡策略来将消息发送到不同的分区中
如果不指定，就会采用默认的随机均衡策略，将消息随机的存储到不同的分区中

在消费者消费消息时，kafka使用offset来记录当前消费的位置
在kafka的设计中，可以有多个不同的group来同时消费同一个topic下的消息，如图，我们有两个不同的group同时消费，他们的的消费的记录位置offset各不项目，不互相干扰。
对于一个group而言，消费者的数量不应该多余分区的数量，因为在一个group中，每个分区至多只能绑定到一个消费者上，即一个消费者可以消费多个分区，一个分区只能给一个消费者消费
因此，若一个group中的消费者数量大于分区数量的话，多余的消费者将不会收到任何消息。




###二、kafka部署(未成功，景象问题，稍后再做景象)：

####（一）、准备nfs
[root@master-1 kafka]# mkdir -p /root/kafkadata/{kafka01,kafka02,kafka03}
[root@master-1 kafka]# echo '/root/kafkadata/kafka01 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 kafka]# echo '/root/kafkadata/kafka02 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 kafka]# echo '/root/kafkadata/kafka03 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 kafka]# exportfs -r
[root@master-1 kafka]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/root/kafkadata/kafka03     *
/root/kafkadata/kafka02     *
/root/kafkadata/kafka01     *
/net/mysql-3                *
/net/mysql-2                *
/net/mysql-1                *
/net/mysql-0                *
/root/mysql-replicationdata *
/root/mongodb-share         *
/root/redis-cluster/pv6     *
/root/redis-cluster/pv5     *
/root/redis-cluster/pv4     *
/root/redis-cluster/pv3     *
/root/redis-cluster/pv2     *
/root/redis-cluster/pv1     *
/root/redis-sentinel/2      *
/root/redis-sentinel/1      *
/root/redis-sentinel/0      *
/root/web1                  *
/root/nfs_data              *
[root@master-1 kafka]# 

[root@node-1 ~]# mkdir -p /root/kafkadata/{kafka01,kafka02,kafka03}
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka01 /root/kafkadata/kafka01
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka02 /root/kafkadata/kafka02
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka03 /root/kafkadata/kafka03
[root@node-1 net]# df -h|grep 134|grep kafka         
172.16.201.134:/root/kafkadata/kafka01                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka01
172.16.201.134:/root/kafkadata/kafka02                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka02
172.16.201.134:/root/kafkadata/kafka03                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka03
[root@node-1 net]# 

[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka01 /root/kafkadata/kafka01
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka02 /root/kafkadata/kafka02
[root@node-2 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/kafka03 /root/kafkadata/kafka03
[root@node-2 ~]# df -h|grep 134|grep kafka       
172.16.201.134:/root/kafkadata/kafka01                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka01
172.16.201.134:/root/kafkadata/kafka02                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka02
172.16.201.134:/root/kafkadata/kafka03                                                                                         50G  7.4G   43G  15% /root/kafkadata/kafka03
[root@node-2 ~]# 



[root@master-1 kafkadata]# mkdir -p /root/kafkadata/{zkdata01,zkdata02,zkdata03}
[root@master-1 kafka]# echo '/root/kafkadata/zkdata01 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 kafka]# echo '/root/kafkadata/zkdata02 *(rw,no_root_squash)' >> /etc/exports
[root@master-1 kafka]# echo '/root/kafkadata/zkdata03 *(rw,no_root_squash)' >> /etc/exports 
[root@master-1 zkdata01]# showmount -e 172.16.201.134
[root@master-1 kafka]# exportfs -r
Export list for 172.16.201.134:
/root/kafkadata/zkdata03    *
/root/kafkadata/zkdata02    *
/root/kafkadata/zkdata01    *
/root/kafkadata/kafka03     *
/root/kafkadata/kafka02     *
/root/kafkadata/kafka01     *

[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/zkdata01 /root/kafkadata/zkdata01
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/zkdata02 /root/kafkadata/zkdata02
[root@node-1 ~]# mount -t nfs 172.16.201.134:/root/kafkadata/zkdata03 /root/kafkadata/zkdata03

[root@node-2 zkdata01]# df -h|grep 134|grep kafka     
172.16.201.134:/root/kafkadata/kafka01                                                                                         50G  7.5G   43G  15% /root/kafkadata/kafka01
172.16.201.134:/root/kafkadata/kafka02                                                                                         50G  7.5G   43G  15% /root/kafkadata/kafka02
172.16.201.134:/root/kafkadata/kafka03                                                                                         50G  7.5G   43G  15% /root/kafkadata/kafka03
172.16.201.134:/root/kafkadata/zkdata03                                                                                        50G  7.5G   43G  15% /root/kafkadata/zkdata03
172.16.201.134:/root/kafkadata/zkdata02                                                                                        50G  7.5G   43G  15% /root/kafkadata/zkdata02
172.16.201.134:/root/kafkadata/zkdata01                                                                                        50G  7.5G   43G  15% /root/kafkadata/zkdata01



####（二）、创建

[root@master-1 kafka]# vim zkpv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk01
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata01
      server: 172.16.201.134

---

apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk02
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata02
      server: 172.16.201.134

---
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk03
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata03
      server: 172.16.201.134


[root@master-1 kafka]# vim kafkapv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-01
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka01
      server: 172.16.201.134

---

apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-02
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka02
      server: 172.16.201.134

---
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-03
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka03
      server: 172.16.201.134




[root@master-1 kafka]# cat zookeeper.yaml 
---
apiVersion: v1
kind: Service
metadata:
  name: zk-svc
  labels:
    app: zk-svc
spec:
  ports:
  - port: 2888
    name: server
  - port: 3888
    name: leader-election
  clusterIP: None
  selector:
    app: zk
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-cm
data:
  jvm.heap: "1G"
  tick: "2000"
  init: "10"
  sync: "5"
  client.cnxns: "60"
  snap.retain: "3"
  purge.interval: "0"
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  selector:
    matchLabels:
      app: zk
  minAvailable: 2
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: wm-zk
spec:
  serviceName: zk-svc
  selector:
    matchLabels:
      app: zk
  replicas: 3
  template:
    metadata:
      labels:
        app: zk
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values: 
                    - zk
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: k8szk
        imagePullPolicy: IfNotPresent
        image: gcr.io/google_samples/k8szk:v3
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env:
        - name : ZK_REPLICAS
          value: "3"
        - name : ZK_HEAP_SIZE
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: jvm.heap
        - name : ZK_TICK_TIME
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: tick
        - name : ZK_INIT_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: init
        - name : ZK_SYNC_LIMIT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: tick
        - name : ZK_MAX_CLIENT_CNXNS
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: client.cnxns
        - name: ZK_SNAP_RETAIN_COUNT
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: snap.retain
        - name: ZK_PURGE_INTERVAL
          valueFrom:
            configMapKeyRef:
                name: zk-cm
                key: purge.interval
        - name: ZK_CLIENT_PORT
          value: "2181"
        - name: ZK_SERVER_PORT
          value: "2888"
        - name: ZK_ELECTION_PORT
          value: "3888"
        command:
        - sh
        - -c
        - zkGenConfig.sh && zkServer.sh start-foreground
        readinessProbe:
          exec:
            command:
            - "zkOk.sh"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - "zkOk.sh"
          initialDelaySeconds: 10
          timeoutSeconds: 5
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
     name: datadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 5Gi
      storageClassName: nas-zk
[root@master-1 kafka]# 

######附：storage: 5Gi 一定要跟pv.yaml里面的容量一致，否则无法创建成功，一直pending。get event找不到pvc


[root@master-1 kafka]# kubectl apply -f zkpv.yaml 
persistentvolume/kafka-zk01 created
persistentvolume/kafka-zk02 created
persistentvolume/kafka-zk03 created

[root@master-1 kafka]# kubectl apply -f kafkapv.yaml 
persistentvolume/kafka-01 created
persistentvolume/kafka-02 created
persistentvolume/kafka-03 created

[root@master-1 kafka]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                                                                     STORAGECLASS                   REASON   AGE
kafka-01                                   5Gi        RWO            Recycle          Available                                                                             nas-zk                                  16s
kafka-02                                   5Gi        RWO            Recycle          Available                                                                             nas-zk                                  16s
kafka-03                                   5Gi        RWO            Recycle          Available                                                                             nas-zk                                  16s
kafka-zk01                                 5Gi        RWO            Recycle          Available                                                                             nas-zk                                  44s
kafka-zk02                                 5Gi        RWO            Recycle          Available                                                                             nas-zk                                  43s
kafka-zk03                                 5Gi        RWO            Recycle          Available                                                                             nas-zk                                  43s

[root@master-1 kafka]# kubectl apply -f zookeeper.yaml   
service/zk-svc created
configmap/zk-cm created
poddisruptionbudget.policy/zk-pdb created
statefulset.apps/zk created


[root@master-1 kafka]# kubectl get pod,pv,pvc,sc

[root@master-1 kafka]# kubectl delete -f 
[root@master-1 kafka]# kubectl get pod --watch  
[root@master-1 kafka]# kubectl describe pvc datadir-zk-0
[root@master-1 kafka]# kubectl delete pvc datadir-zk-0




[root@master-1 kafka]# kubectl apply -f zookeeper.yaml   
service/zk-svc created
configmap/zk-cm created
poddisruptionbudget.policy/zk-pdb created
statefulset.apps/zk created


[root@master-1 kafka]# kubectl get pod,pv,pvc,sc
[root@master-1 kafka]# kubectl delete -f 
[root@master-1 kafka]# kubectl get pod --watch  
[root@master-1 kafka]# kubectl describe pvc datadir-zk-0
[root@master-1 kafka]# kubectl delete pvc datadir-wm-zk-0
[root@master-1 kafka]# kubectl get event
 kubectl patch  persistentvolume kafka-zk01 -p '{"metadata":{"finalizers":null}}'
 kubectl patch pvc datadir-zk-0 -p '{"metadata":{"deletionTimestamp":""}}'

kubectl patch pvc datadir-zk-0 --type=json -p='[{"op": "remove", "path": "/metadata/deletionTimestamp"}]'
persistentvolumeclaim/www-web-0 patched (no change)

kubectl delete pvc datadir-wm-zk-0  --force
kubectl describe pod 

[root@master-1 mongodb]# kubectl logs -f wm-zk-0
server.1=wm-zk-0.zk-svc.default.svc.cluster.local:2888:3888
server.2=wm-zk-1.zk-svc.default.svc.cluster.local:2888:3888
server.3=wm-zk-2.zk-svc.default.svc.cluster.local:2888:3888
Environment validation successful
Creating ZooKeeper configuration
Wrote ZooKeeper configuration file to /opt/zookeeper/conf/zoo.cfg
Creating ZooKeeper log4j configuration
Wrote log4j configuration to /opt/zookeeper/conf/log4j.properties
Creating ZooKeeper data directories and setting permissions
mkdir: cannot create directory '/var/lib/zookeeper/data': Permission denied
chown: cannot access '/var/lib/zookeeper/data': No such file or directory
mkdir: cannot create directory '/var/lib/zookeeper/log': Permission denied
chown: cannot access '/var/lib/zookeeper/log': No such file or directory
/usr/bin/zkGenConfig.sh: line 130: /var/lib/zookeeper/data/myid: No such file or directory
Created ZooKeeper data directories and set permissions in /var/lib/zookeeper/data
Creating JVM configuration file
Wrote JVM configuration to /opt/zookeeper/conf/java.env
ZooKeeper JMX enabled by default
Using config: /usr/bin/../etc/zookeeper/zoo.cfg
mkdir: cannot create directory '/var/lib/zookeeper/data': Permission denied
Invalid maximum heap size: -Xmx0.5G
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
[root@master-1 mongodb]# kubectl logs -f wm-zk-0

kubernetes1.6以后默认开启rbac, 做完如上操作然后再次创建即可.
景象有问题，不重做了


#@@@@@@@@@@@@@@@@@@@@@@@@@@@

#Helm安装Kafka
#####0、Helm安装
[root@master-1 kafka]#curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
[root@master-1 kafka]#chmod 700 get_helm.sh
[root@master-1 kafka]#./get_helm.sh

[root@master-1 kafka]# helm version
version.BuildInfo{Version:"v3.7.1", GitCommit:"1d11fcb5d3f3bf00dbe6fe31b8412839a96b3dc4", GitTreeState:"clean", GoVersion:"go1.16.9"}

介绍
在 helm 中有三个关键概念：Chart，Repo 及 Release
Chart: 一系列 k8s 资源集合的命名，它包含一系列 k8s 资源配置文件的模板与参数，可供灵活配置
Repo: 即 chart 的仓库，其中有很多个 chart 可供选择，如官方 helm/charts[5]
Release: 当一个 Chart 部署后生成一个 releas

查找相关 Chart:helm search hub
添加相关 Repo：helm repo add stable https://apphub.aliyuncs.com/stable
如何部署完成，可以查看安装某个 Release 时的 values：helm get values kafka
如果需要升级，使用 helm upgrade :helm upgrade kafka bitnami/kafka --values values-production.yaml
校验部署状态:helm status kafka

Helm CLI 个别更名:
helm delete更名为 helm uninstall
helm inspect更名为 helm show
helm fetch更名为 helm pull

删除：
[root@master-1 kafka]# helm ls
NAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
kafka   default         1               2021-10-26 18:42:04.279592114 +0800 CST deployed        kafka-0.21.5    5.0.1      
[root@master-1 kafka]# helm uninstall kafka
release "kafka" uninstalled
[root@master-1 kafka]# helm ls
NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION

[root@master-1 kafka]# kubectl get pod
No resources found in default namespace.




#####提前准备nfs、pv，pvc，storageClass：
[root@master-1 kafka]# cat zkpv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk01
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata01
      server: 172.16.201.134

---

apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk02
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata02
      server: 172.16.201.134

---
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-zk03
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Delete
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/zkdata03
      server: 172.16.201.134
[root@master-1 kafka]# 

[root@master-1 kafka]# cat kafkapv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-01
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka01
      server: 172.16.201.134

---

apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-02
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka02
      server: 172.16.201.134

---
apiVersion: v1
kind: PersistentVolume
metadata:
    name: kafka-03
spec:
    capacity:
      storage: 5Gi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Recycle
    storageClassName: nas-zk
    nfs:
      path: /root/kafkadata/kafka03
      server: 172.16.201.134
[root@master-1 kafka]# 


[root@master-1 kafka]# cat storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nas-zk
provisioner: kubernetes.io/nfs
reclaimPolicy: Delete
parameters:
  archiveOnDelete: "false"
[root@master-1 kafka]# 

[root@master-1 v1]# cat clusterrolebinding.yaml 
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner2
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner2
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner2
  apiGroup: rbac.authorization.k8s.io
[root@master-1 v1]# 

[root@master-1 v1]# cat clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-runner2
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services", "endpoints"]
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources: ["podsecuritypolicies"]
    resourceNames: ["nfs-provisioner2"]
    verbs: ["use"]
[root@master-1 v1]# 

[root@master-1 v1]# cat serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner2
[root@master-1 v1]# 

[root@master-1 v1]# kubectl get ClusterRole,ClusterRoleBinding,ServiceAccount|grep provisioner
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner                                                 2021-10-26T07:50:01Z
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner2                                                2021-10-26T07:50:48Z
clusterrole.rbac.authorization.k8s.io/system:persistent-volume-provisioner                                   2021-09-22T04:04:21Z
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner                                    ClusterRole/nfs-provisioner-runner                                                 23h
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner2                                   ClusterRole/nfs-provisioner-runner2                                                23h
serviceaccount/nfs-provisioner    1         24h
serviceaccount/nfs-provisioner2   1         23h


创建/删除：
[root@master-1 kafka]# cat c
kubectl apply -f storageclass.yaml
kubectl apply -f zkpv.yaml
kubectl apply -f kafkapv.yaml
kubectl get pod,pv,pvc,sc

[root@master-1 kafka]# cat d
kubectl delete -f zkpv.yaml
kubectl delete -f kafkapv.yaml
kubectl delete -f storageclass.yaml
kubectl get pod,pv,pvc,sc
[root@master-1 kafka]# 


#####1、获取kafka的chart包测试：
[root@master-1 kafka]# helm repo add incubator https://charts.helm.sh/incubator
"incubator" has been added to your repositories

[root@master-1 kafka]# helm repo list
NAME            URL                             
aliyuncs        https://apphub.aliyuncs.com     
incubator       https://charts.helm.sh/incubator

[root@master-1 kafka]# helm pull incubator/kafka --untar
[root@master-1 kafka]# ll
drwxr-xr-x 4 root root  160 Oct 25 18:28 kafka

#####2、修改部分配置
#######主配置文件values.yaml：
去掉注释，增加  storageClass: "nas-zk"，已经建立好。
[root@master-1 kafka]# cat values.yaml |grep storageClass
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  storageClass: "nas-zk"
	size: "1Gi"

添加type: NodePort
[root@master-1 kafka]# cat values.yaml |grep -A2 -B2 external
external:
  enabled: true
  type: NodePort

#######zookeeper配置文件:
[root@master-1 kafka]# vim charts/zookeeper/values.yaml 
persistence:
  enabled: true
  storageClass: "nas-zk"
  accessMode: ReadWriteOnce
  size: 5Gi


可选：创建namespace，执行：kubectl create namespace kafka-test
在kafka目录下执行：helm install kafka -f values.yaml incubator/kafka --namespace kafka-test

#####3、安装
[root@master-1 kafka]# helm install kafka -f values.yaml incubator/kafka
WARNING: This chart is deprecated
NAME: kafka
LAST DEPLOYED: Tue Oct 26 18:42:04 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
######Connecting to Kafka from inside Kubernetes

You can connect to Kafka by running a simple pod in the K8s cluster like this with a configuration like this:

  apiVersion: v1
  kind: Pod
  metadata:
    name: testclient
    namespace: default
  spec:
    containers:
    - name: kafka
      image: confluentinc/cp-kafka:5.0.1
      command:
        - sh
        - -c
        - "exec tail -f /dev/null"

Once you have the testclient pod above running, you can list all kafka
topics with:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --list

To create a new topic:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1

To listen for messages on a topic:

  kubectl -n default exec -ti testclient -- ./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test1 --from-beginning

To stop the listener session above press: Ctrl+C

To start an interactive message producer session:
  kubectl -n default exec -ti testclient -- ./bin/kafka-console-producer.sh --broker-list kafka-headless:9092 --topic test1

To create a message in the above session, simply type the message and press "enter"
To end the producer session try: Ctrl+C

If you specify "zookeeper.connect" in configurationOverrides, please replace "kafka-zookeeper:2181" with the value of "zookeeper.connect", or you will get error.


######## Connecting to Kafka from outside Kubernetes

You have enabled the external access feature of this chart.

**WARNING:** By default this feature allows Kafka clients outside Kubernetes to
connect to Kafka via NodePort(s) in `PLAINTEXT`.

Please see this chart's README.md for more details and guidance.

If you wish to connect to Kafka from outside please configure your external Kafka
clients to point at the following brokers. Please allow a few minutes for all
associated resources to become healthy.
  
  kafka.cluster.local:31090
  kafka.cluster.local:31091
  kafka.cluster.local:31092
[root@master-1 kafka]# 
如果前面的配置没有问题，控制台提示如上所示。

如何部署完成，可以查看安装某个 Release 时的 values
[root@master-1 kafka]# helm get values kafka
USER-SUPPLIED VALUES:
additionalPorts: {}
affinity: {}
configJob:
  backoffLimit: 6
configurationOverrides:
  confluent.support.metrics.enable: false
envOverrides: {}
external:
  distinct: false
  dns:
    useExternal: true
    useInternal: false
  domain: cluster.local
  enabled: true
  firstListenerPort: 31090
  init:
    image: lwolf/kubectl_deployer
    imagePullPolicy: IfNotPresent
    imageTag: "0.4"
  loadBalancerIP: []
  loadBalancerSourceRanges: []
  servicePort: 19092
  type: NodePort
headless:
  port: 9092
image: confluentinc/cp-kafka
imagePullPolicy: IfNotPresent
imageTag: 5.0.1
jmx:
  configMap:
    enabled: true
    overrideConfig: {}
    overrideName: ""
  port: 5555
  whitelistObjectNames:
  - kafka.controller:*
  - kafka.server:*
  - java.lang:*
  - kafka.network:*
  - kafka.log:*
kafkaHeapOptions: -Xmx1G -Xms1G
logSubPath: logs
nodeSelector: {}
persistence:
  enabled: true
  mountPath: /opt/kafka/data
  size: 1Gi
  storageClass: nas-zk
podAnnotations: {}
podDisruptionBudget: {}
podLabels: {}
podManagementPolicy: OrderedReady
prometheus:
  jmx:
    enabled: false
    image: solsson/kafka-prometheus-jmx-exporter@sha256
    imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8
    interval: 10s
    port: 5556
    resources: {}
    scrapeTimeout: 10s
  kafka:
    affinity: {}
    enabled: false
    image: danielqsj/kafka-exporter
    imageTag: v1.2.0
    interval: 10s
    nodeSelector: {}
    port: 9308
    resources: {}
    scrapeTimeout: 10s
    tolerations: []
  operator:
    enabled: false
    prometheusRule:
      enabled: false
      namespace: monitoring
      releaseNamespace: false
      rules:
      - alert: KafkaNoActiveControllers
        annotations:
          message: The number of active controllers in {{ "{{" }} $labels.namespace
            {{ "}}" }} is less than 1. This usually means that some of the Kafka nodes
            aren't communicating properly. If it doesn't resolve itself you can try
            killing the pods (one by one whilst monitoring the under-replicated partitions
            graph).
        expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by
          (namespace) < 1
        for: 5m
        labels:
          severity: critical
      - alert: KafkaMultipleActiveControllers
        annotations:
          message: The number of active controllers in {{ "{{" }} $labels.namespace
            {{ "}}" }} is greater than 1. This usually means that some of the Kafka
            nodes aren't communicating properly. If it doesn't resolve itself you
            can try killing the pods (one by one whilst monitoring the under-replicated
            partitions graph).
        expr: max(kafka_controller_kafkacontroller_activecontrollercount_value) by
          (namespace) > 1
        for: 5m
        labels:
          severity: critical
      selector:
        prometheus: kube-prometheus
    serviceMonitor:
      namespace: monitoring
      releaseNamespace: false
      selector:
        prometheus: kube-prometheus
readinessProbe:
  failureThreshold: 3
  initialDelaySeconds: 30
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
replicas: 3
resources: {}
securityContext: {}
terminationGracePeriodSeconds: 60
testsEnabled: true
tolerations: []
topics: []
updateStrategy:
  type: OnDelete
zookeeper:
  affinity: {}
  enabled: true
  env:
    ZK_HEAP_SIZE: 1G
  image:
    PullPolicy: IfNotPresent
  persistence:
    enabled: false
  port: 2181
  resources: null
  url: ""
[root@master-1 kafka]# 



[root@master-1 kafka]# helm status kafka
NAME: kafka
LAST DEPLOYED: Tue Oct 26 18:42:04 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
### Connecting to Kafka from inside Kubernetes

You can connect to Kafka by running a simple pod in the K8s cluster like this with a configuration like this:

  apiVersion: v1
  kind: Pod
  metadata:
    name: testclient
    namespace: default
  spec:
    containers:
    - name: kafka
      image: confluentinc/cp-kafka:5.0.1
      command:
        - sh
        - -c
        - "exec tail -f /dev/null"

Once you have the testclient pod above running, you can list all kafka
topics with:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --list

To create a new topic:

  kubectl -n default exec testclient -- ./bin/kafka-topics.sh --zookeeper kafka-zookeeper:2181 --topic test1 --create --partitions 1 --replication-factor 1

To listen for messages on a topic:

  kubectl -n default exec -ti testclient -- ./bin/kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test1 --from-beginning

To stop the listener session above press: Ctrl+C

To start an interactive message producer session:
  kubectl -n default exec -ti testclient -- ./bin/kafka-console-producer.sh --broker-list kafka-headless:9092 --topic test1

To create a message in the above session, simply type the message and press "enter"
To end the producer session try: Ctrl+C

If you specify "zookeeper.connect" in configurationOverrides, please replace "kafka-zookeeper:2181" with the value of "zookeeper.connect", or you will get error.


######## Connecting to Kafka from outside Kubernetes

You have enabled the external access feature of this chart.

**WARNING:** By default this feature allows Kafka clients outside Kubernetes to
connect to Kafka via NodePort(s) in `PLAINTEXT`.

Please see this chart's README.md for more details and guidance.

If you wish to connect to Kafka from outside please configure your external Kafka
clients to point at the following brokers. Please allow a few minutes for all
associated resources to become healthy.
  
  kafka.cluster.local:31090
  kafka.cluster.local:31091
  kafka.cluster.local:31092
[root@master-1 kafka]# 



kafka启动依赖zookeeper，整个启动会耗时数分钟，期间可见zookeeper和kafka的pod逐渐启动：
#####4、查看pod启动
[root@master-1 ~]# kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
kafka-0             0/1     Running   7          18m
kafka-zookeeper-0   0/1     Running   1          18m
kafka-zookeeper-1   1/1     Running   0          12m
kafka-zookeeper-2   1/1     Running   0          9m51s
mongo-0             2/2     Running   2          12d
mongo-1             2/2     Running   2          12d
mongo-2             2/2     Running   2          12d
mongo-3             2/2     Running   2          11d
mongo-4             2/2     Running   2          11d
mongo-5             2/2     Running   2          11d
mysql-ss-0          1/2     Running   2          32h
mysql-ss-1          2/2     Running   1          32h
mysql-ss-2          2/2     Running   0          29h
redis-cluster-0     1/1     Running   1          26d
redis-cluster-1     1/1     Running   1          18d
redis-cluster-2     1/1     Running   1          26d
redis-cluster-3     1/1     Running   1          26d
redis-cluster-4     1/1     Running   1          26d
redis-cluster-5     1/1     Running   1          26d

#####5、查看 端口
查看服务：kubectl get services，如下图红框所示，通过宿主机IP:31090、宿主机IP:31091、宿主机IP:31092即可从外部访问kafka：
[root@master-1 ~]# kubectl get svc         
NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
kafka                      ClusterIP   10.1.70.186    <none>        9092/TCP                         30m
kafka-0-external           NodePort    10.1.23.35     <none>        19092:31090/TCP                  30m
kafka-1-external           NodePort    10.1.112.88    <none>        19092:31091/TCP                  30m
kafka-2-external           NodePort    10.1.60.141    <none>        19092:31092/TCP                  30m
kafka-headless             ClusterIP   None           <none>        9092/TCP                         30m
kafka-zookeeper            ClusterIP   10.1.40.125    <none>        2181/TCP                         30m
kafka-zookeeper-headless   ClusterIP   None           <none>        2181/TCP,3888/TCP,2888/TCP       30m
kubernetes                 ClusterIP   10.1.0.1       <none>        443/TCP                          34d
mongo                      ClusterIP   None           <none>        27017/TCP                        12d
mongo-service              NodePort    10.1.98.245    <none>        27017:27017/TCP                  12d
mysql-headless             ClusterIP   None           <none>        3306/TCP                         32h
mysql-readwrite            NodePort    10.1.24.219    <none>        3306:30006/TCP                   31h
mysqlread                  NodePort    10.1.138.131   <none>        3306:30036/TCP                   31h
redis-cluster              NodePort    10.1.231.82    <none>        6379:31000/TCP,16379:30701/TCP   18d

[root@master-1 ~]# netstat -antup|grep 3109 
tcp        0      0 0.0.0.0:31090           0.0.0.0:*               LISTEN      3889/kube-proxy     
tcp        0      0 0.0.0.0:31091           0.0.0.0:*               LISTEN      3889/kube-proxy     
tcp        0      0 0.0.0.0:31092           0.0.0.0:*               LISTEN      3889/kube-proxy     
[root@master-1 ~]# 


[root@master-1 kafka]# kubectl get svc|grep kafka
kafka                      ClusterIP   10.1.70.186    <none>        9092/TCP                         16h
kafka-0-external           NodePort    10.1.23.35     <none>        19092:31090/TCP                  16h
kafka-1-external           NodePort    10.1.112.88    <none>        19092:31091/TCP                  16h
kafka-2-external           NodePort    10.1.60.141    <none>        19092:31092/TCP                  16h
kafka-headless             ClusterIP   None           <none>        9092/TCP                         16h
kafka-zookeeper            ClusterIP   10.1.40.125    <none>        2181/TCP                         16h
kafka-zookeeper-headless   ClusterIP   None           <none>        2181/TCP,3888/TCP,2888/TCP       16h
[root@master-1 kafka]# 

#####6、连接测试：
[root@master-1 kafka]# kubectl exec -it kafka-0 bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@kafka-0:/# 
root@kafka-0:/# 
root@kafka-0:/#        
root@kafka-0:/# ls /usr/bin |grep kafka
kafka-acls
kafka-broker-api-versions
kafka-configs
kafka-console-consumer
kafka-console-producer
kafka-consumer-groups
kafka-consumer-perf-test
kafka-delegation-tokens
kafka-delete-records
kafka-dump-log
kafka-log-dirs
kafka-mirror-maker
kafka-preferred-replica-election
kafka-producer-perf-test
kafka-reassign-partitions
kafka-replica-verification
kafka-run-class
kafka-server-start
kafka-server-stop
kafka-streams-application-reset
kafka-topics
kafka-verifiable-consumer
kafka-verifiable-producer

root@kafka-0:/# ls /usr/share/java/kafka | grep kafka
kafka-clients-2.0.1-cp1.jar
kafka-log4j-appender-2.0.1-cp1.jar
kafka-streams-2.0.1-cp1.jar
kafka-streams-examples-2.0.1-cp1.jar
kafka-streams-scala_2.11-2.0.1-cp1.jar
kafka-streams-test-utils-2.0.1-cp1.jar
kafka-tools-2.0.1-cp1.jar
kafka.jar
kafka_2.11-2.0.1-cp1-javadoc.jar
kafka_2.11-2.0.1-cp1-scaladoc.jar
kafka_2.11-2.0.1-cp1-sources.jar
kafka_2.11-2.0.1-cp1-test-sources.jar
kafka_2.11-2.0.1-cp1-test.jar
kafka_2.11-2.0.1-cp1.jar


#####7、查看服务：
root@kafka-0:/# ps -efwww
UID         PID   PPID  C STIME TTY          TIME CMD
root          1      0  0 Oct26 ?        00:04:05 java -Xmx1G -Xms1G -server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.awt.headless=true -Xloggc:/var/log/kafka/kafkaServer-gc.log -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=10 -XX:GCLogFileSize=100M -Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=10.244.0.66 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=5555 -Dcom.sun.management.jmxremote.port=5555 -Dcom.sun.management.jmxremote.port=5555 -Dkafka.logs.dir=/var/log/kafka -Dlog4j.configuration=file:/etc/kafka/log4j.properties -cp /usr/bin/../share/java/kafka/*:/usr/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* io.confluent.support.metrics.SupportedKafka /etc/kafka/kafka.properties
root      74544      0  0 07:15 pts/0    00:00:00 bash
root      74570  74544  0 07:16 pts/0    00:00:00 ps -efwww
root@kafka-0:/# 


[root@master-1 ~]# kubectl  get pod
NAME                READY   STATUS    RESTARTS   AGE
kafka-0             0/1     Error     8          46m
kafka-1             0/1     Error     0          17m
kafka-2             0/1     Error     0          16m
kafka-zookeeper-0   0/1     Error     2          46m
kafka-zookeeper-1   1/1     Running   1          40m
kafka-zookeeper-2   0/1     Running   1          37m
mongo-0             2/2     Running   4          12d
mongo-1             0/2     Error     2          12d
mongo-2             0/2     Error     2          12d
mongo-3             0/2     Error     2          12d
mongo-4             2/2     Running   4          11d
mongo-5             0/2     Error     2          11d
mysql-ss-0          0/2     Error     4          32h
mysql-ss-1          0/2     Error     1          32h
mysql-ss-2          1/2     Running   3          30h
redis-cluster-0     1/1     Running   2          26d
redis-cluster-1     1/1     Running   2          18d
redis-cluster-2     1/1     Running   2          26d
redis-cluster-3     0/1     Error     1          26d
redis-cluster-4     1/1     Running   2          26d
redis-cluster-5     1/1     Running   2          26d

逐步都启动完毕
[root@master-1 ~]# kubectl  get pod
NAME                READY   STATUS    RESTARTS   AGE
kafka-0             0/1     Running   9          47m
kafka-1             0/1     Running   1          18m
kafka-2             0/1     Running   1          17m
kafka-zookeeper-0   0/1     Running   3          47m
kafka-zookeeper-1   1/1     Running   1          41m
kafka-zookeeper-2   1/1     Running   1          38m
mongo-0             2/2     Running   4          12d
mongo-1             2/2     Running   4          12d
mongo-2             0/2     Error     2          12d
mongo-3             0/2     Error     2          12d
mongo-4             2/2     Running   4          11d
mongo-5             2/2     Running   4          11d
mysql-ss-0          1/2     Running   6          32h
mysql-ss-1          1/2     Running   3          32h
mysql-ss-2          1/2     Running   3          30h
redis-cluster-0     1/1     Running   2          26d
redis-cluster-1     1/1     Running   2          18d
redis-cluster-2     1/1     Running   2          26d
redis-cluster-3     1/1     Running   2          26d
redis-cluster-4     1/1     Running   2          26d
redis-cluster-5     1/1     Running   2          26d

[root@master-1 ~]# kubectl  get pod
NAME                READY   STATUS    RESTARTS   AGE
kafka-0             1/1     Running   9          47m
kafka-1             1/1     Running   1          18m
kafka-2             1/1     Running   1          18m
kafka-zookeeper-0   1/1     Running   3          47m
kafka-zookeeper-1   1/1     Running   1          41m
kafka-zookeeper-2   1/1     Running   1          38m
mongo-0             2/2     Running   4          12d
mongo-1             2/2     Running   4          12d
mongo-2             2/2     Running   4          12d
mongo-3             2/2     Running   4          12d
mongo-4             2/2     Running   4          11d
mongo-5             2/2     Running   4          11d
mysql-ss-0          2/2     Running   6          32h
mysql-ss-1          1/2     Running   3          32h
mysql-ss-2          1/2     Running   3          30h
redis-cluster-0     1/1     Running   2          26d
redis-cluster-1     1/1     Running   2          18d
redis-cluster-2     1/1     Running   2          26d
redis-cluster-3     1/1     Running   2          26d
redis-cluster-4     1/1     Running   2          26d
redis-cluster-5     1/1     Running   2          26d
[root@master-1 ~]# 

#####8、查看kafka版本：
查看kafka版本：kubectl exec kafka-0 -n kafka-test -- sh -c 'ls /usr/share/java/kafka/kafka_*.jar' ，如下图红框所示，scala版本2.11，kafka版本2.0.1
[root@master-1 kafka]# kubectl exec kafka-0  -- sh -c 'ls /usr/share/java/kafka/kafka_*.jar'
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1-javadoc.jar
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1-scaladoc.jar
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1-sources.jar
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1-test-sources.jar
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1-test.jar
/usr/share/java/kafka/kafka_2.11-2.0.1-cp1.jar
[root@master-1 kafka]# 
kafka启动成功后，咱们来验证服务是否正常；




#####9、对外暴露zookeeper
为了远程操作kafka，有时需要连接到zookeeper，所以需要将zookeeper也暴露出来；
创建文件zookeeper-nodeport-svc.yaml，内容如下：
[root@master-1 helm]# vim zookeeper-nodeport-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-nodeport
spec:
  type: NodePort
  ports:
       - port: 2181
         nodePort: 31181
  selector:
    app: zookeeper
    release: kafka

[root@master-1 helm]# kubectl apply -f zookeeper-nodeport-svc.yaml
service/zookeeper-nodeport created

查看服务，发现已经可以通过宿主机IP:32181访问zookeeper了，如下:
[root@master-1 ~]# kubectl  get svc
NAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
kafka                      ClusterIP   10.1.70.186    <none>        9092/TCP                         17h
kafka-0-external           NodePort    10.1.23.35     <none>        19092:31090/TCP                  17h
kafka-1-external           NodePort    10.1.112.88    <none>        19092:31091/TCP                  17h
kafka-2-external           NodePort    10.1.60.141    <none>        19092:31092/TCP                  17h
kafka-headless             ClusterIP   None           <none>        9092/TCP                         17h
kafka-zookeeper            ClusterIP   10.1.40.125    <none>        2181/TCP                         17h
kafka-zookeeper-headless   ClusterIP   None           <none>        2181/TCP,3888/TCP,2888/TCP       17h
kubernetes                 ClusterIP   10.1.0.1       <none>        443/TCP                          34d
mongo                      ClusterIP   None           <none>        27017/TCP                        12d
mongo-service              NodePort    10.1.98.245    <none>        27017:27017/TCP                  12d
mysql-headless             ClusterIP   None           <none>        3306/TCP                         2d
mysql-readwrite            NodePort    10.1.24.219    <none>        3306:30006/TCP                   2d
mysqlread                  NodePort    10.1.138.131   <none>        3306:30036/TCP                   47h
redis-cluster              NodePort    10.1.231.82    <none>        6379:31000/TCP,16379:30701/TCP   19d
zookeeper-nodeport         NodePort    10.1.84.163    <none>        2181:31181/TCP                   36s

[root@master-1 ~]# netstat -antup|grep 31181
tcp        0      0 0.0.0.0:31181           0.0.0.0:*               LISTEN      3653/kube-proxy     



#####10、验证kafka服务
######（1）、pod外测试：
找一台电脑安装kafka包，就能通过里面自带的命令远程连接和操作K8S的kafka了：

访问kafka官网：http://kafka.apache.org/downloads ，刚才确定了scala版本2.11，kafka版本2.0.1，因此下载下图红框中的版本：
2.0.1
Released November 9, 2018
Release Notes
Source download: kafka-2.0.1-src.tgz (asc, sha512)
Binary downloads:
Scala 2.11  - kafka_2.11-2.0.1.tgz (asc, sha512) ##### 选它
Scala 2.12  - kafka_2.12-2.0.1.tgz (asc, sha512)
We build for multiple versions of Scala. This only matters if you are using Scala and you want a version built for the same Scala version you use. Otherwise any version should work (2.12 is recommended).


[root@master-1 helm]# wget https://archive.apache.org/dist/kafka/2.0.1/kafka_2.11-2.0.1.tgz
[root@master-1 kafka_2.11-2.0.1]# tar -xvf kafka_2.11-2.0.1.tgz ;/root/kafka/helm/kafka_2.11-2.0.1/bin


装java
[root@master-1 kafka_2.11-2.0.1]#yum install java -y
[root@master-1 kafka_2.11-2.0.1]#vim /etc/profile
source /etc/profile
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.302.b08-0.el7_9.x86_64/jre/
export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export PATH=$PATH:$JAVA_HOME/bin
[root@master-1 kafka_2.11-2.0.1]#source /etc/profile


空空如也：
[root@master-1 bin]# ./kafka-topics.sh --list --zookeeper 172.16.201.134:31181
[root@master-1 bin]# 

创建topic：
[root@master-1 bin]# ./kafka-topics.sh --create --zookeeper 172.16.201.134:31181 --replication-factor 1 --partitions 1 --topic test001
Created topic "test001".
[root@master-1 bin]# ./kafka-topics.sh --list --zookeeper 172.16.201.134:31181
test001
[root@master-1 bin]# 

查看名为test001的topic：
[root@master-1 bin]# ./kafka-topics.sh --describe --zookeeper 172.16.201.134:31181 --topic test001
Topic:test001   PartitionCount:1        ReplicationFactor:1     Configs:
        Topic: test001  Partition: 0    Leader: 1       Replicas: 1     Isr: 1
[root@master-1 bin]# 

进入创建消息的交互模式：
./kafka-console-producer.sh --broker-list 172.16.201.134:31090 --topic test001
进入交互模式后，输入任何字符串再输入回车，就会将当前内容作为一条消息发送出去：

kafka-console-producer --bootstrap-server 172.16.201.134:31181 --topic test001 --from-beginning
./zookeeper-shell.sh 172.16.201.134:31181  <<< "get /brokers/ids/0"


######（2）、pod内部测试：
#######a、在k8s集群内运行下面的客户端Pod，访问kafka broker进行测试：
[root@master-1 helm]# vim testclient.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: testclient
spec:
  containers:
  - name: kafka
    image: confluentinc/cp-kafka:5.0.1
    command:
    - sh
    - -c
    - "exec tail -f /dev/null"
[root@master-1 helm]# 
[root@master-1 helm]# kubectl apply -f testclient.yaml
pod/testclient created
[root@master-1 helm]# kubectl exec testclient -it bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@testclient:/# 

#######b、查看kafka相关命令：
root@testclient:/# ls /usr/bin/ | grep kafka
kafka-acls
kafka-broker-api-versions
kafka-configs
kafka-console-consumer
kafka-console-producer
kafka-consumer-groups
kafka-consumer-perf-test
kafka-delegation-tokens
kafka-delete-records
kafka-dump-log
kafka-log-dirs
kafka-mirror-maker
kafka-preferred-replica-election
kafka-producer-perf-test
kafka-reassign-partitions
kafka-replica-verification
kafka-run-class
kafka-server-start
kafka-server-stop
kafka-streams-application-reset
kafka-topics
kafka-verifiable-consumer
kafka-verifiable-producer
root@testclient:/# 

#######c、查看的Topic：
root@testclient:/# kafka-topics --zookeeper kafka-zookeeper:2181 --list
test001

#######注：kafka-zookeeper名字由 kubectl get svc 第一列NAME获得，端口由PORT(S) 获得
#######注：kafka-zookeeper名字由 kubectl get svc 第一列NAME获得，端口由PORT(S) 获得

#######d、进入创建消息的交互模式：
root@testclient:/# kafka-console-producer --broker-list kafka-headless:9092 --topic test001
>123
>123
>3412342134
>12341234
>1234
>2134
>1234
>1234
>发大水的发放
>2俄3 
>ssss
>ssssssssssss


再开一个窗口：
[root@master-1 bin]# kubectl exec testclient -it bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
root@testclient:/# 
root@testclient:/# 
root@testclient:/# kafka-console-consumer --bootstrap-server kafka-headless:9092 --topic test001 --from-beginning
123
123
3412342134
12341234
1234
2134
1234
1234
发大水的发放
2俄3 
ssss
ssssssssssss

#######e、再打开一个窗口，执行命令查看消费者group：
root@testclient:/# kafka-consumer-groups --bootstrap-server kafka-headless:9092 --list
console-consumer-69409
root@testclient:/# 
可见groupid等于console-consumer-69409

#######f、执行命令查看groupid等于console-consumer-69409的消费情况：
root@testclient:/# kafka-consumer-groups --group console-consumer-69409 --describe --bootstrap-server kafka-headless:9092
Consumer group 'console-consumer-69409' has no active members.

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
test001         0          13              13              0               -               -               -
root@testclient:/# 


#######g、查看配置：
root@testclient:/# zookeeper-shell kafka-zookeeper:2181  <<< "get /brokers/ids/0"
Connecting to kafka-zookeeper:2181
Welcome to ZooKeeper!
JLine support is enabled
WATCHER::
WatchedEvent state:SyncConnected type:None path:null
[zk: kafka-zookeeper:2181(CONNECTED) 0] get /brokers/ids/0
{"listener_security_protocol_map":{"PLAINTEXT":"PLAINTEXT"},"endpoints":["PLAINTEXT://10.244.0.66:9092"],"jmx_port":5555,"host":"10.244.0.66","timestamp":"1635317215086","port":9092,"version":4}
cZxid = 0x400000003
ctime = Wed Oct 27 06:46:55 UTC 2021
mZxid = 0x400000003
mtime = Wed Oct 27 06:46:55 UTC 2021
pZxid = 0x400000003
cversion = 0
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x100024fb0fb0000
dataLength = 194
numChildren = 0
[zk: kafka-zookeeper:2181(CONNECTED) 1] root@testclient:/# 

ip显示信息：
PLAINTEXT://10.244.0.66:9092"


#####11、清理资源
[root@master-1 k8s-mysql-replication]# kubectl delete -f .            
service "mysqlread" deleted
service "mysql-readwrite" deleted
Error from server (NotFound): error when deleting "cm.yaml": configmaps "mysql" not found
Error from server (NotFound): error when deleting "pv.yaml": persistentvolumes "pv-a" not found
Error from server (NotFound): error when deleting "pv.yaml": persistentvolumes "pv-b" not found
Error from server (NotFound): error when deleting "pv.yaml": persistentvolumes "pv-c" not found
Error from server (NotFound): error when deleting "pv.yaml": persistentvolumes "pv-d" not found
Error from server (NotFound): error when deleting "service.yaml": services "mysql-headless" not found
Error from server (NotFound): error when deleting "sfs.yaml": statefulsets.apps "mysql-ss" not found


本次实战创建了很多资源：rbac、role、serviceaccount、pod、deployment、service，下面的脚本可以将这些资源清理掉(只剩NFS的文件没有被清理掉)：

helm del --purge kafka
kubectl delete service zookeeper-nodeport
kubectl delete storageclass managed-nfs-storage
kubectl delete deployment nfs-client-provisioner -n kafka-test
kubectl delete clusterrolebinding run-nfs-client-provisioner
kubectl delete serviceaccount nfs-client-provisioner -n kafka-test
kubectl delete role leader-locking-nfs-client-provisioner -n kafka-test
kubectl delete rolebinding leader-locking-nfs-client-provisioner -n kafka-test
kubectl delete clusterrole nfs-client-provisioner-runner
kubectl delete namespace kafka-test
至此，K8S环境部署和验证kafka的实战就完成了，希望能给您提供一些参考；



清理：
 1076  kubectl get pv
 1077  kubectl get pvc
 1078  kubectl delete pvc data-mysql-ss-0
 1079  kubectl delete pvc data-mysql-ss-1
 1080  kubectl delete pvc data-mysql-ss-2
 1081  kubectl delete pvc data-mysql-ss-3
 1082  kubectl delete pvc data-mysql-ss-4
 1083  kubectl get pvc
 1084  kubectl delete pvc mongodb-persistent-storage-claim-mongo-0
 1085  kubectl delete pvc mongodb-persistent-storage-claim-mongo-1
 1086  kubectl delete pvc mongodb-persistent-storage-claim-mongo-2
 1087  kubectl delete pvc mongodb-persistent-storage-claim-mongo-3
 1088  kubectl delete pvc mongodb-persistent-storage-claim-mongo-4
 1089  kubectl delete pvc mongodb-persistent-storage-claim-mongo-6
 1090  kubectl delete pvc mongodb-persistent-storage-claim-mongo-5
 1091  kubectl get pvc
 1092  kubectl get pvc datadir-kafka-0
 1093  kubectl get pvc datadir-kafka-1
 1094  kubectl get pvc datadir-kafka-2
 1095  kubectl get pvc datadir-kafka-3
 1096  kubectl get pvc
 1097  kubectl get pvc datadir-kafka-0
 1098  kubectl delete pvc datadir-kafka-0
 1099  kubectl delete pvc datadir-kafka-1
 1100  kubectl delete pvc datadir-kafka-2
 1101  kubectl get pvc
 1102  kubectl get pv
 1103  kubectl delete pv pvc-00b6d3e1-5c2e-4a4d-8e0d-affad057464d
 1104  kubectl delete pv pvc-0106f431-e76e-4ae4-b6b2-addd76a09317
 1105  kubectl delete pv pvc-263d607a-35e3-4576-b15a-28d6c68e120f
 1106  kubectl delete pv pvc-590e3a5f-39af-4cb3-a005-0570264787d1
 1107  kubectl delete pv pvc-816c4cea-d6ef-4c3c-9460-529d7be02933
 1108  kubectl delete pv pvc-a815ab9a-c2fe-4036-b7fc-4cac32dc3ad0 
 1109  kubectl delete pv kafka-zk03
 1110  kubectl delete pv kafka-zk02
 1111  kubectl delete pv kafka-zk01
 1112  kubectl delete pv kafka-03
 1113  kubectl delete pv kafka-02
 1114  kubectl delete pv kafka-01


参考：
https://www.cnblogs.com/bolingcavalry/p/13917562.html
https://www.cnblogs.com/skgoo/p/11971883.html

#@@@@@@@@@@@@@@@@@@@@@@@@@@@

######附：无法删除 Bound状态的 pv
kafka-zk-pv-1                              5Gi        RWO            Retain           Bound       default/datadir-kafka-0                                                   local-storage                           117m

kubectl edit pv kafka-zk-pv-1 去掉Claim部分配置即可
######附：无法删除 Bound状态的 pv


######附：Kubernetes 
在v1.6之前的版本，通过volume.beta.kubernetes.io/storage-class注释类请求动态供应存储；
在v1.6版本之后，用户应该使用PersistentVolumeClaim对象的storageClassName参数来请求动态存储。

apiVersion: v1
kind: PersistentVolume
metadata:
  name: k8s-pv-kafka01
  namespace: tools
  labels:
    app: kafka
  annotations:
    volume.beta.kubernetes.io/storage-class: "mykafka"
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: /root/kafkadata/kafka01
  persistentVolumeReclaimPolicy: Recycle



        volumeMounts:
        - name: datadir
          mountPath: /var/lib/kafka
        readinessProbe:
          exec:
           command:
            - sh
            - -c
            - "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server=localhost:9092"
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
  volumeClaimTemplates:
  - metadata:
      name: datadir
      annotations:
        volume.beta.kubernetes.io/storage-class: "mykafka"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi

######附：Kubernetes 





##(十)、k8s部署 Prometheus

###1、Kubernetes Operator 介绍
在 Kubernetes 的支持下，管理和伸缩 Web 应用、移动应用后端以及 API 服务都变得比较简单了。其原因是这些应用一般都是无状态的，所以 Deployment 这样的基础 Kubernetes API 对象就可以在无需附加操作的情况下，对应用进行伸缩和故障恢复了。

而对于数据库、缓存或者监控系统等有状态应用的管理，就是个挑战了。这些系统需要应用领域的知识，来正确的进行伸缩和升级，当数据丢失或不可用的时候，要进行有效的重新配置。我们希望这些应用相关的运维技能可以编码到软件之中，从而借助 Kubernetes 的能力，正确的运行和管理复杂应用。

Operator 这种软件，使用 TPR(第三方资源，现在已经升级为 CRD) 机制对 Kubernetes API 进行扩展，将特定应用的知识融入其中，让用户可以创建、配置和管理应用。和 Kubernetes 的内置资源一样，Operator 操作的不是一个单实例应用，而是集群范围内的多实例。

Kubernetes 的 Prometheus Operator 为 Kubernetes 服务和 Prometheus 实例的部署和管理提供了简单的监控定义。

安装完毕后，Prometheus Operator提供了以下功能：

创建/毁坏： 在 Kubernetes namespace 中更容易启动一个 Prometheus 实例，一个特定的应用程序或团队更容易使用Operator。
简单配置: 配置 Prometheus 的基础东西，比如在 Kubernetes 的本地资源 versions, persistence, retention policies, 和 replicas。
Target Services 通过标签： 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。

######Prometheus Operator 系统架构：
Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。
Prometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。
Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。
ServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。
Service： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus 监控的对象，例如 Node Exporter Service、Mysql Exporter Service 等等。
Alertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群。

######Prometheus 三大套件
Server 主要负责数据采集和存储，提供PromQL查询语言的支持。
Alertmanager 警告管理器，用来进行报警。
Push Gateway 支持临时性Job主动推送指标的中间网关。

######Prometheus服务过程
Prometheus Daemon 负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。
Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。
Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。
PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。
Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。


###2、版本说明
线上kubernetes集群为1.16版本 Prometheus oprator 分支为0.4关于Prometheus oprator与kubernetes版本对应关系如下图。可见https://github.com/prometheus-operator/kube-prometheus.
注： Prometheus operator？ kube-prometheus? kube-prometheus 就是 Prometheus的一种operator的部署方式…Prometheus-operator 已经改名为 Kube-promethues。

官网：https://github.com/prometheus-operator/kube-prometheus


ubernetes compatibility matrix
The following versions are supported and work as we test against these versions in their respective branches. But note that other versions might work!

kube-prometheus stack	Kubernetes1.18	Kubernetes1.19	Kubernetes1.20	Kubernetes1.21
release-0.5	          ✔	               ✗	             ✗	             ✗
release-0.6	          ✗	               ✔	             ✗	             ✗
release-0.7	          ✗	               ✔	             ✔	             ✗
release-0.8	          ✗	               ✗	             ✔	             ✔
HEAD	                ✗	               ✗	             ✔	             ✔




###3、Kubernetes1.19下载v0.7.0版本：

[root@master-1 prometheus]# wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.7.0.tar.gz
[root@master-1 prometheus]# tar -xvf v0.7.0.tar.gz 
[root@master-1 prometheus]# ll
total 292
drwxrwxr-x 11 root root   4096 Dec 10  2020 kube-prometheus-0.7.0
-rw-r--r--  1 root root 294457 Nov  4 15:45 v0.7.0.tar.gz
[root@master-1 prometheus]# 

由于它的文件都存放在项目源码的 manifests 文件夹下，所以需要进入其中进行启动这些 kubernetes 应用 yaml 文件。又由于这些文件堆放在一起，不利于分类启动，所以这里将它们分类。

进入源码的 manifests 文件夹

先简单部署一下Prometheus oprator（or或者叫kube-promethus）。完成微信报警的集成，其他的慢慢在生成环境中研究。
基本过程就是Prometheus oprator 添加存储，增加微信报警，外部traefik代理应用。

[root@master-1 manifests]# cd /root/prometheus/kube-prometheus-0.7.0/manifests

部署Prometheus+Grafana+Alertmanager

###### 修改镜像源
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' setup/prometheus-operator-deployment.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-prometheus.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' alertmanager-alertmanager.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' kube-state-metrics-deployment.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' node-exporter-daemonset.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-adapter-deployment.yaml
[root@master-1 manifests]# grep -r quay.io ./

###4、安装CRD和prometheus-operator
[root@master-1 manifests]# kubectl apply -f setup/
namespace/monitoring created
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
service/prometheus-operator created
serviceaccount/prometheus-operator created
[root@master-1 manifests]# 

###5、查看进度：
[root@master-1 manifests]# kubectl get pod -n monitoring --watch
NAME                                   READY   STATUS              RESTARTS   AGE
prometheus-operator-66bd8d7bd7-n4hl6   0/2     ContainerCreating   0          3s
prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running             0          44s

装完了：
[root@master-1 ~]# kubectl get pod -n monitoring 
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running   0          2m19s


看看都建立了什么：
[root@master-1 ~]# kubectl get all -n monitoring    
NAME                                       READY   STATUS    RESTARTS   AGE
pod/prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running   0          2m26s

NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/prometheus-operator   ClusterIP   None         <none>        8443/TCP   2m27s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-operator   1/1     1            1           2m27s

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-operator-66bd8d7bd7   1         1         1       2m27s


也可以在部署前各节点提前下载镜像
为了保证服务启动速度，所以最好部署节点提前下载所需镜像。
docker pull quay.io/coreos/configmap-reload:v0.0.1
docker pull quay.io/coreos/prometheus-config-reloader:v0.29.0
docker pull quay.io/coreos/prometheus-operator:v0.44.1
docker pull quay.io/coreos/k8s-prometheus-adapter-amd64:v0.4.1
docker pull quay.io/prometheus/alertmanager:v0.17.0
docker pull quay.io/prometheus/node-exporter:v0.17.0 
docker pull quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0
docker pull quay.io/coreos/kube-state-metrics:v1.5.0
docker pull registry.aliyuncs.com/google_containers/addon-resizer:1.8.4
docker pull quay.io/prometheus/prometheus:v2.7.2


"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"grafana/grafana:7.3.4"
"quay.mirrors.ustc.edu.cn/coreos/kube-state-metrics:v1.9.7"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"directxman12/k8s-prometheus-adapter:v0.8.2"
"quay.mirrors.ustc.edu.cn/prometheus/prometheus:v2.22.1"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/prometheus:v2.22.1"


使用国外服务器下载镜像，并打包为tar包下载到本地。
docker pull k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0-rc.0
docker save k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0-rc.0 -o kube-state-metrics.tar





[root@master-1 manifests]# kubectl get event -n monitoring
LAST SEEN   TYPE      REASON              OBJECT                                      MESSAGE
25s         Normal    Scheduled           pod/prometheus-operator-66bd8d7bd7-4qx5g    Successfully assigned monitoring/prometheus-operator-66bd8d7bd7-4qx5g to node-2
23s         Normal    Pulled              pod/prometheus-operator-66bd8d7bd7-4qx5g    Container image "quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-operator:v0.44.1" already present on machine
22s         Normal    Created             pod/prometheus-operator-66bd8d7bd7-4qx5g    Created container prometheus-operator
22s         Normal    Started             pod/prometheus-operator-66bd8d7bd7-4qx5g    Started container prometheus-operator
22s         Normal    Pulled              pod/prometheus-operator-66bd8d7bd7-4qx5g    Container image "quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0" already present on machine
22s         Normal    Created             pod/prometheus-operator-66bd8d7bd7-4qx5g    Created container kube-rbac-proxy
21s         Normal    Started             pod/prometheus-operator-66bd8d7bd7-4qx5g    Started container kube-rbac-proxy
26s         Warning   FailedCreate        replicaset/prometheus-operator-66bd8d7bd7   Error creating: pods "prometheus-operator-66bd8d7bd7-" is forbidden: error looking up service account monitoring/prometheus-operator: serviceaccount "prometheus-operator" not found
25s         Normal    SuccessfulCreate    replicaset/prometheus-operator-66bd8d7bd7   Created pod: prometheus-operator-66bd8d7bd7-4qx5g
27s         Normal    ScalingReplicaSet   deployment/prometheus-operator              Scaled up replica set prometheus-operator-66bd8d7bd7 to 1


###6、安装prometheus, alertmanager, grafana, kube-state-metrics, node-exporter等资源,时间较长
#####增加 prometheus访问端口：
type: NodePort
nodePort: 32101


[root@master-1 manifests]# cat prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    prometheus: k8s
  name: prometheus-k8s
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: web
    port: 9090
    targetPort: web
		nodePort: 32101
  selector:
    app: prometheus
    prometheus: k8s
  sessionAffinity: ClientIP



#####增加grafana访问端口：
type: NodePort
nodePort: 32102

[root@master-1 manifests]# cat grafana-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: http
    port: 3000
    targetPort: http
    nodePort: 32102
  selector:
    app: grafana

#####执行安装：
[root@master-1 manifests]# kubectl apply -f .
alertmanager.monitoring.coreos.com/main created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager created
secret/grafana-datasources created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-statefulset created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
servicemonitor.monitoring.coreos.com/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-operator created
prometheus.monitoring.coreos.com/k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
[root@master-1 manifests]# 

#####查看：
[root@master-1 manifests]# kubectl get pod -n monitoring --watch
NAME                                   READY   STATUS              RESTARTS   AGE
alertmanager-main-0                    0/2     ContainerCreating   0          30s
alertmanager-main-1                    0/2     ContainerCreating   0          30s
alertmanager-main-2                    0/2     ContainerCreating   0          30s
grafana-f8cd57fcf-pz56g                0/1     ContainerCreating   0          23s
kube-state-metrics-58c88f48b7-5bqrj    0/3     ContainerCreating   0          21s
node-exporter-5hzp8                    0/2     ContainerCreating   0          16s
node-exporter-k2cmg                    0/2     ContainerCreating   0          18s
node-exporter-qtph2                    0/2     ContainerCreating   0          16s
prometheus-adapter-69b8496df6-5z9x5    0/1     ContainerCreating   0          8s
prometheus-k8s-0                       0/2     ContainerCreating   0          6s
prometheus-k8s-1                       0/2     ContainerCreating   0          6s
prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running             0          2m20s


#####纠结了很长时间：
[root@master-1 manifests]# kubectl get pod -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
alertmanager-main-0                    2/2     Running   0          7m5s
alertmanager-main-1                    2/2     Running   0          7m5s
alertmanager-main-2                    2/2     Running   0          7m5s
grafana-f8cd57fcf-pz56g                1/1     Running   0          6m58s
kube-state-metrics-58c88f48b7-5bqrj    3/3     Running   0          6m56s
node-exporter-5hzp8                    2/2     Running   0          6m51s
node-exporter-k2cmg                    2/2     Running   0          6m53s
node-exporter-qtph2                    2/2     Running   0          6m51s
prometheus-adapter-69b8496df6-5z9x5    1/1     Running   0          6m43s
prometheus-k8s-0                       2/2     Running   1          6m41s
prometheus-k8s-1                       2/2     Running   2          6m41s
prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running   0          8m55s



#####查看所有信息：
[root@master-1 manifests]# kubectl get all -n monitoring 
NAME                                       READY   STATUS    RESTARTS   AGE
pod/alertmanager-main-0                    2/2     Running   0          11m
pod/alertmanager-main-1                    2/2     Running   0          11m
pod/alertmanager-main-2                    2/2     Running   0          11m
pod/grafana-f8cd57fcf-pz56g                1/1     Running   0          10m
pod/kube-state-metrics-58c88f48b7-5bqrj    3/3     Running   0          10m
pod/node-exporter-5hzp8                    2/2     Running   0          10m
pod/node-exporter-k2cmg                    2/2     Running   0          10m
pod/node-exporter-qtph2                    2/2     Running   0          10m
pod/prometheus-adapter-69b8496df6-5z9x5    1/1     Running   0          10m
pod/prometheus-k8s-0                       2/2     Running   1          10m
pod/prometheus-k8s-1                       2/2     Running   2          10m
pod/prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running   0          12m

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.1.24.38     <none>        9093/TCP                     11m
service/alertmanager-operated   ClusterIP   None           <none>        9093/TCP,9094/TCP,9094/UDP   11m
service/grafana                 ClusterIP   10.1.109.224   <none>        3000/TCP                     11m
service/kube-state-metrics      ClusterIP   None           <none>        8443/TCP,9443/TCP            10m
service/node-exporter           ClusterIP   None           <none>        9100/TCP                     10m
service/prometheus-adapter      ClusterIP   10.1.101.198   <none>        443/TCP                      10m
service/prometheus-k8s          ClusterIP   10.1.102.83    <none>        9090/TCP                     10m
service/prometheus-operated     ClusterIP   None           <none>        9090/TCP                     10m
service/prometheus-operator     ClusterIP   None           <none>        8443/TCP                     12m

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   3         3         3       3            3           kubernetes.io/os=linux   10m

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               1/1     1            1           11m
deployment.apps/kube-state-metrics    1/1     1            1           10m
deployment.apps/prometheus-adapter    1/1     1            1           10m
deployment.apps/prometheus-operator   1/1     1            1           12m

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-f8cd57fcf                1         1         1       11m
replicaset.apps/kube-state-metrics-58c88f48b7    1         1         1       10m
replicaset.apps/prometheus-adapter-69b8496df6    1         1         1       10m
replicaset.apps/prometheus-operator-66bd8d7bd7   1         1         1       12m

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   3/3     11m
statefulset.apps/prometheus-k8s      2/2     10m




#####查看端口
[root@master-1 manifests]# kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.1.24.38     <none>        9093/TCP                     13m
alertmanager-operated   ClusterIP   None           <none>        9093/TCP,9094/TCP,9094/UDP   13m
grafana                 ClusterIP   10.1.109.224   <none>        3000/TCP                     12m
kube-state-metrics      ClusterIP   None           <none>        8443/TCP,9443/TCP            12m
node-exporter           ClusterIP   None           <none>        9100/TCP                     12m
prometheus-adapter      ClusterIP   10.1.101.198   <none>        443/TCP                      12m
prometheus-k8s          ClusterIP   10.1.102.83    <none>        9090/TCP                     12m
prometheus-operated     ClusterIP   None           <none>        9090/TCP                     12m
prometheus-operator     ClusterIP   None           <none>        8443/TCP                     14m


#####访问地址：
prometheus
http://172.16.201.134:32101/graph


grafana
http://172.16.201.134:32102/
admin/admin




###7、安装metrics-server（下载指定版本，不能下最新的）

####Kubernetes Metrics Server：

Kubernetes Metrics Server 是 Cluster 的核心监控数据的聚合器，kubeadm 默认是不部署的。

Metrics Server 供 Dashboard 等其他组件使用，是一个扩展的 APIServer，依赖于 API Aggregator。所以，在安装 Metrics Server 之前需要先在 kube-apiserver 中开启 API Aggregator。
Metrics API 只可以查询当前的度量数据，并不保存历史数据。
Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 下维护。
必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 kubelet Summary API 获取数据。

####前提条件

注意：使用 Metrics Server 有必备两个条件：

1、API Server 启用 Aggregator Routing 支持。否则 API Server 不识别请求：

Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)
2、API Server 能访问 Metrics Server Pod IP。否则 API Server 无法访问 Metrics Server：

E1223 07:23:04.330206       1 available_controller.go:420] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.171.248.214:4443/apis/metrics.k8s.io/v1beta1: Get https://10.171.248.214:4443/apis/metrics.k8s.io/v1beta1: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

3、启用API Aggregator，API Aggregation 允许在不修改 Kubernetes 核心代码的同时扩展 Kubernetes API，即：将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问第三方服务了，例如：Metrics Server API。注：另外一种扩展 Kubernetes API 的方法是使用 CRD（Custom Resource Definition，自定义资源定义）。


###安装：
####1、修改每个 API Server 的 kube-apiserver.yaml 配置开启 Aggregator Routing：
####修改 manifests 配置后 API Server 会自动重启生效。
[root@master-1 prometheus]# vim /etc/kubernetes/manifests/kube-apiserver.yaml
    - --secure-port=6443
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-cluster-ip-range=10.1.0.0/16
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --enable-aggregator-routing=true # 添加本行

[root@master-1 base]# systemctl restart kubelet
[root@master-1 base]# kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml
######不行执行2次，最好重启kubelet

####2、下载：
[root@master-1 prometheus]# wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml

####3、修改：
[root@master-1 prometheus]# vim components.yaml
        - --kubelet-preferred-address-types=InternalIP   # 删掉 ExternalIP,Hostname这两个，这里已经改好了，你那边要自己核对一下
        - --kubelet-use-node-status-port
        - --kubelet-insecure-tls                    #   加上该启动参数
        image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1                 # 镜像地址根据情况修改

####4、执行安装：
[root@master-1 base]# kubectl delete -f components.yaml
[root@master-1 base]# kubectl apply -f components.yaml

[root@master-1 prometheus]# kubectl apply -f components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created

####5、查看pod：
[root@master-1 prometheus]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-bfcc967d6-b7s8z     0/1     ContainerCreating   0          7s
[root@master-1 prometheus]# 
[root@master-1 prometheus]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-bfcc967d6-b7s8z     1/1     Running   0          75s

####6、查看具体信息：
[root@master-1 prometheus]# kubectl describe svc metrics-server -n kube-system
Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP:                10.1.21.56
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         10.244.1.24:4443
Session Affinity:  None
Events:            <none>

####7、测试网络：
[root@master-1 prometheus]# ping 10.244.1.24
PING 10.244.1.24 (10.244.1.24) 56(84) bytes of data.
64 bytes from 10.244.1.24: icmp_seq=1 ttl=63 time=0.693 ms
64 bytes from 10.244.1.24: icmp_seq=2 ttl=63 time=0.693 ms
64 bytes from 10.244.1.24: icmp_seq=3 ttl=63 time=0.868 ms

####8、测试命令：
[root@master-1 prometheus]#  kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master-1   294m         14%    1337Mi          36%       
node-1     1317m        65%    1432Mi          83%       
node-2     192m         9%     1189Mi          69%       
[root@master-1 prometheus]# 


[root@master-1 prometheus]#  kubectl top pods
NAME                           CPU(cores)   MEMORY(bytes)   
test-centos-6ccff47f57-vhtvc   0m           5Mi  

[root@master-1 prometheus]# kubectl top pod --all-namespaces
NAMESPACE     NAME                                   CPU(cores)   MEMORY(bytes)   
default       test-centos-6ccff47f57-vhtvc           0m           5Mi             
kube-system   coredns-6d56c8448f-7c4dp               4m           16Mi            
kube-system   coredns-6d56c8448f-pm6r6               4m           16Mi            
kube-system   etcd-master-1                          34m          96Mi            
kube-system   kube-apiserver-master-1                97m          455Mi           
kube-system   kube-controller-manager-master-1       20m          58Mi            
kube-system   kube-flannel-ds-cvmdw                  10m          18Mi            
kube-system   kube-flannel-ds-gcckr                  5m           32Mi            
kube-system   kube-flannel-ds-nkl2p                  3m           31Mi            
kube-system   kube-proxy-fkj8w                       1m           30Mi            
kube-system   kube-proxy-glbfj                       1m           17Mi            
kube-system   kube-proxy-pxfg7                       10m          16Mi            
kube-system   kube-scheduler-master-1                5m           25Mi            
kube-system   metrics-server-bfcc967d6-b7s8z         31m          15Mi            
monitoring    alertmanager-main-0                    27m          22Mi            
monitoring    alertmanager-main-1                    5m           28Mi            
monitoring    alertmanager-main-2                    19m          22Mi            
monitoring    grafana-f8cd57fcf-pz56g                10m          52Mi            
monitoring    kube-state-metrics-58c88f48b7-5bqrj    3m           43Mi            
monitoring    node-exporter-5hzp8                    6m           38Mi            
monitoring    node-exporter-k2cmg                    21m          17Mi            
monitoring    node-exporter-qtph2                    6m           21Mi            
monitoring    prometheus-adapter-69b8496df6-6lcbj    9m           23Mi            
monitoring    prometheus-k8s-0                       126m         308Mi           
monitoring    prometheus-k8s-1                       18m          340Mi           
monitoring    prometheus-operator-66bd8d7bd7-hdq2p   3m           48Mi            
[root@master-1 prometheus]# 



[root@master-1 base]# kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master-1   303m         15%    1719Mi          46%       
node-1     164m         8%     1651Mi          96%       
node-2     209m         10%    1310Mi          76%    


[root@master-1 base]# kubectl top pods -n monitoring
NAME                                   CPU(cores)   MEMORY(bytes)   
alertmanager-main-0                    7m           21Mi            
alertmanager-main-1                    4m           23Mi            
alertmanager-main-2                    5m           19Mi            
grafana-f8cd57fcf-pz56g                10m          43Mi            
kube-state-metrics-58c88f48b7-5bqrj    1m           40Mi            
node-exporter-5hzp8                    8m           14Mi            
node-exporter-k2cmg                    6m           25Mi            
node-exporter-qtph2                    8m           18Mi            
prometheus-adapter-69b8496df6-6lcbj    7m           23Mi            
prometheus-k8s-0                       41m          273Mi           
prometheus-k8s-1                       17m          277Mi           
prometheus-operator-66bd8d7bd7-hdq2p   10m          42Mi      


[root@master-1 prometheus]# kubectl  get endpoints -n kube-system 
NAME                      ENDPOINTS                                                                    AGE
kube-controller-manager   <none>                                                                       6d23h
kube-dns                  10.244.0.13:53,10.244.0.14:53,10.244.0.13:53 + 3 more...                     6d23h
kube-scheduler            <none>                                                                       6d23h
kubelet                   172.16.201.134:10250,172.16.201.135:10250,172.16.201.136:10250 + 6 more...   22h
metrics-server            10.244.1.24:4443                                                             7m51s


[root@master-1 prometheus]# kubectl get apiservices |egrep metrics
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        8m15s




####9、故障涉及命令如下：

[root@master-1 base]# kubectl get apiservices |egrep metrics
v1beta1.metrics.k8s.io                 kube-system/metrics-server   False (MissingEndpoints)   2m45s


[root@master-1 base]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-95b5cb586-4nsm8     1/1     Running   0          7m21s

[root@master-1 base]# kubectl get endpoints
NAME         ENDPOINTS             AGE
kubernetes   172.16.201.134:6443   6d6h

[root@master-1 base]# kubectl  get endpoints -n kube-system 
NAME                      ENDPOINTS                                                                    AGE
kube-controller-manager   <none>                                                                       6d6h
kube-dns                  10.244.0.11:53,10.244.0.12:53,10.244.0.11:53 + 3 more...                     6d6h
kube-scheduler            <none>                                                                       6d6h
kubelet                   172.16.201.134:10250,172.16.201.135:10250,172.16.201.136:10250 + 6 more...   5h20m
metrics-server            <none>                                                                       7m55s
[root@master-1 base]# 

metrics-server ENDPOINTS没地址



[root@master-1 base]#  kubectl  get svc -n kube-system             
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
kube-dns         ClusterIP   10.1.0.10     <none>        53/UDP,53/TCP,9153/TCP         6d6h
kubelet          ClusterIP   None          <none>        10250/TCP,10255/TCP,4194/TCP   5h21m
metrics-server   ClusterIP   10.1.89.103   <none>        443/TCP                        8m38s
	
	
[root@master-1 base]#  kubectl api-versions|grep metrics
metrics.k8s.io/v1beta1
[root@master-1 base]# 


kubectl logs metrics-server-c778b76f8-9kzp4 -n kube-system
kubectl logs metrics-server-c778b76f8-9kzp4 -n kube-system



[root@master-1 etcd-v3.5.1-linux-amd64]# etcdctl version
etcdctl version: 3.5.1
API version: 3.5



ETCDCTL_API=3 /usr/local/bin/etcdctl --endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
--key=/etc/kubernetes/pki/etcd/healthcheck-client.key

alias etcdctl="ETCDCTL_API=3 /bin/etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key"

etcdctl get / --prefix --keys-only



[root@m1master kubernetes]# etcdctl get /registry/services --prefix --keys-only
/registry/services/endpoints/default/kubernetes
/registry/services/endpoints/istio-system/istio-ingressgateway
/registry/services/endpoints/istio-system/istiod-1-6-10
/registry/services/endpoints/istio-system/jaeger-collector
/registry/services/endpoints/istio-system/jaeger-collector-headless
/registry/services/endpoints/istio-system/jaeger-operator-metrics



[root@master-1 base]# kubectl describe APIService  
。。。。。。。。。。。
Name:         v1beta1.metrics.k8s.io
Namespace:    
Labels:       <none>
Annotations:  <none>
API Version:  apiregistration.k8s.io/v1
Kind:         APIService
Metadata:
  Creation Timestamp:  2021-11-04T14:12:27Z
  Resource Version:    373642
  Self Link:           /apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io
  UID:                 73f03067-8e6c-4097-8d17-b9689572e9c8
Spec:
  Group:                     metrics.k8s.io
  Group Priority Minimum:    100
  Insecure Skip TLS Verify:  true
  Service:
    Name:            metrics-server
    Namespace:       kube-system
    Port:            443
  Version:           v1beta1
  Version Priority:  100
Status:
  Conditions:
    Last Transition Time:  2021-11-04T14:12:27Z
    Message:               endpoints for service/metrics-server in "kube-system" have no addresses with port name "https"
    Reason:                MissingEndpoints
    Status:                False
    Type:                  Available
Events:                    <none>
。。。。。。。。。。。



[root@master-1 base]# kubectl get apiservice v1beta1.metrics.k8s.io
NAME                     SERVICE                      AVAILABLE                  AGE
v1beta1.metrics.k8s.io   kube-system/metrics-server   False (MissingEndpoints)   1


[root@master-1 ~]# kubectl exec -it metrics-server-5646c885bc-2568r -- bash    



也可以直接通过 kubectl 命令来访问这些 API，比如：
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name>
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>



[root@master-1 base]# kubectl -n kube-system edit deploy metrics-server
 

kubectl  get endpoints -n kube-system

####10、引入服务：
kind: Endpoints
apiVersion: v1
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
subsets:
  - addresses:
      - ip: 10.1.168.106
    ports:
      - port: 443
        protocol: TCP
        name: https

####11、不出图问题处理
访问：
http://172.16.201.134:32101/service-discovery

注意时间 如果提示如下，就是服务器时间跟浏览器时间不一致，意思就是服务器与浏览器时间不同步
然后我进行了核实，确实相差这么多时间。时间调整完后，不用重启Prometheus ，Prometheus 的web端报警消失。
Warning! Detected 254.70 seconds time difference between your browser and the server. Prometheus relies on accurate time and time drift might cause unexpected query results.


左侧加号-->Import via grafana.com输入：11074-->点load-->点Import

添加 Dashboard -> New Dashboard -> Import Dashboard -> 输入11074，导入Linux监控模板. 并配置数据源为Prometheus，即上一步中的name
配置完保存后即可看到逼格非常高的系统主机节点监控信息，包括系统运行时间, 内存和CPU的配置, CPU、内存、磁盘、网络流量等信息, 以及磁盘IO、CPU温度等信息。


####12、参考资料:
https://www.cnblogs.com/shunzi115/p/13950131.html
https://www.cnblogs.com/walkersss/p/12553189.html
官网地址：https://prometheus.io/
GitHub: https://github.com/prometheus
官方文档中文版: https://github.com/Alrights/prometheus
官方监控agent列表:https://prometheus.io/docs/instrumenting/exporters/



##(十一)、k8s部署MySQL8  多主MGR集群(未成功)


172.16.201.134  master-1 ceph-manager k8s1 k8s2 mysql-mgr-0
172.16.201.135  node-1 ceph-mon-1 k8s3 mysql-mgr-1
172.16.201.136  node-2 ceph-osd-1 k8s4 mysql-mgr-2


[root@master-1 .ssh]# ssh-keygen  
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.135
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.136


 1046  ssh k8s1
 1047  ssh k8s2
 1048  ssh k8s3
 1049  ssh k8s4


[root@master-1 ~]# kubectl get node -o wide
NAME       STATUS   ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
master-1   Ready    master   7d8h   v1.19.9   172.16.201.134   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-1     Ready    <none>   7d8h   v1.19.9   172.16.201.135   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-2     Ready    <none>   7d8h   v1.19.9   172.16.201.136   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13



在使用 Kubernetes 部署应用后，一般会习惯与将应用的配置文件外置，用 ConfigMap 存储，然后挂载进入镜像内部。这样，只要修改 ConfigMap 里面的配置，再重启应用就能很方便就能够使应用重新加载新的配置，很方便。

创建 ConfigMap 存储 Mysql 配置文件
创建 Kubernetes 的 ConfigMap 资源，用于存储 Mysql 的配置文件 my.cnf 内容：


[root@node-1 ~]# docker pull mysql
Using default tag: latest
latest: Pulling from library/mysql
b380bbd43752: Pull complete 
f23cbf2ecc5d: Pull complete 
30cfc6c29c0a: Pull complete 
b38609286cbe: Pull complete 
8211d9e66cd6: Pull complete 
2313f9eeca4a: Pull complete 
7eb487d00da0: Pull complete 
4d7421c8152e: Pull complete 
77f3d8811a28: Pull complete 
cce755338cba: Pull complete 
69b753046b9f: Pull complete 
b2e64b0ab53c: Pull complete 
Digest: sha256:6d7d4524463fe6e2b893ffc2b89543c81dec7ef82fb2020a1b27606666464d87
Status: Downloaded newer image for mysql:latest
docker.io/library/mysql:latest
[root@node-1 ~]# 




[root@master-1 mysql8]# vim namespace.yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: mysqldb


[root@master-1 mysql8]# vim service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mgrtest
  namespace: mysqldb
spec:
  selector:
    name: mysql-mgr
  clusterIP:   None
  ports:
  - name:   foo
    port:   3306
    targetPort: 3306
[root@master-1 mysql8]# 


---节点1的configmap文件
vim mysql-mgr-cnf-0.yaml
apiVersion: v1
data:
  mysql-mgr-0.cnf: |
    [mysqld]
    port = 3306
    character_set_server = utf8
    socket = /tmp/mysql.sock
    basedir = /usr/local/mysql
    log-error = /data/mysql/data/mysql.err
    pid-file = /data/mysql/data/mysql.pid
    datadir = /data/mysql/data
    server_id = 092832
    log_bin = mysql-bin
    relay-log = relay-bin
    interactive_timeout = 5022397
    max_connect_errors = 1000
    relay-log-recovery=1
    sort_buffer_size = 4M
    read_buffer_size = 4M
    join_buffer_size = 8M
    thread_cache_size = 64
    log_slave_updates=1
    long_query_time = 1
    slow_query_log = 1
    slow_query_log_file = /data/mysql/data/slow_sql.log
    skip-name-resolve
    sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
    innodb_buffer_pool_size=700M
    innodb_flush_log_at_trx_commit=1
    innodb_log_buffer_size = 16M
    innodb_log_file_size = 256M
    innodb_log_files_in_group = 2
    innodb_max_dirty_pages_pct = 50
    sync_binlog=1
    master_info_repository=TABLE
    relay_log_info_repository=TABLE
    log_timestamps=SYSTEM
    gtid_mode = ON
    enforce_gtid_consistency = ON
    master_info_repository = TABLE
    relay_log_info_repository = TABLE
    log_slave_updates = ON
    binlog_checksum = NONE
    log_slave_updates = ON
    slave_parallel_type=LOGICAL_CLOCK
    slave_parallel_workers=8
    slave-preserve-commit-order=on
    transaction_write_set_extraction = XXHASH64
    loose-group_replication_group_name="01e5fb97-be64-41f7-bafd-3afc7a6ab555"
    loose-group_replication_start_on_boot=off
    loose-group_replication_local_address="mysql-mgr-0.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_group_seeds="mysql-mgr-0.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-1.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-2.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_bootstrap_group = off
    loose-group_replication_ip_whitelist='10.244.0.0/16,172.17.0.0/16,10.229.0.0/16,10.228.0.0/16'
    report_host = mysql-mgr-0.mgrtest.mysqldb.svc.cluster.local
    [mysqldump]
    quick
    max_allowed_packet = 32M
kind: ConfigMap
metadata:
  name: mysql-mgr-0-cnf
  namespace: mysqldb


---节点2的configmap文件
vim mysql-mgr-cnf-1.yaml
apiVersion: v1
data:
  mysql-mgr-1.cnf: |
    [mysqld]
    port = 3306
    character_set_server = utf8
    socket = /tmp/mysql.sock
    basedir = /usr/local/mysql
    log-error = /data/mysql/data/mysql.err
    pid-file = /data/mysql/data/mysql.pid
    datadir = /data/mysql/data
    server_id = 092231
    log_bin = mysql-bin
    relay-log = relay-bin
    interactive_timeout = 5022397
    max_connect_errors = 1000
    relay-log-recovery=1
    sort_buffer_size = 4M
    read_buffer_size = 4M
    join_buffer_size = 8M
    thread_cache_size = 64
    log_slave_updates=1
    long_query_time = 1
    slow_query_log = 1
    slow_query_log_file = /data/mysql/data/slow_sql.log
    skip-name-resolve
    sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
    innodb_buffer_pool_size=700M
    innodb_flush_log_at_trx_commit=1
    innodb_log_buffer_size = 16M
    innodb_log_file_size = 256M
    innodb_log_files_in_group = 2
    innodb_max_dirty_pages_pct = 50
    sync_binlog=1
    master_info_repository=TABLE
    relay_log_info_repository=TABLE
    log_timestamps=SYSTEM
    gtid_mode = ON
    enforce_gtid_consistency = ON
    master_info_repository = TABLE
    relay_log_info_repository = TABLE
    log_slave_updates = ON
    binlog_checksum = NONE
    log_slave_updates = ON
    slave_parallel_type=LOGICAL_CLOCK
    slave_parallel_workers=8
    slave-preserve-commit-order=on
    transaction_write_set_extraction = XXHASH64
    loose-group_replication_group_name="01e5fb97-be64-41f7-bafd-3afc7a6ab555"
    loose-group_replication_start_on_boot=off
    loose-group_replication_local_address="mysql-mgr-1.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_group_seeds="mysql-mgr-0.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-1.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-2.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_bootstrap_group = off
    loose-group_replication_ip_whitelist='10.244.0.0/16,172.17.0.0/16,10.229.0.0/16,10.228.0.0/16'
    report_host = mysql-mgr-1.mgrtest.mysqldb.svc.cluster.local
    [mysqldump]
    quick
    max_allowed_packet = 32M
kind: ConfigMap
metadata:
  name: mysql-mgr-1-cnf
  namespace: mysqldb


---节点3的configmap文件
vim mysql-mgr-cnf-2.yaml
apiVersion: v1
data:
  mysql-mgr-2.cnf: |
    [mysqld]
    port = 3306
    character_set_server = utf8
    socket = /tmp/mysql.sock
    basedir = /usr/local/mysql
    log-error = /data/mysql/data/mysql.err
    pid-file = /data/mysql/data/mysql.pid
    datadir = /data/mysql/data
    server_id = 092132
    log_bin = mysql-bin
    relay-log = relay-bin
    interactive_timeout = 5022397
    max_connect_errors = 1000
    relay-log-recovery=1
    sort_buffer_size = 4M
    read_buffer_size = 4M
    join_buffer_size = 8M
    thread_cache_size = 64
    log_slave_updates=1
    long_query_time = 1
    slow_query_log = 1
    slow_query_log_file = /data/mysql/data/slow_sql.log
    skip-name-resolve
    sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
    innodb_buffer_pool_size=700M
    innodb_flush_log_at_trx_commit=1
    innodb_log_buffer_size = 16M
    innodb_log_file_size = 256M
    innodb_log_files_in_group = 2
    innodb_max_dirty_pages_pct = 50
    sync_binlog=1
    master_info_repository=TABLE
    relay_log_info_repository=TABLE
    log_timestamps=SYSTEM
    gtid_mode = ON
    enforce_gtid_consistency = ON
    master_info_repository = TABLE
    relay_log_info_repository = TABLE
    log_slave_updates = ON
    binlog_checksum = NONE
    log_slave_updates = ON
    slave_parallel_type=LOGICAL_CLOCK
    slave_parallel_workers=8
    slave-preserve-commit-order=on
    transaction_write_set_extraction = XXHASH64
    loose-group_replication_group_name="01e5fb97-be64-41f7-bafd-3afc7a6ab555"
    loose-group_replication_start_on_boot=off
    loose-group_replication_local_address="mysql-mgr-2.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_group_seeds="mysql-mgr-0.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-1.mgrtest.mysqldb.svc.cluster.local.:13306,mysql-mgr-2.mgrtest.mysqldb.svc.cluster.local.:13306"
    loose-group_replication_bootstrap_group = off
    loose-group_replication_ip_whitelist='10.244.0.0/16,172.17.0.0/16,10.229.0.0/16,10.228.0.0/16'
    report_host = mysql-mgr-2.mgrtest.mysqldb.svc.cluster.local
    [mysqldump]
    quick
    max_allowed_packet = 32M
kind: ConfigMap
metadata:
  name: mysql-mgr-2-cnf
  namespace: mysqldb






----节点1的pod的yaml文件:
[root@master-1 mysql8]# vim node1_pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: mysql-mgr-0
  namespace: mysqldb
  labels:
    name: mysql-mgr
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: mysqlrole
              operator: In
              values: ["mysql-mgr-0"]
  hostname: mysql-mgr-0
  subdomain: mgrtest
  containers:
  - image: mysql:latest
    name: mysql-mgr-0
    imagePullPolicy: IfNotPresent
    command: [ "/bin/bash", "-ce", "cd /usr/local/mysql && bin/mysqld_safe --defaults-file=/etc/my.cnf && tail -f /dev/null" ]
    ports:
      - containerPort: 3306
    volumeMounts:
    - name: tz-config
      mountPath: /etc/localtime
    - name: mysql-data
      mountPath: /data/mysql/data/
    - name: mysql-config
      mountPath: /etc/my.cnf
      subPath: my.cnf
    env:
    - name: INNODB_BUFFER_POOL_SIZE
      value: 100M
  volumes:
    - name: tz-config
      hostPath:
        path: /etc/localtime
    - name: mysql-data
      hostPath:
        path: /data/mysql/data/
    - name: mysql-config
      configMap:
        name: mysql-mgr-0-cnf
        items:
          - key: mysql-mgr-0.cnf
            path: my.cnf
[root@master-1 mysql8]# 


----节点2的pod的yaml文件:
[root@master-1 mysql8]# vim node2_pod.yaml  
apiVersion: v1
kind: Pod
metadata:
  name: mysql-mgr-1
  namespace: mysqldb
  labels:
    name: mysql-mgr
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: mysqlrole
              operator: In
              values: ["mysql-mgr-1"]
  hostname: mysql-mgr-1
  subdomain: mgrtest
  containers:
  - image: mysql8.0:latest
    name: mysql-mgr-1
    imagePullPolicy: IfNotPresent
    command: [ "/bin/bash", "-ce", "cd /usr/local/mysql && bin/mysqld_safe --defaults-file=/etc/my.cnf && tail -f /dev/null" ]
    ports:
      - containerPort: 3306
    volumeMounts:
    - name: tz-config
      mountPath: /etc/localtime
    - name: mysql-data
      mountPath: /data/mysql/data
    - name: mysql-config
      mountPath: /etc/my.cnf
      subPath: my.cnf
    env:
    - name: INNODB_BUFFER_POOL_SIZE
      value: 500M
  volumes:
    - name: tz-config
      hostPath:
        path: /etc/localtime
    - name: mysql-data
      hostPath:
        path: /data/mysql/data/
    - name: mysql-config
      configMap:
        name: mysql-mgr-1-cnf
        items:
          - key: mysql-mgr-1.cnf
            path: my.cnf




---节点3的pod的yaml文件:
[root@master-1 mysql8]# vim node3_pod.yaml  
apiVersion: v1
kind: Pod
metadata:
  name: mysql-mgr-2
  namespace: mysqldb
  labels:
    name: mysql-mgr
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: mysqlrole
              operator: In
              values: ["mysql-mgr-2"]
  hostname: mysql-mgr-2
  subdomain: mgrtest
  containers:
  - image: mysql8.0:latest
    name: mysql-mgr-2
    imagePullPolicy: IfNotPresent
    command: [ "/bin/bash", "-ce", "cd /usr/local/mysql && bin/mysqld_safe --defaults-file=/etc/my.cnf && tail -f /dev/null" ]
    ports:
      - containerPort: 3306
    volumeMounts:
    - name: tz-config
      mountPath: /etc/localtime
    - name: mysql-data
      mountPath: /data/mysql/data
    - name: mysql-config
      mountPath: /etc/my.cnf
      subPath: my.cnf
    env:
    - name: INNODB_BUFFER_POOL_SIZE
      value: 500M
  volumes:
    - name: tz-config
      hostPath:
        path: /etc/localtime
    - name: mysql-data
      hostPath:
        path: /data/mysql/data/
    - name: mysql-config
      configMap:
        name: mysql-mgr-2-cnf
        items:
          - key: mysql-mgr-2.cnf
            path: my.cnf







[root@master-1 mysql8]# ll
total 32
-rw-r--r-- 1 root root 1832 Nov  6 00:17 mysql-mgr-cnf-0.yaml
-rw-r--r-- 1 root root 1791 Nov  6 00:20 mysql-mgr-cnf-1.yaml
-rw-r--r-- 1 root root 1791 Nov  6 00:24 mysql-mgr-cnf-2.yaml
-rw-r--r-- 1 root root   56 Nov  6 00:16 namespace.yaml
-rw-r--r-- 1 root root  885 Nov  6 00:25 node1_pod.yaml
-rw-r--r-- 1 root root  880 Nov  6 00:25 node2_pod.yaml
-rw-r--r-- 1 root root  880 Nov  6 00:25 node3_pod.yaml
-rw-r--r-- 1 root root  180 Nov  6 00:16 service.yaml

kubectl delete -f components.yaml
kubectl apply -f node1_pod.yaml

kubectl create -f namespace.yaml;kubectl create -f service.yaml
kubectl create -f mysql-mgr-cnf-0.yaml 
kubectl create -f mysql-mgr-cnf-1.yaml 
kubectl create -f mysql-mgr-cnf-2.yaml 
kubectl create -f node1_pod.yaml
kubectl create -f node2_pod.yaml 
kubectl create -f node3_pod.yaml


kubectl delete -f service.yaml;kubectl delete -f namespace.yaml
kubectl delete -f mysql-mgr-cnf-0.yaml 
kubectl delete -f mysql-mgr-cnf-1.yaml 
kubectl delete -f mysql-mgr-cnf-2.yaml 
kubectl delete -f node1_pod.yaml
kubectl create -f node2_pod.yaml
kubectl create -f node3_pod.yaml


kubectl get pods -n mysqldb -o wide
kubectl get svc  -n mysqldb
kubectl get event -n mysqldb
kubectl describe pod mysql-mgr-0 -n mysqldb


while (true);do date ;sleep 1;echo "------------------------------------";done
#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

##(十二)、k8s部署 mysql8.0.27
[root@master-1 mysql8]# cat mysql-master-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
 name: mysql-master
 labels:
  name: mysql-master
spec:
  replicas: 1
  selector:
   name: mysql-master
  template:
   metadata:
    labels:
     name: mysql-master
   spec:
    containers:
    - name: mysql-master
      image: mysql 
      imagePullPolicy: IfNotPresent
      ports:
      - containerPort: 3306
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "master123456"
      - name: MYSQL_REPLICATION_USER
        value: "slave"
      - name: MYSQL_REPLICAITON_PASSWORD
        value: "123456"
[root@master-1 mysql8]# 

[root@master-1 mysql8]# cat mysql-slave-rc.yaml 
apiVersion: v1
kind: ReplicationController
metadata:
 name: mysql-slave
 labels:
  name: mysql-slave
spec:
  replicas: 1
  selector:
   name: mysql-slave
  template:
   metadata:
    labels:
     name: mysql-slave
   spec:
    containers:
    - name: mysql-slave
      image: mysql
      imagePullPolicy: IfNotPresent
      resources:
        requests:
          memory: 256Mi
          cpu: 100m
        limits:
          memory: 512Mi
          cpu: 200m
      ports:
      - containerPort: 3306
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "slave123456"
      - name: MYSQL_REPLICATION_USER
        value: "slave"
      - name: MYSQL_REPLICAITON_PASSWORD
        value: "123456"
[root@master-1 mysql8]# 




[root@master-1 mysql8]# cat mysql-svc.yaml 
apiVersion: v1
kind: Service
metadata:
  name: mysql-master
  labels:
   name: mysql-master
spec:
  type: NodePort
  ports:
  - port: 3306
    targetPort: 3306
    name: http
    nodePort: 30006
  selector:
    name: mysql-master
---

apiVersion: v1
kind: Service
metadata:
  name: mysql-slave
  labels:
   name: mysql-slave
spec:
  type: NodePort
  ports:
  - port: 3306
    targetPort: 3306
    name: http
    nodePort: 30066
  selector:
    name: mysql-slave
[root@master-1 mysql8]# 

master 主机外网连接端口 30006
slave 从机外网连接端口 30066
推荐使用 Navicat 测试连接

#####安装：
kubectl create -f mysql-master-rc.yaml
kubectl create -f mysql-slave-rc.yaml
kubectl create -f mysql-svc.yaml


[root@master-1 mysql8]# kubectl get po
NAME                 READY   STATUS    RESTARTS   AGE
mysql-master-zdc5z   1/1     Running   4          2d23h
mysql-slave-mqdjl    1/1     Running   0          4m41s


#####连接：
[root@master-1 mysql8]# kubectl exec -it mysql-slave-mqdjl -- bash
root@mysql-slave-mqdjl:/# mysql -u root -pslave123456
mysql: [Warning] Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 10
Server version: 8.0.27 MySQL Community Server - GPL

Copyright (c) 2000, 2021, Oracle and/or its affiliates.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> status;
--------------
mysql  Ver 8.0.27 for Linux on x86_64 (MySQL Community Server - GPL)

Connection id:          10
Current database:
Current user:           root@localhost
SSL:                    Not in use
Current pager:          stdout
Using outfile:          ''
Using delimiter:        ;
Server version:         8.0.27 MySQL Community Server - GPL
Protocol version:       10
Connection:             Localhost via UNIX socket
Server characterset:    utf8mb4
Db     characterset:    utf8mb4
Client characterset:    latin1
Conn.  characterset:    latin1
UNIX socket:            /var/run/mysqld/mysqld.sock
Binary data as:         Hexadecimal
Uptime:                 6 min 1 sec

Threads: 2  Questions: 8  Slow queries: 0  Opens: 117  Flush tables: 3  Open tables: 36  Queries per second avg: 0.022
--------------

mysql> 



######景象需要做主从配置，上面景象是默认的，未做主从配置。


#####手动扩容：
[root@master-1 mysql8]# kubectl scale rc mysql-slave --replicas=6
replicationcontroller/mysql-slave scaled

[root@master-1 mysql8]# kubectl scale rc mysql-master --replicas=3


[root@master-1 mysql8]# kubectl get po -o wide
NAME                 READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
mysql-master-6h7wh   1/1     Running   0          3m36s   10.244.0.39   master-1   <none>           <none>
mysql-master-zdc5z   1/1     Running   0          2d23h   10.244.1.44   node-1     <none>           <none>
mysql-slave-59gpm    1/1     Running   0          4m48s   10.244.2.30   node-2     <none>           <none>
mysql-slave-bbrb9    1/1     Running   0          4m19s   10.244.0.38   master-1   <none>           <none>
mysql-slave-fxszk    1/1     Running   0          4m19s   10.244.0.37   master-1   <none>           <none>
mysql-slave-mqdjl    1/1     Running   0          13m     10.244.0.36   master-1   <none>           <none>
mysql-slave-vvrwq    1/1     Running   0          8m22s   10.244.1.48   node-1     <none>           <none>
mysql-slave-wtwvk    1/1     Running   0          4m19s   10.244.1.49   node-1     <none>           <none>



#####自动伸缩扩容：
[root@master-1 mysql8]# kubectl autoscale rc mysql-slave --min=1 --max=10 --cpu-percent=50
horizontalpodautoscaler.autoscaling/mysql-slave autoscaled
[root@master-1 mysql8]# kubectl get hpa
NAME          REFERENCE                           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
mysql-slave   ReplicationController/mysql-slave   <unknown>/50%   1         10        0          8s

--min （容器数量下限）
--max （容器数量上限）
--cpu-percent （cpu使用率达到指定百分比）
容器 cpu 使用率上升至 50% 以上时，自动扩充容器数量
容器 cpu 使用率下降至 50% 以上时，自动缩减容器数量



kubectl delete -f 
kubectl create -f mysql-master-rc.yaml
kubectl create -f mysql-slave-rc-103.yaml
kubectl create -f mysql-slave-rc-105.yaml

kubectl get pods -o wide
kubectl get svc
kubectl get event
kubectl describe pod mysql-master-6h7wh

kubectl exec -it   -- bash

######节点镜像存储清理
因为k8s 节点的磁盘空间不大，当k8s 节点的镜像越来越多导致磁盘空间不够，所以我们要清理没有容器实验的docker镜像
提供一个命令用于清理当前节点上无用的、报错的、镜像和docker资源文件
docker system prune  命令可以用于清理磁盘，删除关闭的容器、无用的数据卷和网络，以及dangling镜像(即无tag的镜像)
docker system prune -a 命令清理得更加彻底，可以将没有容器使用Docker镜像都删掉。
 


[root@node-2 ~]# docker images
REPOSITORY                                                                TAG                 IMAGE ID            CREATED             SIZE
gcr.io/k8s-staging-metrics-server/metrics-server                          master              b46b63105b5a        2 weeks ago         65.3MB
quay.io/coreos/flannel                                                    v0.15.0             09b38f011a29        4 weeks ago         69.5MB
rancher/mirrored-flannelcni-flannel-cni-plugin                            v1.2                98660e6e4c3a        5 weeks ago         8.98MB
registry.aliyuncs.com/google_containers/kube-proxy                        v1.19.9             4a76fb49d490        8 months ago        118MB
quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader   v0.44.1             73b3005d83c4        11 months ago       13.4MB
quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-operator          v0.44.1             effbe93e08c1        11 months ago       42.6MB
grafana/grafana                                                           7.3.4               651ff2dc930f        11 months ago       187MB
k8s.gcr.io/metrics-server/metrics-server                                  v0.4.1              9759a41ccdf0        12 months ago       60.5MB
pingcap/tidb-backup-manager                                               v1.1.7              8d9ce547af25        12 months ago       805MB
pingcap/tidb-operator                                                     v1.1.7              021b4825bf26        12 months ago       168MB
directxman12/k8s-prometheus-adapter                                       v0.8.2              7f1062c43dbb        12 months ago       68.2MB
quay.mirrors.ustc.edu.cn/prometheus/prometheus                            v2.22.1             7cc97b58fb0e        12 months ago       168MB
quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy                           v0.8.0              ad393d6a4d1b        12 months ago       49MB
pingcap/tidb-monitor-initializer                                          v4.0.8              7f9825436e3b        12 months ago       3.33MB
pingcap/ticdc                                                             v4.0.8              56701d8a6fd7        12 months ago       114MB
pingcap/tidb-binlog                                                       v4.0.8              be89abc6ab8c        12 months ago       189MB
pingcap/tikv                                                              v4.0.8              d6d8f7141fc5        12 months ago       298MB
pingcap/pd                                                                v4.0.8              c5a198b7141d        12 months ago       138MB
pingcap/tidb                                                              v4.0.8              2e3719bf32d9        12 months ago       126MB
pingcap/tiflash                                                           v4.0.8              017447cd82ed        13 months ago       1.43GB
registry.aliyuncs.com/google_containers/coredns                           1.7.0               bfe3a36ebd25        17 months ago       45.2MB
quay.mirrors.ustc.edu.cn/prometheus/alertmanager                          v0.21.0             c876f5897d7b        17 months ago       55.5MB
quay.mirrors.ustc.edu.cn/prometheus/node-exporter                         v1.0.1              0e0218889c33        17 months ago       26.4MB
quay.mirrors.ustc.edu.cn/coreos/kube-state-metrics                        v1.9.7              6497f02dbdad        17 months ago       32.8MB
pingcap/advanced-statefulset                                              v0.3.3              ec5f3bf9faf8        18 months ago       88.7MB
prom/prometheus                                                           v2.18.1             de242295e225        18 months ago       140MB
quay.io/external_storage/local-volume-provisioner                         v2.3.4              878d033518e5        21 months ago       98.2MB
registry.aliyuncs.com/google_containers/pause                             3.2                 80d28bedfe5d        21 months ago       683kB
bluersw/metrics-server-amd64                                              v0.3.6              9dd718864ce6        2 years ago         39.9MB
k8s.gcr.io/metrics-server-amd64                                           v0.3.6              9dd718864ce6        2 years ago         39.9MB
pingcap/tidb-monitor-reloader                                             v1.0.1              4d7a712ea679        2 years ago         24.7MB
nginx/nginx-ingress                                                       1.5.5               1e674eebb1af        2 years ago         161MB
k8s.gcr.io/kube-scheduler                                                 v1.14.0             00638a24688b        2 years ago         81.6MB
grafana/grafana                                                           6.0.1               ffd9c905f698        2 years ago         241MB
busybox                                                                   1.26.2              c30178c5239f        4 years ago         1.11MB
[root@node-2 ~]# docker images|wc -l
36
[root@node-2 ~]# 


[root@node-2 ~]# docker image prune
WARNING! This will remove all dangling images.
Are you sure you want to continue? [y/N] y
Total reclaimed space: 0B


[root@node-2 ~]# docker container prune
WARNING! This will remove all stopped containers.
Are you sure you want to continue? [y/N] y
Deleted Containers:
bdf87b0fe75588da64e664e2dac539ce82e3897379f84e735c55febcdde3b3f0
da77ddddf469546da3c2adf4bd869cdaabbcef4d1f359772ed810aef2209b351
c489669c13704404d005c6d15bb81d23b47f890266c33d4a9a363e4ef390cb6d
defe6f89256a37f710a9b482f61312135fa875a817d25782c5c333d05b309a01
1767d8de9674681e17f9b64972bf819aa418179517a5268f4e5f35e4ae1c7703
9dfb824382aef22f23243960ad2223fc14d21de8457202eef052cae734b43b41
1c70a7b2f6b986d9f0a90fe4805fe19104acdda2f0a9571e0add3422e8db2249

Total reclaimed space: 2.45kB
[root@node-2 ~]# 

[root@node-2 ~]# docker volume prune
WARNING! This will remove all local volumes not used by at least one container.
Are you sure you want to continue? [y/N] y
Total reclaimed space: 0B


[root@node-2 ~]# docker system prune -a 
WARNING! This will remove:
  - all stopped containers
  - all networks not used by at least one container
  - all images without at least one container associated to them
  - all build cache

Are you sure you want to continue? [y/N] y
Deleted Containers:
4ff5f1f5213b78ab1871faab6ac715b245f417096550faefc44dd018fafa9b21
036244fd0d731a78e1667647cfa324dfcf9d50783a812f5114018efd060db749
5f5639db912ace5c01400ea562b69785f9f6e1354dedd2dc30cf97d22950152f

................一堆没用的images

Total reclaimed space: 4.85GB
[root@node-2 ~]# 




[root@node-2 ~]# docker ps -a
CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS                        PORTS               NAMES
b5fdbbd2e18d        quay.io/external_storage/local-volume-provisioner   "/local-provisioner"     19 seconds ago      Exited (255) 15 seconds ago                       k8s_provisioner_local-volume-provisioner-5twgt_kube-system_3411ca13-5759-4bbd-814e-cd4258c98ad7_0
f57e25b56448        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 14 minutes ago      Up 14 minutes                                     k8s_POD_local-volume-provisioner-5twgt_kube-system_3411ca13-5759-4bbd-814e-cd4258c98ad7_11
aca6e1292e10        09b38f011a29                                        "/opt/bin/flanneld -…"   14 minutes ago      Up 14 minutes                                     k8s_kube-flannel_kube-flannel-ds-nkl2p_kube-system_503118fc-6b72-4311-ac36-45ed6b311a20_11
72e91fdeff67        4a76fb49d490                                        "/usr/local/bin/kube…"   14 minutes ago      Up 14 minutes                                     k8s_kube-proxy_kube-proxy-fkj8w_kube-system_e64e81c6-b446-4414-ab1f-3b0214c1d200_11
992cda54a397        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 14 minutes ago      Up 14 minutes                                     k8s_POD_kube-proxy-fkj8w_kube-system_e64e81c6-b446-4414-ab1f-3b0214c1d200_11
bad89d91024e        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 14 minutes ago      Up 14 minutes                                     k8s_POD_kube-flannel-ds-nkl2p_kube-system_503118fc-6b72-4311-ac36-45ed6b311a20_11
[root@node-2 ~]# 
[root@node-2 ~]# 
[root@node-2 ~]# docker images
REPOSITORY                                           TAG                 IMAGE ID            CREATED             SIZE
quay.io/coreos/flannel                               v0.15.0             09b38f011a29        4 weeks ago         69.5MB
registry.aliyuncs.com/google_containers/kube-proxy   v1.19.9             4a76fb49d490        8 months ago        118MB
quay.io/external_storage/local-volume-provisioner    v2.3.4              878d033518e5        21 months ago       98.2MB
registry.aliyuncs.com/google_containers/pause        3.2                 80d28bedfe5d        21 months ago       683kB
[root@node-2 ~]# 



3台机器都执行，清理的很干净了，能节省出15g 空间。





##(十三)、k8s部署 Tidb集群
#####软件版本要求
软件名称	版本
Docker	Docker CE 18.09.6
Kubernetes	v1.12.5+
CentOS	CentOS 7.6，内核要求为 3.10.0-957 或之后版本
Helm	v3.0.0+

#####建议关闭防火墙：
systemctl stop firewalld
systemctl disable firewalld

如果无法关闭 firewalld 服务，为了保证 Kubernetes 正常运行，需要打开以下端口：
在 Master 节点上，打开以下端口，然后重新启动服务：
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --add-masquerade --permanent

当需要在 Master 节点上暴露 NodePort 时候设置
firewall-cmd --permanent --add-port=30000-32767/tcp
systemctl restart firewalld


#####在 Node 节点上，打开以下端口，然后重新启动服务：
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --add-masquerade --permanent
systemctl restart firewalld

#####配置 Iptables
FORWARD 链默认配置成 ACCEPT，并将其设置到开机启动脚本里：
iptables -P FORWARD ACCEPT

#####禁用 SELinux
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

#####关闭 Swap
Kubelet 正常工作需要关闭 Swap，并且把 /etc/fstab 里面有关 Swap 的那行注释掉：
swapoff -a
sed -i 's/^\(.*swap.*\)$/#\1/' /etc/fstab 


#####内核参数设置
按照下面的配置设置内核参数，也可根据自身环境进行微调：
modprobe br_netfilter

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.core.somaxconn = 32768
vm.swappiness = 0
net.ipv4.tcp_syncookies = 0
net.ipv4.ip_forward = 1
fs.file-max = 1000000
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 1024
net.ipv4.conf.all.rp_filter = 1
net.ipv4.neigh.default.gc_thresh1 = 80000
net.ipv4.neigh.default.gc_thresh2 = 90000
net.ipv4.neigh.default.gc_thresh3 = 100000
EOF

sysctl --system

#####配置 Irqbalance 服务
Irqbalance 服务可以将各个设备对应的中断号分别绑定到不同的 CPU 上，以防止所有中断请求都落在同一个 CPU 上而引发性能瓶颈。
systemctl enable irqbalance
systemctl start irqbalance

#####CPUfreq 调节器模式设置
为了让 CPU 发挥最大性能，请将 CPUfreq 调节器模式设置为 performance 模式。详细参考在部署目标机器上配置 CPUfreq 调节器模式。
cpupower frequency-set --governor performance

#####Ulimit 设置
TiDB 集群默认会使用很多文件描述符，需要将工作节点上面的 ulimit 设置为大于等于 1048576：
cat <<EOF >>  /etc/security/limits.conf
root        soft        nofile        1048576
root        hard        nofile        1048576
root        soft        stack         10240
EOF
sysctl --system

#####Docker 服务
安装 Docker 时，建议选择 Docker CE 18.09.6 及以上版本。请参考 Docker 安装指南 进行安装。
安装完 Docker 服务以后，执行以下步骤：
#####1、将 Docker 的数据保存到一块单独的盘上，Docker 的数据主要包括镜像和容器日志数据。通过设置 --data-root 参数来实现：
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "data-root": "/data1/docker"
}
EOF
上面会将 Docker 的数据目录设置为 /data1/docker




#####2、设置 Docker daemon 的 ulimit。

创建 docker service 的 systemd drop-in 目录 /etc/systemd/system/docker.service.d：

mkdir -p /etc/systemd/system/docker.service.d
创建 /etc/systemd/system/docker.service.d/limit-nofile.conf 文件，并配置 LimitNOFILE 参数的值，取值范围为大于等于 1048576 的数字即可。

cat > /etc/systemd/system/docker.service.d/limit-nofile.conf <<EOF
[Service]
LimitNOFILE=1048576
EOF
注意：

请勿将 LimitNOFILE 的值设置为 infinity。由于 systemd 的 bug，infinity 在 systemd 某些版本中指的是 65536。

重新加载配置：
systemctl daemon-reload && systemctl restart docker



#####3、Kubernetes 服务
参考 Kubernetes 官方文档，部署一套多 Master 节点高可用集群。

Kubernetes Master 节点的配置取决于 Kubernetes 集群中 Node 节点个数，节点数越多，需要的资源也就越多。节点数可根据需要做微调。

Kubernetes 集群 Node 节点个数	Kubernetes Master 节点配置
1-5	1vCPUs 4GB Memory
6-10	2vCPUs 8GB Memory
11-100	4vCPUs 16GB Memory
101-250	8vCPUs 32GB Memory
251-500	16vCPUs 64GB Memory
501-5000	32vCPUs 128GB Memory

安装完 Kubelet 之后，执行以下步骤：
将 Kubelet 的数据保存到一块单独盘上（可跟 Docker 共用一块盘），Kubelet 主要占盘的数据是 emptyDir 所使用的数据。通过设置 --root-dir 参数来实现：

echo "KUBELET_EXTRA_ARGS=--root-dir=/data1/kubelet" > /etc/sysconfig/kubelet
systemctl restart kubelet
上面会将 Kubelet 数据目录设置为 /data1/kubelet。

通过 kubelet 设置预留资源，保证机器上的系统进程以及 Kubernetes 的核心进程在工作负载很高的情况下仍然有足够的资源来运行，从而保证整个系统的稳定。

#####TiDB 集群资源需求
请根据服务器建议配置来规划机器的配置。
另外，在生产环境中，尽量不要在 Kubernetes Master 节点部署 TiDB 实例，或者尽可能少地部署 TiDB 实例。因为网卡带宽的限制，Master 节点网卡满负荷工作会影响到 Worker 节点和 Master 节点之间的心跳信息汇报，导致比较严重的问题。





####部署 TiDB Operator4.0

#####一、创建 TiDB Operator CRD
######1、下载 TiDB Cluster CRD 部署文件
wget https://raw.githubusercontent.com/pingcap/tidb-operator/v1.1.7/manifests/crd.yaml

######2、创建 TiDB Cluster CRD
[root@master-1 tidb]# kubectl apply -f crd.yaml
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/tidbclusters.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/backups.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/restores.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/backupschedules.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbmonitors.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbinitializers.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbclusterautoscalers.pingcap.com created
[root@master-1 tidb]# 

######3、检查 CRD 状态
[root@master-1 tidb]# kubectl get crd
NAME                                 CREATED AT
backups.pingcap.com                  2021-11-10T04:59:24Z
backupschedules.pingcap.com          2021-11-10T04:59:27Z
restores.pingcap.com                 2021-11-10T04:59:26Z
tidbclusterautoscalers.pingcap.com   2021-11-10T04:59:28Z
tidbclusters.pingcap.com             2021-11-10T04:59:22Z
tidbinitializers.pingcap.com         2021-11-10T04:59:28Z
tidbmonitors.pingcap.com             2021-11-10T04:59:27Z
[root@master-1 tidb]# 



#####二、部署 TiDB Operator
######1、下载 TiDB Operator 的 docker image
[root@master-1 ~]# docker pull pingcap/tidb-operator:v1.1.7
[root@master-1 ~]# docker pull pingcap/tidb-backup-manager:v1.1.7
[root@master-1 ~]# docker pull pingcap/advanced-statefulset:v0.3.3

mkdir -p /opt/soft/docker-image
docker save -o tidb-backup-manager.tar pingcap/tidb-backup-manager
docker save -o tidb-operator.tar pingcap/tidb-operator
docker save -o advanced-statefulset.tar pingcap/advanced-statefulset


[root@master-1 ~]# docker images|grep pingcap
pingcap/tidb-backup-manager                                               v1.1.7              8d9ce547af25        12 months ago       805MB
pingcap/tidb-operator                                                     v1.1.7              021b4825bf26        12 months ago       168MB
pingcap/advanced-statefulset                                              v0.3.3              ec5f3bf9faf8        18 months ago       88.7MB
[root@master-1 ~]# 

 



################################################
######其他教程参考：
1、导入需要的镜像（所有节点）
(1).联网环境镜像 pull 地址
docker pull pingcap/pd:v4.0.8
docker pull pingcap/tikv:v4.0.8
docker pull pingcap/tidb:v4.0.8
docker pull pingcap/tidb-binlog:v4.0.8
docker pull pingcap/ticdc:v4.0.8
docker pull pingcap/tiflash:v4.0.8
docker pull pingcap/tidb-monitor-reloader:v1.0.1
docker pull pingcap/tidb-monitor-initializer:v4.0.8
docker pull grafana/grafana:6.0.1
docker pull prom/prometheus:v2.18.1
docker pull busybox:1.26.2
docker pull quay.io/external_storage/local-volume-provisioner:v2.3.4
docker pull pingcap/tidb-operator:v1.1.7
docker pull pingcap/tidb-backup-manager:v1.1.7
docker pull bitnami/kubectl:latest
docker pull pingcap/advanced-statefulset:v0.3.3

(2).导出镜像
docker save -o local-volume-provisioner-v2.3.4.tar quay.io/external_storage/local-volume-provisioner:v2.3.4
docker save -o tidb-operator-v1.1.7.tar pingcap/tidb-operator:v1.1.7
docker save -o tidb-backup-manager-v1.1.7.tar pingcap/tidb-backup-manager:v1.1.7
docker save -o bitnami-kubectl.tar bitnami/kubectl:latest
docker save -o advanced-statefulset-v0.3.3.tar pingcap/advanced-statefulset:v0.3.3
docker save -o pd-v4.0.8.tar pingcap/pd:v4.0.8
docker save -o tikv-v4.0.8.tar pingcap/tikv:v4.0.8
docker save -o tidb-v4.0.8.tar pingcap/tidb:v4.0.8
docker save -o tidb-binlog-v4.0.8.tar pingcap/tidb-binlog:v4.0.8
docker save -o ticdc-v4.0.8.tar pingcap/ticdc:v4.0.8
docker save -o tiflash-v4.0.8.tar pingcap/tiflash:v4.0.8
docker save -o tidb-monitor-reloader-v1.0.1.tar pingcap/tidb-monitor-reloader:v1.0.1
docker save -o tidb-monitor-initializer-v4.0.8.tar pingcap/tidb-monitor-initializer:v4.0.8
docker save -o grafana-6.0.1.tar grafana/grafana:6.0.1
docker save -o prometheus-v2.18.1.tar prom/prometheus:v2.18.1
docker save -o busybox-1.26.2.tar busybox:1.26.2

(3).导入镜像
docker load -i advanced-statefulset-v0.3.3.tar
docker load -i bitnami-kubectl.tar
docker load -i busybox-1.26.2.tar
docker load -i grafana-6.0.1.tar
docker load -i kube-scheduler-v1.15.9.tar
docker load -i kube-scheduler-v1.16.9.tar
docker load -i local-volume-provisioner-v2.3.4.tar
docker load -i mysqlclient-latest.tar
docker load -i pd-v4.0.8.tar
docker load -i prometheus-v2.18.1.tar
docker load -i ticdc-v4.0.8.tar
docker load -i tidb-backup-manager-v1.1.7.tar
docker load -i tidb-binlog-v4.0.8.tar
docker load -i tidb-monitor-initializer-v4.0.8.tar
docker load -i tidb-monitor-reloader-v1.0.1.tar
docker load -i tidb-operator-v1.1.7.tar
docker load -i tidb-v4.0.8.tar
docker load -i tiflash-v4.0.8.tar
docker load -i tikv-v4.0.8.tar
docker load -i tiller-v2.16.7.tar


################################################

######2、创建 tidb-operator 部署文件

[root@master-1 tidb]# vim tidb-operator-deploy.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: tidb-scheduler-policy
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
data:
  policy.cfg: |-
    {
      "kind" : "Policy",
      "apiVersion" : "v1",
      "predicates": [
        {"name": "NoVolumeZoneConflict"},
        {"name": "MaxEBSVolumeCount"},
        {"name": "MaxAzureDiskVolumeCount"},
        {"name": "NoDiskConflict"},
        {"name": "GeneralPredicates"},
        {"name": "PodToleratesNodeTaints"},
        {"name": "CheckVolumeBinding"},
        {"name": "MaxGCEPDVolumeCount"},
        {"name": "MatchInterPodAffinity"},
        {"name": "CheckVolumeBinding"}
      ],
      "priorities": [
        {"name": "SelectorSpreadPriority", "weight": 1},
        {"name": "InterPodAffinityPriority", "weight": 1},
        {"name": "LeastRequestedPriority", "weight": 1},
        {"name": "BalancedResourceAllocation", "weight": 1},
        {"name": "NodePreferAvoidPodsPriority", "weight": 1},
        {"name": "NodeAffinityPriority", "weight": 1},
        {"name": "TaintTolerationPriority", "weight": 1}
      ],
      "extenders": [
        {
          "urlPrefix": "http://127.0.0.1:10262/scheduler",
          "filterVerb": "filter",
          "preemptVerb": "preempt",
          "weight": 1,
          "httpTimeout": 30000000000,
          "enableHttps": false
        }
      ]
    }


---


kind: ServiceAccount
apiVersion: v1
metadata:
  name: tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
rules:
- apiGroups: [""]
  resources:
  - services
  - events
  verbs: ["*"]
- apiGroups: [""]
  resources: ["endpoints","configmaps"]
  verbs: ["create", "get", "list", "watch", "update","delete"]
- apiGroups: [""]
  resources: ["serviceaccounts"]
  verbs: ["create","get","update","delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create", "update", "get", "list", "watch","delete"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "delete", "patch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch","update", "delete"]
- apiGroups: ["apps"]
  resources: ["statefulsets","deployments", "controllerrevisions"]
  verbs: ["*"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["*"]
- apiGroups: ["apps.pingcap.com"]
  resources: ["statefulsets", "statefulsets/status"]
  verbs: ["*"]
- apiGroups: ["pingcap.com"]
  resources: ["*"]
  verbs: ["*"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch", "patch","update"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]

- apiGroups: ["rbac.authorization.k8s.io"]
  resources: [clusterroles,roles]
  verbs: ["escalate","create","get","update", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings","clusterrolebindings"]
  verbs: ["create","get","update", "delete"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-controller-manager
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: tidb-operator:tidb-controller-manager
  apiGroup: rbac.authorization.k8s.io

---


kind: ServiceAccount
apiVersion: v1
metadata:
  name: tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
rules:

- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
- apiGroups: ["pingcap.com"]
  resources: ["tidbclusters"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "update"]

- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["delete", "get", "patch", "update"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  resourceNames: ["tidb-scheduler"]
  verbs: ["get", "update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: tidb-operator:tidb-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:kube-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:volume-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: system:volume-scheduler
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-operator
      app.kubernetes.io/instance: tidb-operator
      app.kubernetes.io/component: controller-manager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-operator
        app.kubernetes.io/instance: tidb-operator
        app.kubernetes.io/component: controller-manager
    spec:
      serviceAccount: tidb-controller-manager
      containers:
      - name: tidb-operator
        image: pingcap/tidb-operator:v1.1.7
        imagePullPolicy: IfNotPresent
        resources:
            requests:
              cpu: 80m
              memory: 50Mi

        command:
          - /usr/local/bin/tidb-controller-manager
          - -tidb-backup-manager-image=pingcap/tidb-backup-manager:v1.1.7
          - -tidb-discovery-image=pingcap/tidb-operator:v1.1.7
          - -cluster-scoped=true
          - -auto-failover=true
          - -pd-failover-period=5m
          - -tikv-failover-period=5m
          - -tiflash-failover-period=5m
          - -tidb-failover-period=5m
          - -v=2
        env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: TZ
            value: UTC


---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-operator
      app.kubernetes.io/instance: tidb-operator
      app.kubernetes.io/component: scheduler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-operator
        app.kubernetes.io/instance: tidb-operator
        app.kubernetes.io/component: scheduler
    spec:
      serviceAccount: tidb-scheduler
      containers:
      - name: tidb-scheduler
        image: pingcap/tidb-operator:v1.1.7
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi

        command:
          - /usr/local/bin/tidb-scheduler
          - -v=2
          - -port=10262
      - name: kube-scheduler
        image: k8s.gcr.io/kube-scheduler:v1.14.0
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi

        command:
        - kube-scheduler
        - --port=10261
        - --leader-elect=true
        - --lock-object-name=tidb-scheduler
        - --lock-object-namespace=tidb-admin
        - --scheduler-name=tidb-scheduler
        - --v=2
        - --policy-configmap=tidb-scheduler-policy
        - --policy-configmap-namespace=tidb-admin



######3、创建 tidb-operator
create tidb-admin namespace
[root@master-1 tidb]# kubectl create namespace tidb-admin
namespace/tidb-admin created

create tidb-operator 
[root@master-1 tidb]# kubectl apply -f tidb-operator-deploy.yaml -n tidb-admin
configmap/tidb-scheduler-policy created
serviceaccount/tidb-controller-manager created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/tidb-operator:tidb-controller-manager created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:tidb-controller-manager created
serviceaccount/tidb-scheduler created
clusterrole.rbac.authorization.k8s.io/tidb-operator:tidb-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:tidb-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:kube-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:volume-scheduler created
deployment.apps/tidb-controller-manager created
deployment.apps/tidb-scheduler created
[root@master-1 tidb]# 


######4、检查 TiDB-Operator 状态
[root@master-1 tidb]# kubectl get pods -n tidb-admin --watch
NAME                                       READY   STATUS              RESTARTS   AGE
tidb-controller-manager-76796956f8-4flwj   1/1     Running             0          9s
tidb-scheduler-6d68c54f5f-nz6t2            0/2     ContainerCreating   0          9s

[root@master-1 tidb]# kubectl get pods -n tidb-admin --watch
NAME                                       READY   STATUS    RESTARTS   AGE
tidb-controller-manager-76796956f8-4flwj   1/1     Running   0          27s
tidb-scheduler-6d68c54f5f-nz6t2            2/2     Running   0          27s



调试命令
kubectl delete -f 
kubectl create -f tidb-operator-deploy.yaml
kubectl create -f tidb-operator-deploy.yaml

kubectl get pods -o wide
kubectl get svc
kubectl get event
kubectl describe pod mysql-master-6h7wh

kubectl exec -it   -- bash

############################################################常用语句：

[root@master-1 tidb]# for i in `seq 9`; do mkdir -p /mnt/disks/pv0$i; done
[root@master-1 tidb]# ll /mnt/disks/
total 0
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv01
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv02
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv03
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv04
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv05
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv06
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv07
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv08
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv09


用以下命令为 TiDB Cluster 创建 9 个 PV。创建 多少个 PV 个你的需求有关，但至少创建 9 个吧，防止不够用。
delete apply
[root@master-1 tidb]#for i in `seq 9`; do
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: tidb-cluster-master-pv0${i}
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/pv0${i}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - 172.16.201.134
EOF
done


delete apply
[root@master-1 ~]# for i in `seq 9`; do
cat <<EOF | kubectl delete -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tidb-pvc-pv0${i}
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage
EOF
done
############################################################常用语句：
#####三、为 TiDB Cluster 创建 PV
######1、准备创建持久化存储 local-volume-provisioner.yaml

[root@master-1 tidb]# vim local-volume-provisioner.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: "local-storage"
provisioner: "kubernetes.io/no-provisioner"
volumeBindingMode: "WaitForFirstConsumer"
 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: kube-system
data:
  setPVOwnerRef: "true"
  nodeLabelsForPV: |
    - kubernetes.io/hostname
  storageClassMap: |
    local-storage:
      hostDir: /mnt/disks
      mountDir: /mnt/disks
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: kube-system
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.3.4"
          name: provisioner
          securityContext:
            privileged: true
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: JOB_CONTAINER_IMAGE
            value: "quay.io/external_storage/local-volume-provisioner:v2.3.4"
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath: /mnt/disks
              name: local-disks
              mountPropagation: "HostToContainer"
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: local-disks
          hostPath:
            path: /mnt/disks
 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
  namespace: kube-system
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io

挂载磁盘，其Provisioner本身其并不提供local volume，但它在各个节点上的provisioner会去动态的“发现”挂载点（discovery directory），当某node的provisioner在/mnt/disks目录下发现有挂载点时，会创建PV，该PV的local.path就是挂载点，并设置nodeAffinity为该node。lvp.sh


########!/bin/bash
for i in $(seq 1 10); do
  mkdir -p /mnt/disks-bind/vol${i}
  mkdir -p /mnt/disks/vol${i}
  mount --bind /mnt/disks-bind/vol${i} /mnt/disks/vol${i}
done

######2、创建 静态PVC

[root@master-1 tidb]# ./lvp.sh 
[root@master-1 tidb]# ls /mnt/*
/mnt/disks:
vol1  vol2  vol3  vol4  vol5

/mnt/disks-bind:
vol1  vol2  vol3  vol4  vol5
[root@master-1 tidb]# 



########拉镜像
docker pull quay.io/external_storage/local-volume-provisioner:v2.3.4
docker save -o local-volume-provisioner-v2.3.4.tar quay.io/external_storage/local-volume-provisioner:v2.3.4
docker load -i local-volume-provisioner-v2.3.4.tar

########执行：
[root@master-1 tidb]# kubectl apply -f local-volume-provisioner.yaml
 
 
########查看：
[root@master-1 tidb]# kubectl get po -n kube-system -l app=local-volume-provisioner && kubectl get pv | grep local-storage
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          2m8s
local-volume-provisioner-f5l6c   1/1     Running   0          2m8s
local-volume-provisioner-ldjvd   1/1     Running   0          2m8s
local-pv-3208a670   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-52f01d7c   49Gi       RWO            Delete           Available           local-storage            2m3s
local-pv-8090c14a   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-836fbe4d   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-aff38cdf   49Gi       RWO            Delete           Available           local-storage            2m2s
 

#####四、 部署并测试 TiDB Cluster

#####1、部署 TiDB Cluster
下载 TiDB cluster 的 docker images
docker pull pingcap/pd:v4.0.8
docker pull pingcap/tikv:v4.0.8
docker pull pingcap/tidb:v4.0.8
docker pull pingcap/tidb-binlog:v4.0.8
docker pull pingcap/ticdc:v4.0.8
docker pull pingcap/tiflash:v4.0.8
docker pull pingcap/tidb-monitor-reloader:v1.0.1
docker pull pingcap/tidb-monitor-initializer:v4.0.8
docker pull grafana/grafana:6.0.1
docker pull prom/prometheus:v2.18.1
docker pull busybox:1.26.2

[root@master-1 tidb]# docker pull pingcap/pd:v4.0.8;docker pull pingcap/tikv:v4.0.8;docker pull pingcap/tidb:v4.0.8;docker pull pingcap/tidb-binlog:v4.0.8;docker pull pingcap/ticdc:v4.0.8;docker pull pingcap/tiflash:v4.0.8;docker pull pingcap/tidb-monitor-reloader:v1.0.1;docker pull pingcap/tidb-monitor-initializer:v4.0.8;docker pull grafana/grafana:6.0.1;docker pull prom/prometheus:v2.18.1;docker pull busybox:1.26.2


docker save -o pd-v4.0.8.tar pingcap/pd:v4.0.8
docker save -o tikv-v4.0.8.tar pingcap/tikv:v4.0.8
docker save -o tidb-v4.0.8.tar pingcap/tidb:v4.0.8
docker save -o tidb-binlog-v4.0.8.tar pingcap/tidb-binlog:v4.0.8
docker save -o ticdc-v4.0.8.tar pingcap/ticdc:v4.0.8
docker save -o tiflash-v4.0.8.tar pingcap/tiflash:v4.0.8
docker save -o tidb-monitor-reloader-v1.0.1.tar pingcap/tidb-monitor-reloader:v1.0.1
docker save -o tidb-monitor-initializer-v4.0.8.tar pingcap/tidb-monitor-initializer:v4.0.8
docker save -o grafana-6.0.1.tar grafana/grafana:6.0.1
docker save -o prometheus-v2.18.1.tar prom/prometheus:v2.18.1
docker save -o busybox-1.26.2.tar busybox:1.26.2


docker load -i pd-v4.0.8.tar
docker load -i tikv-v4.0.8.tar
docker load -i tidb-v4.0.8.tar
docker load -i tidb-binlog-v4.0.8.tar
docker load -i ticdc-v4.0.8.tar
docker load -i tiflash-v4.0.8.tar
docker load -i tidb-monitor-reloader-v1.0.1.tar
docker load -i tidb-monitor-initializer-v4.0.8.tar
docker load -i grafana-6.0.1.tar
docker load -i prometheus-v2.18.1.tar
docker load -i busybox-1.26.2.tar


#####2、下载 TiDB 的部署 yaml 文件

Download TiDB deployment yaml file 
[root@master-1 tidb]# wget https://github.com/pingcap/tidb-operator/blob/v1.1.7/examples/advanced/tidb-cluster.yaml

部署 TiDB 集群（master 节点）[我这里是在测试搭建环境 只有一个k8s-master和k8s-node 所以副本是2，生产应该是>=3的奇数]
TiDB deployment yaml
[root@master-1 tidb]# vim tidb-cluster.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: mycluster
  namespace: mycluster
 
spec:
  version: "v4.0.8"
  timezone: UTC
  configUpdateStrategy: RollingUpdate
  hostNetwork: false
  imagePullPolicy: IfNotPresent
  helper:
    image: busybox:1.26.2
  enableDynamicConfiguration: true
 
  pd:
    enableDashboardInternalProxy: true
    baseImage: pingcap/pd
    config: {}
    replicas: 3
    requests:
      cpu: "100m"
      storage: 1Gi
    mountClusterClientSecret: false
    storageClassName: "local-storage"
 
  tidb:
    baseImage: pingcap/tidb
    replicas: 3
    requests:
      cpu: "100m"
    config: {}
    service:
      type: NodePort
      externalTrafficPolicy: Cluster
      mysqlNodePort: 30011
      statusNodePort: 30012
 
  tikv:
    baseImage: pingcap/tikv
    config: {}
    replicas: 2
    requests:
      cpu: "100m"
      storage: 1Gi
    mountClusterClientSecret: false
    storageClassName: "local-storage"
 
  tiflash:
    baseImage: pingcap/tiflash
    maxFailoverCount: 1
    replicas: 1
    storageClaims:
    - resources:
        requests:
          storage: 1Gi
      storageClassName: local-storage
 
  pump:
    baseImage: pingcap/tidb-binlog
    replicas: 1
    storageClassName: local-storage
    requests:
      storage: 1Gi
    schedulerName: default-scheduler
    config:
      addr: 0.0.0.0:8250
      gc: 7
      heartbeat-interval: 2
 
  ticdc:
    baseImage: pingcap/ticdc
    replicas: 2
    config:
      logLevel: info
 
  enablePVReclaim: false
  pvReclaimPolicy: Retain
  tlsCluster: {}
[root@master-1 tidb]# 

#####3、创建 TiDB Cluster
[root@master-1 tidb]# kubectl create namespace mycluster

[root@master-1 tidb]# kubectl apply -f tidb-cluster.yaml -n mycluster

#####4、检查 TiDB cluster 状态
[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          2m36s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          2m35s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          2m35s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          2m35s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          2m21s   10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          105s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          104s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          103s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          105s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          105s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          104s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          2m22s   10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          2m22s   10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m    10.244.2.58   node-2     <none>           <none>
[root@master-1 tidb]# 



[root@master-1 tidb]# kubectl get TidbCluster -o wide  -n mycluster       
NAME        READY   PD                  STORAGE   READY   DESIRE   TIKV                  STORAGE   READY   DESIRE   TIDB                  READY   DESIRE   STATUS                        AGE
mycluster   False   pingcap/pd:v4.0.8   1Gi       2       2        pingcap/tikv:v4.0.8   1Gi       2       2        pingcap/tidb:v4.0.8   2       2        TiFlash store(s) are not up   13m
[root@master-1 tidb]# 


[root@master-1 tidb]# kubectl get pod -n kube-system -l app=local-volume-provisioner 
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          18m
local-volume-provisioner-f5l6c   1/1     Running   0          18m
local-volume-provisioner-ldjvd   1/1     Running   0          18m
[root@master-1 tidb]# 



[root@master-1 tidb]# kubectl get po -n kube-system -l app=local-volume-provisioner && kubectl get pv | grep local-storage
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          6h19m
local-volume-provisioner-f5l6c   1/1     Running   0          6h19m
local-volume-provisioner-ldjvd   1/1     Running   0          6h19m
local-pv-26438be0   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-3208a670   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-1           local-storage            6h19m
local-pv-4ddc9629   49Gi       RWO            Retain           Bound       mycluster/data0-mycluster-tiflash-0   local-storage            6h8m
local-pv-52f01d7c   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-0           local-storage            6h19m
local-pv-5ebc1c66   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-660f09b0   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-2       local-storage            6h8m
local-pv-6be69e27   49Gi       RWO            Delete           Available                                         local-storage            6h8m
local-pv-8090c14a   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-0       local-storage            6h19m
local-pv-836fbe4d   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-1       local-storage            6h19m
local-pv-83bf5cf6   49Gi       RWO            Delete           Available                                         local-storage            6h8m
local-pv-95d1a179   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-3       local-storage            6h7m
local-pv-aff38cdf   49Gi       RWO            Retain           Bound       mycluster/data-mycluster-pump-0       local-storage            6h19m
local-pv-ced17877   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-e3f48c6a   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-2           local-storage            6h8m
local-pv-eec6f63d   49Gi       RWO            Delete           Available                                         local-storage            6h7m

############调试命令
kubectl delete -f 
kubectl apply -f tidb-cluster.yaml -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster

kubectl get pods -n mycluster -o wide
kubectl get pv,pvc -n mycluster -o wide
kubectl get svc  -n mycluster
kubectl get event  -n mycluster
kubectl describe pod  mycluster-pd-0 -n mycluster
kubectl exec -it   -- bash
kubectl patch pv pd-detailed-tidb-pd-0  -p '{"metadata":{"finalizers":null}}' -n mycluster                 
kubectl delete pvc pd-mycluster-pd-0 -n mycluster
kubectl delete pv tidb-cluster-node-1-pv05


kubectl describe pod   mycluster-tiflash-0 -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster
############调试命令





[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          108s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          107s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          107s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          107s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          93s    10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          57s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          56s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          55s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          57s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          57s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          56s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          94s    10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          94s    10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m   10.244.2.58   node-2     <none>           <none>
[root@master-1 tidb]# 

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
service/mycluster-discovery      ClusterIP   10.1.158.207   <none>        10261/TCP,10262/TCP              28m
service/mycluster-pd             ClusterIP   10.1.178.77    <none>        2379/TCP                         28m
service/mycluster-pd-peer        ClusterIP   None           <none>        2380/TCP                         28m
service/mycluster-pump           ClusterIP   None           <none>        8250/TCP                         28m
service/mycluster-ticdc-peer     ClusterIP   None           <none>        8301/TCP                         27m
service/mycluster-tidb           NodePort    10.1.226.247   <none>        4000:30011/TCP,10080:30012/TCP   27m
service/mycluster-tidb-peer      ClusterIP   None           <none>        10080/TCP                        27m
service/mycluster-tiflash-peer   ClusterIP   None           <none>        3930/TCP,20170/TCP               27m
service/mycluster-tikv-peer      ClusterIP   None           <none>        20160/TCP                        28m

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mycluster-discovery   1/1     1            1           28m

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/mycluster-discovery-66899c8ff6   1         1         1       28m

NAME                                 READY   AGE
statefulset.apps/mycluster-pd        2/2     28m
statefulset.apps/mycluster-pump      1/1     28m
statefulset.apps/mycluster-ticdc     2/2     27m
statefulset.apps/mycluster-tidb      2/2     27m
statefulset.apps/mycluster-tiflash   0/1     27m
statefulset.apps/mycluster-tikv      2/2     28m
[root@master-1 tidb]# 



#####五、 测试 TiDB Cluster

######1、测试链接：默认无密码：
[root@node-1 mnt]# mysql -uroot -p -h172.16.201.134 -P30011
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 379
Server version: 5.7.25-TiDB-v4.0.8 TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| INFORMATION_SCHEMA |
| METRICS_SCHEMA     |
| PERFORMANCE_SCHEMA |
| mysql              |
| test               |
+--------------------+
5 rows in set (0.02 sec)

MySQL [(none)]> 

######2、查询版本号：
MySQL [(none)]> select tidb_version();
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tidb_version()                                                                                                                                                                                                                                                                                                     |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Release Version: v4.0.8
Edition: Community
Git Commit Hash: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
Git Branch: heads/refs/tags/v4.0.8
UTC Build Time: 2020-10-30 08:21:16
GoVersion: go1.13
Race Enabled: false
TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306
Check Table Before Drop: false |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.03 sec)


######3、查询集群存储状态：
MySQL [(none)]> select * from INFORMATION_SCHEMA.tikv_store_status\G;
*************************** 1. row ***************************
         STORE_ID: 4
          ADDRESS: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 36.5GiB
     LEADER_COUNT: 10
    LEADER_WEIGHT: 1
     LEADER_SCORE: 10
      LEADER_SIZE: 10
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:05:52
LAST_HEARTBEAT_TS: 2021-11-10 18:12:04
           UPTIME: 6m12.048304662s
*************************** 2. row ***************************
         STORE_ID: 1001
          ADDRESS: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Down
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 0B
        AVAILABLE: 0B
     LEADER_COUNT: 0
    LEADER_WEIGHT: 1
     LEADER_SCORE: 0
      LEADER_SIZE: 0
     REGION_COUNT: 0
    REGION_WEIGHT: 1
     REGION_SCORE: 0
      REGION_SIZE: 0
         START_TS: 2021-11-10 18:09:39
LAST_HEARTBEAT_TS: 1970-01-01 00:00:00
           UPTIME: 
*************************** 3. row ***************************
         STORE_ID: 1002
          ADDRESS: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 40.35GiB
     LEADER_COUNT: 1
    LEADER_WEIGHT: 1
     LEADER_SCORE: 1
      LEADER_SIZE: 1
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:11:43
LAST_HEARTBEAT_TS: 2021-11-10 18:12:03
           UPTIME: 20.538416648s
*************************** 4. row ***************************
         STORE_ID: 1
          ADDRESS: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 36.5GiB
     LEADER_COUNT: 10
    LEADER_WEIGHT: 1
     LEADER_SCORE: 10
      LEADER_SIZE: 10
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:05:52
LAST_HEARTBEAT_TS: 2021-11-10 18:12:03
           UPTIME: 6m11.092363558s
4 rows in set (0.01 sec)

ERROR: No query specified

MySQL [(none)]> 

######4、查询tidb集群基本信息：
MySQL [(none)]> select TYPE,INSTANCE,STATUS_ADDRESS,VERSION,START_TIME from INFORMATION_SCHEMA.cluster_info;
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
| TYPE | INSTANCE                                                 | STATUS_ADDRESS                                           | VERSION | START_TIME           |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
| tidb | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:49Z |
| tidb | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:48Z |
| tidb | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:49Z |
| pd   | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:37Z |
| pd   | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:37Z |
| pd   | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:09:39Z |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
9 rows in set (0.20 sec)

MySQL [(none)]> select * from INFORMATION_SCHEMA.cluster_info;                                              
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
| TYPE | INSTANCE                                                 | STATUS_ADDRESS                                           | VERSION | GIT_HASH                                 | START_TIME           | UPTIME          |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
| tidb | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:48Z | 4m47.468737723s |
| tidb | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:49Z | 4m46.468746566s |
| tidb | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:49Z | 4m46.468757835s |
| pd   | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:37Z | 5m58.468760058s |
| pd   | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:37Z | 5m58.468762034s |
| pd   | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:52Z | 5m43.468763504s |
| tikv | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:09:39Z | 1m56.468765234s |
| tikv | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:05:52Z | 5m43.46876678s  |
| tikv | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:05:52Z | 5m43.468768286s |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
9 rows in set (0.03 sec)



MySQL [(none)]> 
MySQL [(none)]> select * from INFORMATION_SCHEMA.cluster_info\G;
*************************** 1. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:49Z
        UPTIME: 6m1.268138556s
*************************** 2. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:48Z
        UPTIME: 6m2.268150039s
*************************** 3. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:49Z
        UPTIME: 6m1.268151855s
*************************** 4. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:37Z
        UPTIME: 7m13.268153527s
*************************** 5. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:37Z
        UPTIME: 7m13.268155133s
*************************** 6. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268156933s
*************************** 7. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:11:43Z
        UPTIME: 1m7.268158437s
*************************** 8. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268160807s
*************************** 9. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268163087s
*************************** 10. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:09:39Z
        UPTIME: 3m11.268165802s
10 rows in set (10.10 sec)

ERROR: No query specified
MySQL [(none)]> 


MySQL [(none)]> create database pingcap;
Query OK, 0 rows affected (0.59 sec)

MySQL [(none)]> use pingcap;
Database changed
MySQL [pingcap]> CREATE TABLE `tab_tidb` (
    -> `id` int(11) NOT NULL AUTO_INCREMENT,
    -> `name` varchar(20) NOT NULL DEFAULT '',
    -> `age` int(11) NOT NULL DEFAULT 0,
    -> `version` varchar(20) NOT NULL DEFAULT '',
    -> PRIMARY KEY (`id`),
    -> KEY `idx_age` (`age`));
Query OK, 0 rows affected (0.28 sec)

MySQL [pingcap]> insert into `tab_tidb` values (1,'TiDB',5,'TiDB-v5.0.0');
Query OK, 1 row affected (0.08 sec)

MySQL [pingcap]> select * from tab_tidb;
+----+------+-----+-------------+
| id | name | age | version     |
+----+------+-----+-------------+
|  1 | TiDB |   5 | TiDB-v5.0.0 |
+----+------+-----+-------------+
1 row in set (0.04 sec)

MySQL [pingcap]> 


查看 TiKV store 状态、store_id、存储情况以及启动时间
MySQL [pingcap]> select STORE_ID,ADDRESS,STORE_STATE,STORE_STATE_NAME,CAPACITY,AVAILABLE,UPTIME from INFORMATION_SCHEMA.TIKV_STORE_STATUS;
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
| STORE_ID | ADDRESS                                                  | STORE_STATE | STORE_STATE_NAME | CAPACITY | AVAILABLE | UPTIME           |
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
|     1002 | mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 40.35GiB  | 4m30.639851204s  |
|        1 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m21.228623636s |
|        4 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m22.188095387s |
|     1001 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m20.223623636s |
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
4 rows in set (0.13 sec)

MySQL [pingcap]> 


######5、初始化 TiDB 集群设置密码（master 节点）
创建 Secret 指定 root 账号密码
[root@master-1 tidb]# kubectl create secret generic tidb-secret --from-literal=root=123456 --namespace=mycluster

创建 Secret 指定 root 账号密码 自动创建其它用户
[root@master-1 tidb]# kubectl create secret generic tidb-secret --from-literal=root=123456 --from-literal=developer=123456 --namespace=mycluster
[root@master-1 tidb]#  cat ./tidb-initializer.yaml
---
apiVersion: pingcap.com/v1alpha1
kind: TidbInitializer
metadata:
  name: mycluster
  namespace: mycluster
spec:
  image: tnir/mysqlclient
  cluster:
    namespace: tidb
    name: tidb
  initSql: |-
    create database app;
  passwordSecret: tidb-secret

 
[root@master-1 tidb]# kubectl apply -f ./tidb-initializer.yaml --namespace=mycluster


######查看 TiDB 集群信息命令
kubectl get pods -n mycluster -o wide
kubectl get all -n mycluster 
kubectl get svc -n mycluster 

####### TiDB 集群配置命令
kubectl edit tc -n mycluster

#####六、 部署TiDB Cluster监控
######1、部署TidbMonitor
[root@master-1 tidb]# vim tidb-monitor.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbMonitor
metadata:
  name: mycluster
spec:
  clusters:
  - name: mycluster
  prometheus:
    baseImage: prom/prometheus
    version: v2.18.1
  grafana:
    baseImage: grafana/grafana
    version: 6.0.1
  initializer:
    baseImage: pingcap/tidb-monitor-initializer
    version: v4.0.8
  reloader:
    baseImage: pingcap/tidb-monitor-reloader
    version: v1.0.1
  imagePullPolicy: IfNotPresent
 
[root@master-1 tidb]# kubectl -n mycluster apply -f tidb-monitor.yaml
tidbinitializer.pingcap.com/mycluster configured

[root@master-1 tidb]# kubectl get TidbMonitor -n mycluster -o wide       
NAME               AGE
myclustertidbmon   4m39s


[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          108s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          107s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          107s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          107s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          93s    10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          57s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          56s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          55s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          57s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          57s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          56s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          94s    10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          94s    10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m   10.244.2.58   node-2     <none>           <none>


######2、部署ingress
借用ingress来访问，创建 tidb-ingress.yaml 文件如下：【 kubectl -n tidb apply -f ./tidb-ingress.yaml  然后配置域名】

[root@master-1 tidb]#vim tidb-ingress.yaml
apiVersion: extensions/v1beta1 
kind: Ingress 
metadata:
  name: myclusterIngress
  namespace: mycluster
spec:
  rules:
    - host: k8s.grafana.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-grafana
              servicePort: 3000
    - host: k8s.tidb.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-pd
              servicePort: 2379
    - host: k8s.prometheus.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-prometheus
              servicePort: 9090

[root@master-1 tidb]# kubectl -n mycluster apply -f tidb-ingress.yaml  
ingress.extensions/myclusteringress created

[root@master-1 tidb]# kubectl get Ingress -n mycluster -o wide    
NAME               CLASS    HOSTS                                             ADDRESS   PORTS   AGE
myclusteringress   <none>   k8s.grafana.com,k8s.tidb.com,k8s.prometheus.com             80      29s


[root@master-1 tidb]# kubectl get svc -n mycluster 
NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
mycluster-discovery                 ClusterIP   10.1.158.207   <none>        10261/TCP,10262/TCP              3h23m
mycluster-pd                        ClusterIP   10.1.178.77    <none>        2379/TCP                         3h23m
mycluster-pd-peer                   ClusterIP   None           <none>        2380/TCP                         3h23m
mycluster-pump                      ClusterIP   None           <none>        8250/TCP                         3h22m
mycluster-ticdc-peer                ClusterIP   None           <none>        8301/TCP                         3h22m
mycluster-tidb                      NodePort    10.1.226.247   <none>        4000:30011/TCP,10080:30012/TCP   3h22m
mycluster-tidb-peer                 ClusterIP   None           <none>        10080/TCP                        3h22m
mycluster-tiflash-peer              ClusterIP   None           <none>        3930/TCP,20170/TCP               3h22m
mycluster-tikv-peer                 ClusterIP   None           <none>        20160/TCP                        3h22m
myclustertidbmon-grafana            ClusterIP   10.1.228.93    <none>        3000/TCP                         5m54s
myclustertidbmon-monitor-reloader   ClusterIP   10.1.251.244   <none>        9089/TCP                         5m55s
myclustertidbmon-prometheus         ClusterIP   10.1.120.77    <none>        9090/TCP                         5m55s




######3、配置域名
lex@lexliudeMacBook-Pro Downloads %cat /etc/hosts
172.16.201.134 k8s.grafana.com
172.16.201.134 k8s.tidb.com
172.16.201.134 k8s.prometheus.com

访问即可：
访问 http://k8s.tidb.com/dashboard 用户名root 密码 空
访问 http://k8s.grafana.com/login  用户名和密码 都是admin
访问 http://k8s.prometheus.com/

以上ingress的svc 要根据自己的实际配置


######4，日志查看
[root@master-1 tidb]# kubectl logs myclustertidbmon-monitor-6cdfdcf447-vzs2t -n mycluster prometheus
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:302 msg="No time or size retention was set so using the default time retention" duration=15d
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:337 msg="Starting Prometheus" version="(version=2.18.1, branch=HEAD, revision=ecee9c8abfd118f139014cb1b174b08db3f342cf)"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:338 build_context="(go=go1.14.2, user=root@2117a9e64a7e, date=20200507-16:51:47)"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:339 host_details="(Linux 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 myclustertidbmon-monitor-6cdfdcf447-vzs2t (none))"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:340 fd_limits="(soft=1048576, hard=1048576)"
level=info ts=2021-11-10T15:29:05.395Z caller=main.go:341 vm_limits="(soft=unlimited, hard=unlimited)"
level=info ts=2021-11-10T15:29:05.420Z caller=main.go:678 msg="Starting TSDB ..."
level=info ts=2021-11-10T15:29:05.447Z caller=web.go:523 component=web msg="Start listening for connections" address=0.0.0.0:9090
level=info ts=2021-11-10T15:29:05.457Z caller=head.go:575 component=tsdb msg="Replaying WAL, this may take awhile"
level=info ts=2021-11-10T15:29:05.466Z caller=head.go:624 component=tsdb msg="WAL segment loaded" segment=0 maxSegment=0
level=info ts=2021-11-10T15:29:05.466Z caller=head.go:627 component=tsdb msg="WAL replay completed" duration=8.860784ms
level=info ts=2021-11-10T15:29:05.468Z caller=main.go:694 fs_type=XFS_SUPER_MAGIC
level=info ts=2021-11-10T15:29:05.473Z caller=main.go:695 msg="TSDB started"
level=info ts=2021-11-10T15:29:05.473Z caller=main.go:799 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2021-11-10T15:29:05.506Z caller=kubernetes.go:253 component="discovery manager scrape" discovery=k8s msg="Using pod service account via in-cluster config"
level=info ts=2021-11-10T15:29:05.880Z caller=main.go:827 msg="Completed loading of configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2021-11-10T15:29:05.880Z caller=main.go:646 msg="Server is ready to receive web requests."
[root@master-1 tidb]# 


[root@master-1 tidb]# kubectl logs myclustertidbmon-monitor-6cdfdcf447-vzs2t -n mycluster reloader  
2021/11/10 15:29:05 need to store latest file to store path
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
 - using env:   export GIN_MODE=release
 - using code:  gin.SetMode(gin.ReleaseMode)

[GIN-debug] GET    /monitoring/rules         --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).ListRules-fm (3 handlers)
[GIN-debug] GET    /monitoring/configs       --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).ListConfigs-fm (3 handlers)
[GIN-debug] GET    /monitoring/configs/:config --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).GetConfig-fm (3 handlers)
[GIN-debug] PUT    /monitoring/configs/:config --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).UpdateConfig-fm (3 handlers)
[GIN-debug] Listening and serving HTTP on 0.0.0.0:9089
[root@master-1 tidb]# 

######5，日志查看暂时开放端口：
[root@master-1 tidb]# kubectl port-forward mycluster-discovery-66899c8ff6-59qxx  -n mycluster 10262:10262                   
Forwarding from 127.0.0.1:10262 -> 10262




##(十四)、k8s部署Gitlab Harbor Jenkins

172.16.201.134	master1、Harbor、Jenkins
172.16.201.135	node1
172.16.201.136	node2、Gitlab


###一、下载harbor二进制文件
下载harbor二进制文件：https://github.com/goharbor/harbor/releases

###二、安装 docker compose
[root@master-1 ~]# curl -L https://github.com/docker/compose/releases/download/1.22.0/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   633  100   633    0     0    176      0  0:00:03  0:00:03 --:--:--   176
100 11.2M  100 11.2M    0     0   621k      0  0:00:18  0:00:18 --:--:-- 1169k

然后把下载的docker-compose 设置可执行权限
[root@master-1 ~]# chmod +x /usr/local/bin/docker-compose
[root@master-1 ~]# docker-compose version
docker-compose version 1.22.0, build f46880fe
docker-py version: 3.4.1
CPython version: 3.6.6
OpenSSL version: OpenSSL 1.1.0f  25 May 2017
[root@master-1 ~]# 


###三、设置自签证书的
[root@master-1 ssl]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=nginxsvc/O=nginxsvc"
Generating a 2048 bit RSA private key
.....................................................+++
..............+++
writing new private key to 'tls.key'
-----
[root@master-1 ssl]# ll
total 8
lrwxrwxrwx. 1 root root   16 Nov 19 18:02 certs -> ../pki/tls/certs
-rw-r--r--  1 root root 1143 Nov 20 11:49 tls.crt
-rw-r--r--  1 root root 1704 Nov 20 11:49 tls.key
[root@master-1 ssl]# pwd
/etc/ssl

###四、Harbor2.3.0二进制包上传到服务器，解压
lex@lexliudeMacBook-Pro Downloads %  scp harbor-offline-installer-v2.3.4.tgz root@172.16.201.134:/root/
The authenticity of host '172.16.201.134 (172.16.201.134)' can't be established.
ECDSA key fingerprint is SHA256:HtE35snzTJJeXnx+Y4cDhCZ30O2/TuhmRGu422FAx/c.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '172.16.201.134' (ECDSA) to the list of known hosts.
root@172.16.201.134's password: 
harbor-offline-installer-v2.3.4.tgz                                                100%  579MB  17.4MB/s   00:33    
lex@lexliudeMacBook-Pro Downloads %  

[root@master-1 ~]# tar -xvf harbor-offline-installer-v2.3.4.tgz
[root@master-1 ~]# cd harbor

###五、修改配置文件：
[root@master-1 harbor]# mv harbor.yml.tmp harbor.yml
[root@master-1 harbor]# vim harbor.yml
[root@master-1 harbor]# cat harbor.yml |grep -v "#"

hostname: 172.16.201.134

http:
  port: 80

https:
  port: 443
  certificate: /etc/ssl/tls.crt
  private_key: /etc/ssl/tls.key

harbor_admin_password: Harbor12345

database:
  password: root123
  max_idle_conns: 100
  max_open_conns: 900

data_volume: /data

trivy:
  ignore_unfixed: false
  skip_update: false
  insecure: false

jobservice:
  max_job_workers: 10

notification:
  webhook_job_max_retry: 10

chart:
  absolute_url: disabled

log:
  level: info
  local:
    rotate_count: 50
    rotate_size: 200M
    location: /var/log/harbor


_version: 2.3.0
proxy:
  http_proxy:
  https_proxy:
  no_proxy:
  components:
    - core
    - jobservice
    - trivy

[root@master-1 harbor]# 


###六、在当前文件夹中开启harbor
####1、准备
[root@master-1 harbor]# ./prepare
prepare base dir is set to /root/cicd/harbor
Unable to find image 'goharbor/prepare:v2.3.4' locally
v2.3.4: Pulling from goharbor/prepare
c931d4b85338: Pull complete 
99e0f8e91189: Pull complete 
232a147744dc: Pull complete 
10882d0cd853: Pull complete 
5612f307eae2: Pull complete 
8e53db3b18bb: Pull complete 
1696a17c367c: Pull complete 
547e86f041f4: Pull complete 
Digest: sha256:c5c93fca8cc7766159ccdaff7c4fcac62503304d411914381928c1bc43d19df4
Status: Downloaded newer image for goharbor/prepare:v2.3.4
Generated configuration file: /config/portal/nginx.conf
Generated configuration file: /config/log/logrotate.conf
Generated configuration file: /config/log/rsyslog_docker.conf
Generated configuration file: /config/nginx/nginx.conf
Generated configuration file: /config/core/env
Generated configuration file: /config/core/app.conf
Generated configuration file: /config/registry/config.yml
Generated configuration file: /config/registryctl/env
Generated configuration file: /config/registryctl/config.yml
Generated configuration file: /config/db/env
Generated configuration file: /config/jobservice/env
Generated configuration file: /config/jobservice/config.yml
Generated and saved secret to file: /data/secret/keys/secretkey
Successfully called func: create_root_cert
Generated configuration file: /compose_location/docker-compose.yml
Clean up the input dir
[root@master-1 harbor]# 


####2、安装  (运行此处的时候需要一定的时间，请等待吧)：
[root@master-1 harbor]# ./install.sh 
[Step 0]: checking if docker is installed ...
Note: docker version: 19.03.13
[Step 1]: checking docker-compose is installed ...
Note: docker-compose version: 1.22.0
[Step 2]: loading Harbor images ...
bab588f83459: Loading layer [==================================================>]   8.75MB/8.75MB
5f0fc8804b08: Loading layer [==================================================>]  11.64MB/11.64MB
c28f2daf5359: Loading layer [==================================================>]  1.688MB/1.688MB
Loaded image: goharbor/harbor-portal:v2.3.4
e1768f3b0fc8: Loading layer [==================================================>]   8.75MB/8.75MB
Loaded image: goharbor/nginx-photon:v2.3.4
80c494db90e8: Loading layer [==================================================>]   6.82MB/6.82MB
f7c774b190f5: Loading layer [==================================================>]  6.219MB/6.219MB
3f9238d93e31: Loading layer [==================================================>]  15.88MB/15.88MB
357454838775: Loading layer [==================================================>]  29.29MB/29.29MB
bbdcee8bdadb: Loading layer [==================================================>]  22.02kB/22.02kB
ac89f4dce657: Loading layer [==================================================>]  15.88MB/15.88MB
Loaded image: goharbor/notary-server-photon:v2.3.4
305b24ea374e: Loading layer [==================================================>]  7.363MB/7.363MB
55dcab863ae7: Loading layer [==================================================>]  4.096kB/4.096kB
4832b43b9049: Loading layer [==================================================>]  3.072kB/3.072kB
c5065fd5a724: Loading layer [==================================================>]  31.52MB/31.52MB
c1732b627bc6: Loading layer [==================================================>]  11.39MB/11.39MB
84e3371d6d71: Loading layer [==================================================>]   43.7MB/43.7MB
Loaded image: goharbor/trivy-adapter-photon:v2.3.4
0bda5dd4507f: Loading layer [==================================================>]  9.918MB/9.918MB
9b8b0a75131b: Loading layer [==================================================>]  3.584kB/3.584kB
8a27a2e44308: Loading layer [==================================================>]   2.56kB/2.56kB
c2d79ade2e58: Loading layer [==================================================>]  73.36MB/73.36MB
a7efb206c673: Loading layer [==================================================>]  5.632kB/5.632kB
a0151a9b651b: Loading layer [==================================================>]  94.72kB/94.72kB
077455b7a6e9: Loading layer [==================================================>]  11.78kB/11.78kB
77235bb05a07: Loading layer [==================================================>]  74.26MB/74.26MB
d55c045b5185: Loading layer [==================================================>]   2.56kB/2.56kB
Loaded image: goharbor/harbor-core:v2.3.4
1dcd2f87d99c: Loading layer [==================================================>]  1.096MB/1.096MB
5991768c62a8: Loading layer [==================================================>]  5.888MB/5.888MB
4cbb6847a67d: Loading layer [==================================================>]  173.7MB/173.7MB
437252ecb71f: Loading layer [==================================================>]  15.73MB/15.73MB
324517377bf0: Loading layer [==================================================>]  4.096kB/4.096kB
4697090444de: Loading layer [==================================================>]  6.144kB/6.144kB
19cb7f4e8295: Loading layer [==================================================>]  3.072kB/3.072kB
eb573ba4e927: Loading layer [==================================================>]  2.048kB/2.048kB
a30648a5fa3e: Loading layer [==================================================>]   2.56kB/2.56kB
65ab99d7c381: Loading layer [==================================================>]   2.56kB/2.56kB
9395849bf38f: Loading layer [==================================================>]   2.56kB/2.56kB
23a2711d2570: Loading layer [==================================================>]  8.704kB/8.704kB
Loaded image: goharbor/harbor-db:v2.3.4
2fcbfe43743b: Loading layer [==================================================>]  9.918MB/9.918MB
71f1cf1a21e7: Loading layer [==================================================>]  3.584kB/3.584kB
a5fd6aea12f3: Loading layer [==================================================>]   2.56kB/2.56kB
5d286dafbc99: Loading layer [==================================================>]  82.47MB/82.47MB
421d40a4b24e: Loading layer [==================================================>]  83.27MB/83.27MB
Loaded image: goharbor/harbor-jobservice:v2.3.4
a95079faecd9: Loading layer [==================================================>]  6.825MB/6.825MB
baf6f07b0d35: Loading layer [==================================================>]  4.096kB/4.096kB
5021c842bb8d: Loading layer [==================================================>]  3.072kB/3.072kB
964d95b989da: Loading layer [==================================================>]  19.02MB/19.02MB
f75de434d758: Loading layer [==================================================>]  19.81MB/19.81MB
Loaded image: goharbor/registry-photon:v2.3.4
Loaded image: goharbor/prepare:v2.3.4
b370aa55f6bb: Loading layer [==================================================>]  6.825MB/6.825MB
c40e1854804c: Loading layer [==================================================>]  4.096kB/4.096kB
62c713c68f94: Loading layer [==================================================>]  19.02MB/19.02MB
067cb9d13dc2: Loading layer [==================================================>]  3.072kB/3.072kB
da711fd41a09: Loading layer [==================================================>]  25.43MB/25.43MB
61af5bc5684d: Loading layer [==================================================>]  45.24MB/45.24MB
Loaded image: goharbor/harbor-registryctl:v2.3.4
1fce02e2f0b2: Loading layer [==================================================>]  9.918MB/9.918MB
90182ef8d6af: Loading layer [==================================================>]  17.71MB/17.71MB
30cf2783eb4e: Loading layer [==================================================>]  4.608kB/4.608kB
c8fa87f0c432: Loading layer [==================================================>]   18.5MB/18.5MB
Loaded image: goharbor/harbor-exporter:v2.3.4
1bbdf18315cc: Loading layer [==================================================>]   6.82MB/6.82MB
332423af2705: Loading layer [==================================================>]  6.219MB/6.219MB
a2024685b4fa: Loading layer [==================================================>]  14.47MB/14.47MB
a04184f058e2: Loading layer [==================================================>]  29.29MB/29.29MB
8fec5d89081c: Loading layer [==================================================>]  22.02kB/22.02kB
da3e11c34f87: Loading layer [==================================================>]  14.47MB/14.47MB
Loaded image: goharbor/notary-signer-photon:v2.3.4
01d27e9ffb2b: Loading layer [==================================================>]  125.5MB/125.5MB
dc823a6e78ed: Loading layer [==================================================>]  3.584kB/3.584kB
65b4a979ece5: Loading layer [==================================================>]  3.072kB/3.072kB
bd00d96da856: Loading layer [==================================================>]   2.56kB/2.56kB
5270920b2bb1: Loading layer [==================================================>]  3.072kB/3.072kB
9c736a3f305b: Loading layer [==================================================>]  3.584kB/3.584kB
e3053dfef34c: Loading layer [==================================================>]  19.97kB/19.97kB
Loaded image: goharbor/harbor-log:v2.3.4
56dfaad4f3ae: Loading layer [==================================================>]  121.4MB/121.4MB
1e54038c4760: Loading layer [==================================================>]  3.072kB/3.072kB
3283554b0538: Loading layer [==================================================>]   59.9kB/59.9kB
607e2816db21: Loading layer [==================================================>]  61.95kB/61.95kB
Loaded image: goharbor/redis-photon:v2.3.4
a18bcd8df6d3: Loading layer [==================================================>]  6.824MB/6.824MB
0dcfc4641990: Loading layer [==================================================>]  67.47MB/67.47MB
88be11c9e4c9: Loading layer [==================================================>]  3.072kB/3.072kB
3a99925cf064: Loading layer [==================================================>]  4.096kB/4.096kB
a426fee0fb8a: Loading layer [==================================================>]  68.26MB/68.26MB
Loaded image: goharbor/chartmuseum-photon:v2.3.4

[Step 3]: preparing environment ...
[Step 4]: preparing harbor configs ...
prepare base dir is set to /root/cicd/harbor
Clearing the configuration file: /config/portal/nginx.conf
Clearing the configuration file: /config/log/logrotate.conf
Clearing the configuration file: /config/log/rsyslog_docker.conf
Clearing the configuration file: /config/nginx/nginx.conf
Clearing the configuration file: /config/core/env
Clearing the configuration file: /config/core/app.conf
Clearing the configuration file: /config/registry/passwd
Clearing the configuration file: /config/registry/config.yml
Clearing the configuration file: /config/registryctl/env
Clearing the configuration file: /config/registryctl/config.yml
Clearing the configuration file: /config/db/env
Clearing the configuration file: /config/jobservice/env
Clearing the configuration file: /config/jobservice/config.yml
Generated configuration file: /config/portal/nginx.conf
Generated configuration file: /config/log/logrotate.conf
Generated configuration file: /config/log/rsyslog_docker.conf
Generated configuration file: /config/nginx/nginx.conf
Generated configuration file: /config/core/env
Generated configuration file: /config/core/app.conf
Generated configuration file: /config/registry/config.yml
Generated configuration file: /config/registryctl/env
Generated configuration file: /config/registryctl/config.yml
Generated configuration file: /config/db/env
Generated configuration file: /config/jobservice/env
Generated configuration file: /config/jobservice/config.yml
loaded secret from file: /data/secret/keys/secretkey
Generated configuration file: /compose_location/docker-compose.yml
Clean up the input dir



[Step 5]: starting Harbor ...
Creating network "harbor_harbor" with the default driver
Creating harbor-log ... done
Creating redis         ... done
Creating registry      ... done
Creating harbor-db     ... done
Creating harbor-portal ... done
Creating registryctl   ... done
Creating harbor-core   ... done
Creating nginx             ... done
Creating harbor-jobservice ... done
✔ ----Harbor has been installed and started successfully.----
[root@master-1 harbor]# 




###七、启动成功，查看一下（完美的运行）
[root@master-1 harbor]# docker-compose ps
      Name                     Command                  State                          Ports                   
---------------------------------------------------------------------------------------------------------------
harbor-core         /harbor/entrypoint.sh            Up (healthy)                                              
harbor-db           /docker-entrypoint.sh 96 13      Up (healthy)                                              
harbor-jobservice   /harbor/entrypoint.sh            Up (healthy)                                              
harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514->10514/tcp                  
harbor-portal       nginx -g daemon off;             Up (healthy)                                              
nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80->8080/tcp, 0.0.0.0:443->8443/tcp
redis               redis-server /etc/redis.conf     Up (healthy)                                              
registry            /home/harbor/entrypoint.sh       Up (healthy)                                              
registryctl         /home/harbor/start.sh            Up (healthy)                                              
[root@master-1 harbor]# 

访问：
https://172.16.201.134/




[root@master-1 harbor]# docker-compose stop
Stopping nginx             ... done
Stopping harbor-jobservice ... done
Stopping harbor-core       ... done
Stopping harbor-portal     ... done
Stopping registryctl       ... done
Stopping harbor-db         ... done
Stopping redis             ... done
Stopping registry          ... done
Stopping harbor-log        ... done
[root@master-1 harbor]#
[root@master-1 harbor]# docker-compose ps  
      Name                     Command                State     Ports
---------------------------------------------------------------------
harbor-core         /harbor/entrypoint.sh            Exit 137        
harbor-db           /docker-entrypoint.sh 96 13      Exit 0          
harbor-jobservice   /harbor/entrypoint.sh            Exit 137        
harbor-log          /bin/sh -c /usr/local/bin/ ...   Exit 137        
harbor-portal       nginx -g daemon off;             Exit 0          
nginx               nginx -g daemon off;             Exit 0          
redis               redis-server /etc/redis.conf     Exit 0          
registry            /home/harbor/entrypoint.sh       Exit 137        
registryctl         /home/harbor/start.sh            Exit 137        
[root@master-1 harbor]# 


###八、安装Rancher
v1版本的dockerhub地址：
https://hub.docker.com/r/rancher/server
 
v2版本的dockerhub地址：
https://hub.docker.com/r/rancher/rancher/
 
Rancher version v2.6.2 

####1、本文采用的v2版本，下载命令为：
[root@node-1 ~]# docker pull rancher/rancher:stable
stable: Pulling from rancher/rancher
af637d9f9908: Pull complete 
f9224574abce: Pull complete 
ebae3352c76e: Pull complete 
7b21d0842057: Pull complete 
88a4b8d1c9ea: Pull complete 
068e40008a4a: Pull complete 
af8d478ed359: Pull complete 
0570bed5c15a: Pull complete 
f7dfd304c929: Pull complete 
e0f5d341d113: Pull complete 
bd89a58c597d: Pull complete 
c03c718eef46: Pull complete 
5e1f456cdbf8: Pull complete 
e55dca76e75f: Pull complete 
c9a821480bc3: Pull complete 
c6e3491b680d: Pull complete 
9d7c10a11f0f: Pull complete 
Digest: sha256:51658639bf2b28f4796e7daa04b0434e29fcf69ce062bf649462be77ae76cd2c
Status: Downloaded newer image for rancher/rancher:stable
docker.io/rancher/rancher:stable
[root@node-1 ~]# 


[root@master-1 ~]# docker pull rancher/rancher:latest
latest: Pulling from rancher/rancher
af637d9f9908: Pull complete 
f9224574abce: Pull complete 
ebae3352c76e: Pull complete 
7b21d0842057: Pull complete 
88a4b8d1c9ea: Pull complete 
068e40008a4a: Pull complete 
af8d478ed359: Pull complete 
0570bed5c15a: Pull complete 
f7dfd304c929: Pull complete 
e0f5d341d113: Pull complete 
bd89a58c597d: Pull complete 
c03c718eef46: Pull complete 
5e1f456cdbf8: Pull complete 
e55dca76e75f: Pull complete 
c9a821480bc3: Pull complete 
c6e3491b680d: Pull complete 
9d7c10a11f0f: Pull complete 
Digest: sha256:51658639bf2b28f4796e7daa04b0434e29fcf69ce062bf649462be77ae76cd2c
Status: Downloaded newer image for rancher/rancher:latest
docker.io/rancher/rancher:latest
[root@master-1 ~]# 


####2、安装Rancher
[root@master-1 ~]# docker run --privileged -d --restart=always --name rancher -p 800:80 -p 4433:443 rancher/rancher:stable
56e501bd15792fc5a0c0b3188a7b663a7024e0be40202403c617cd1f1ead4639
 

[root@master-1 ~]# docker run --privileged -d --restart=always --name rancher -p 800:80 -p 4433:443 rancher/rancher:latest
ff25c8a049592f9d553820ac0b2fe67054c83398e28b6ccfd7b8bb5d5b8d313a


####3、查看日志
[root@master-1 data]#  docker logs -f rancher
一堆启动日志

[root@master-1 data]# docker ps -a|grep rancher
cd512a7b1500        rancher/rancher:stable                              "entrypoint.sh"          4 minutes ago       Up About a minute         0.0.0.0:80->80/tcp, 0.0.0.0:443->443/tcp   rancher



等待几分钟，没有日志输出了，表示已经启动好了。


####4、访问页面
https://172.16.201.134:4433/dashboard/auth/login

注意：这里必须要用http。即使你用http访问，它还是会强制跳转到https
第一次访问时，显示的是英文。提示默认的admin密码不安装，需要设置一个复杂性密码。


临时密码：
[root@master-1 harbor]# docker logs  rancher  2>&1 | grep "Bootstrap Password:"    
2021/11/20 04:30:04 [INFO] Bootstrap Password: 2tgv68s7rn5rd8wxcbh5gm5dz5h7rxvbb68zctqnnf8flp475brg5x
[root@master-1 harbor]# 

[root@master-1 ~]# docker logs  rancher  2>&1 | grep "Bootstrap Password:"    
2021/12/15 06:18:37 [INFO] Bootstrap Password: 7sfhztlc55bblqppq8cvxqbc4m5jmsxsjcl5cq966nkspzl672l2r9



####主页左下角配置中文


####安装k8s

复制下方命令，在集群对应角色的机器上执行。
执行完之后，页面会出现注册成功的提示：

####3个节点分别执行：
[root@master-1 ~]# sudo docker run -d --privileged --restart=unless-stopped --net=host -v /etc/kubernetes:/etc/kubernetes -v /var/run:/var/run  rancher/rancher-agent:v2.6.2 --server https://172.16.201.134:4433 --token hnpn2cv67dqpj5zdbkczpbpgnscrtp75xsj6btwkwxj86cd29jq5x8 --ca-checksum 5b2cfdd1628922da133f33e1d19d354485ce739d9a2cba8bfb2e9bed93976950 --etcd --controlplane --worker
Unable to find image 'rancher/rancher-agent:v2.6.2' locally
v2.6.2: Pulling from rancher/rancher-agent
af637d9f9908: Already exists 
99818c97c8ef: Pull complete 
133a71f76cc8: Pull complete 
baa07b051d77: Pull complete 
1f6c467d9ef7: Pull complete 
653de0bb9577: Pull complete 
fcbbfd6f12f6: Pull complete 
Digest: sha256:13da2f87c3b74f3b0f1f61fb4c68122f96cd7abaa46648d66abb0e2c8c49f977
Status: Downloaded newer image for rancher/rancher-agent:v2.6.2
154ac759008a1a6f87807cf8fdb9c3777ada3a302944952348cdb5a331bc7b28




rm -rf /etc/ceph
rm -fr /etc/cni
rm -fr /etc/kubernetes
rm -fr /opt/cni
rm -fr /opt/rke
rm -fr /run/secrets/kubernetes.io
rm -fr /run/calico
rm -fr /run/flannel
rm -fr /var/lib/calico
rm -fr /var/lib/etcd
rm -fr /var/lib/cni
rm -fr /var/lib/kubelet
rm -fr /var/lib/rancher/rke/log
rm -fr /var/log/containers
rm -fr /var/log/pods
rm -fr /var/run/calico



[root@master-1 ~]# docker ps -a
CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS                                        NAMES
154ac759008a        rancher/rancher-agent:v2.6.2   "run.sh --server htt…"   7 minutes ago       Up 7 minutes                                                     focused_leavitt
ff25c8a04959        rancher/rancher:latest         "entrypoint.sh"          26 minutes ago      Up 26 minutes       0.0.0.0:800->80/tcp, 0.0.0.0:4433->443/tcp   rancher
[root@master-1 ~]# 


[root@node-1 ~]#  docker ps -a
CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
5597d3d1fa26        rancher/rancher-agent:v2.6.2   "run.sh --server htt…"   4 minutes ago       Up 4 minutes                            pensive_raman
[root@node-1 ~]# 


[root@node-2 ~]#  docker ps -a
CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS              PORTS               NAMES
4ec75d82dac7        rancher/rancher-agent:v2.6.2   "run.sh --server htt…"   4 minutes ago       Up 4 minutes                            amazing_feistel
[root@node-2 ~]# 


######Clusters页面等待一段时间集群自动搭建完成，显示为active状态集群就搭建完成，中间有2次重启



###九、安装GitLab(docker-compose / K8S)
####（一）docker-compose
[root@master-1 cicd]# mkdir docker-gitlab;cd docker-gitlab
[root@master-1 docker-gitlab]# vim docker-compose.yml
version: '3'
services:
  gitlab:
    image: 'gitlab/gitlab-ce:latest'
    container_name: gitlab
    restart: always
    environment:
      GITLAB_OMNIBUS_CONFIG: |
        external_url 'http://172.16.201.134:8929' #若有域名可以写域名
        gitlab_rails['gitlab_shell_ssh_port'] = 2224
    ports:
      - '8929:8929'
      - '2224:22'
    volumes:
      - './config:/etc/gitlab'
      - './logs:/var/log/gitlab'
      - './data:/var/opt/gitlab'


[root@master-1 docker-gitlab]# docker-compose up -d
Creating network "docker-gitlab_default" with the default driver
Pulling gitlab (gitlab/gitlab-ce:latest)...
latest: Pulling from gitlab/gitlab-ce
7b1a6ab2e44d: Pull complete
f1684ebc6541: Pull complete
02468abe4769: Pull complete
67434d82c6fc: Pull complete
f3b378955ed9: Pull complete
9161b9f516d4: Pull complete
7b8b542d764e: Pull complete
e297c8f137fb: Pull complete
Digest: sha256:03b2f405be1ece43f6c3693a70ed592e8c2031dbaea438b5c3b9405c9ed84bd7
Status: Downloaded newer image for gitlab/gitlab-ce:latest
Creating gitlab ... done

[root@master-1 docker-gitlab]# docker-compose ps
 Name        Command               State                                       Ports                            
----------------------------------------------------------------------------------------------------------------
gitlab   /assets/wrapper   Up (health: starting)   0.0.0.0:2224->22/tcp, 443/tcp, 80/tcp, 0.0.0.0:8929->8929/tcp
[root@master-1 docker-gitlab]# 


访问：
http://172.16.201.134:8929/


[root@master-1 docker-gitlab]# docker-compose stop
Stopping gitlab ... done

[root@master-1 docker-gitlab]# docker-compose rm -v
Going to remove gitlab
Are you sure? [yN] y
Removing gitlab ... done
[root@master-1 docker-gitlab]# 
–f, –force，强制直接删除，包括非停止状态的容器
-v，删除容器所挂载的数据卷

----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------

####（二）K8S安装GitLab
######主要步骤：
持久化存储（本地nfs服务）
postgres数据库
redis缓存服务
gitlab应用

1、创建一个用于存储密码的secret文件:
创建username和password文件:
[root@master-1 gitlab]# echo -n "admin" > ./username
[root@master-1 gitlab]# echo -n "1f2d1e2e67df" > ./password

用kubectl生成secret对象:
[root@master-1 gitlab]# kubectl create secret generic db-user-pass --from-file=./username --from-file=./password
secret/db-user-pass created


2、nfs配置
[root@master-1 gitlab]# mkdir -p /nfs_dir/{vol1,vol2,vol3,vol4}
[root@master-1 gitlab]# exportfs -r
[root@master-1 gitlab]# exportfs
/nfs_dir/vol1   172.16.201.0/24
/nfs_dir/vol2   172.16.201.0/24
/nfs_dir/vol3   172.16.201.0/24
/nfs_dir/vol4   172.16.201.0/24


mount -t nfs 172.16.201.134:/nfs_dir/vol1  /nfs_dir/vol1
mount -t nfs 172.16.201.134:/nfs_dir/vol2  /nfs_dir/vol2
mount -t nfs 172.16.201.134:/nfs_dir/vol3  /nfs_dir/vol3
mount -t nfs 172.16.201.134:/nfs_dir/vol4  /nfs_dir/vol4


[root@node-1 ~]# df -h|grep 172.16.201.134
172.16.201.134:/nfs_dir/vol1   17G  9.6G  7.5G  57% /nfs_dir/vol1
172.16.201.134:/nfs_dir/vol2   17G  9.6G  7.5G  57% /nfs_dir/vol2
172.16.201.134:/nfs_dir/vol3   17G  9.6G  7.5G  57% /nfs_dir/vol3
172.16.201.134:/nfs_dir/vol4   17G  9.6G  7.5G  57% /nfs_dir/vol4

[root@node-2 ~]# df -h|grep 172.16.201.134
172.16.201.134:/nfs_dir/vol1   17G  9.6G  7.5G  57% /nfs_dir/vol1
172.16.201.134:/nfs_dir/vol2   17G  9.6G  7.5G  57% /nfs_dir/vol2
172.16.201.134:/nfs_dir/vol3   17G  9.6G  7.5G  57% /nfs_dir/vol3
172.16.201.134:/nfs_dir/vol4   17G  9.6G  7.5G  57% /nfs_dir/vol4


3、创建PV资源
[root@master-1 gitlab]# vim nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001
  labels:
    name: pv001
spec:
  nfs:
    path: /nfs_dir/vol1
    server: 172.16.201.134
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 1Gi

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv002
  labels:
    name: pv002
spec:
  nfs:
    path: /nfs_dir/vol2
    server: 172.16.201.134
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 1Gi

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv003
  labels:
    name: pv003
spec:
  nfs:
    path: /nfs_dir/vol3
    server: 172.16.201.134
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 1Gi

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv004
  labels:
    name: pv004
spec:
  nfs:
    path: /nfs_dir/vol4
    server: 172.16.201.134
  accessModes: ["ReadWriteMany","ReadWriteOnce"]
  capacity:
    storage: 1Gi

[root@master-1 gitlab]# kubectl apply -f nfs-pv.yaml
persistentvolume/pv001 created
persistentvolume/pv002 created
persistentvolume/pv003 created
persistentvolume/pv004 created
[root@master-1 gitlab]# kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv001   1Gi        RWO,RWX        Retain           Available                                   3s
pv002   1Gi        RWO,RWX        Retain           Available                                   3s
pv003   1Gi        RWO,RWX        Retain           Available                                   3s
pv004   1Gi        RWO,RWX        Retain           Available                                   3s
[root@master-1 gitlab]# 

4、部署Postgresql数据库，准备PostgreSQL数据库PVC
[root@master-1 gitlab]# vim gitlab-postgresql-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitlab-postgresql-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteMany"]
  resources:
    requests:
      storage: 1Gi

[root@master-1 gitlab]# kubectl apply -f gitlab-postgresql-pvc.yaml
persistentvolumeclaim/gitlab-postgresql-pvc created

[root@master-1 gitlab]# kubectl get pvc
NAME                    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
gitlab-postgresql-pvc   Bound    pv001    1Gi        RWO,RWX                       12s


5、部署PostgresSQL数据库实例及Service资源
[root@master-1 gitlab]# vim postgresql-Deployment-Service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql
  namespace: default
  labels:
    name: postgresql
spec:
  replicas: 1
  selector:
    matchLabels:
      name: postgresql
  template:
    metadata:
      name: postgresql
      labels:
        name: postgresql
    spec:
      containers:
      - name: postgresql
        image: sameersbn/postgresql
        imagePullPolicy: IfNotPresent
        env:
        - name: DB_USER
          value: gitlab
        - name: DB_PASS
          value: passw0rd
        - name: DB_NAME
          value: gitlab_production
        - name: DB_EXTENSION
          value: pg_trgm
        ports:
        - name: postgres
          containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql
          name: data
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -h
            - localhost
            - -U
            - postgres
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -h
            - localhost
            - -U
            - postgres
          initialDelaySeconds: 5
          timeoutSeconds: 1
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: gitlab-postgresql-pvc

---

apiVersion: v1
kind: Service
metadata:
  name: postgresql
  namespace: default
  labels:
    name: postgresql
spec:
  ports:
    - name: postgres
      port: 5432
      targetPort: postgres
  selector:
    name: postgresql

[root@master-1 gitlab]# kubectl apply -f postgresql-Deployment-Service.yaml
deployment.apps/postgresql created
[root@master-1 gitlab]# kubectl get pod -o wide  
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
postgresql-67cb84d6f8-cppqg   0/1     Running   0          81s   10.244.2.2   node-2   <none>           <none>


6、部署redis缓存数据库

[root@master-1 gitlab]# vim redis-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitlab-redis-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteMany"]
  resources:
    requests:
      storage: 1Gi

[root@master-1 gitlab]# kubectl apply -f redis-pvc.yaml
persistentvolumeclaim/gitlab-redis-pvc created
[root@master-1 gitlab]# kubectl get pvc -o wide
NAME                    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
gitlab-postgresql-pvc   Bound    pv001    1Gi        RWO,RWX                       21m   Filesystem
gitlab-redis-pvc        Bound    pv002    1Gi        RWO,RWX                       18s   Filesystem
[root@master-1 gitlab]# 

[root@master-1 gitlab]# vim redis-Deployment-Service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: default
  labels:
    name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      name: redis
  template:
    metadata:
      name: redis
      labels:
        name: redis
    spec:
      containers:
      - name: redis
        image: sameersbn/redis
        imagePullPolicy: IfNotPresent
        ports:
        - name: redis
          containerPort: 6379
        volumeMounts:
        - mountPath: /var/lib/redis
          name: data
        livenessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          timeoutSeconds: 1
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: gitlab-redis-pvc

---

apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: default
  labels:
    name: redis
spec:
  ports:
    - name: redis
      port: 6379
      targetPort: redis
  selector:
    name: redis

[root@master-1 gitlab]# kubectl apply -f redis-Deployment-Service.yaml
deployment.apps/redis created
service/redis created

[root@master-1 gitlab]# kubectl get pod -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
postgresql-67cb84d6f8-cppqg   1/1     Running   0          9m37s   10.244.2.2   node-2   <none>           <none>
redis-f68fd858-bzc4s          0/1     Running   0          34s     10.244.1.3   node-1   <none>           <none>



7、部署gitlab服务
[root@master-1 gitlab]# vim gitlab-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gitlab-pvc
  namespace: default
spec:
  accessModes: ["ReadWriteMany"]
  resources:
    requests:
      storage: 1Gi


[root@master-1 gitlab]# kubectl apply -f gitlab-pvc.yaml
persistentvolumeclaim/gitlab-pvc created

[root@master-1 gitlab]# kubectl get pvc -o wide
NAME                    STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE     VOLUMEMODE
gitlab-postgresql-pvc   Bound    pv001    1Gi        RWO,RWX                       29m     Filesystem
gitlab-pvc              Bound    pv003    1Gi        RWO,RWX                       15s     Filesystem
gitlab-redis-pvc        Bound    pv002    1Gi        RWO,RWX                       8m35s   Filesystem



8、添加secret资源配置root密码
echo -n "admin" > ./username
echo -n "admin123" > ./password
kubectl create secret generic db-user-pass --from-file=./username --from-file=./password


[root@master-1 gitlab]# kubectl get secret
NAME                  TYPE                                  DATA   AGE
db-user-pass          Opaque                                2      1s
default-token-q94x6   kubernetes.io/service-account-token   3      27h



9、gitlab实例及Service部署

[root@master-1 gitlab]# vim gitlab-Deployment-Service.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gitlab
  namespace: default
  labels:
    name: gitlab
spec:
  replicas: 1
  selector:
    matchLabels:
      name: gitlab
  template:
    metadata:
      name: gitlab
      labels:
        name: gitlab
    spec:
      containers:
      - name: gitlab
        image: sameersbn/gitlab:12.1.6
        imagePullPolicy: IfNotPresent
        env:
        - name: TZ
          value: Asia/Shanghai
        - name: GITLAB_TIMEZONE
          value: Beijing
        - name: GITLAB_SECRETS_DB_KEY_BASE
          value: long-and-random-alpha-numeric-string
        - name: GITLAB_SECRETS_SECRET_KEY_BASE
          value: long-and-random-alpha-numeric-string
        - name: GITLAB_SECRETS_OTP_KEY_BASE
          value: long-and-random-alpha-numeric-string
        - name: GITLAB_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-user-pass
              key: password
        - name: GITLAB_ROOT_EMAIL
          value: 1689991551@qq.com
        - name: GITLAB_HOST
          value: gitlab.jzh.com
        - name: GITLAB_PORT
          value: "80"
        - name: GITLAB_SSH_PORT
          value: "30022"
        - name: GITLAB_NOTIFY_ON_BROKEN_BUILDS
          value: "true"
        - name: GITLAB_NOTIFY_PUSHER
          value: "false"
        - name: GITLAB_BACKUP_SCHEDULE
          value: daily
        - name: GITLAB_BACKUP_TIME
          value: 01:00
        - name: DB_TYPE
          value: postgres
        - name: DB_HOST
          value: postgresql
        - name: DB_PORT
          value: "5432"
        - name: DB_USER
          value: gitlab
        - name: DB_PASS
          value: passw0rd
        - name: DB_NAME
          value: gitlab_production
        - name: REDIS_HOST
          value: redis
        - name: REDIS_PORT
          value: "6379"
        ports:
        - name: http
          containerPort: 80
        - name: ssh
          containerPort: 22
        volumeMounts:
        - mountPath: /home/git/data
          name: data
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 180
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          timeoutSeconds: 1
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: gitlab-pvc

---

apiVersion: v1
kind: Service
metadata:
  name: gitlab
  namespace: default
  labels:
    name: gitlab
spec:
  ports:
    - name: http
      port: 80
      targetPort: http
    - name: ssh
      port: 22
      targetPort: ssh
      nodePort: 30022
  type: NodePort
  selector:
    name: gitlab

[root@master-1 gitlab]# kubectl apply -f gitlab-Deployment-Service.yaml
deployment.apps/gitlab created
service/gitlab created

[root@master-1 gitlab]# kubectl get pod -o wide 
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
gitlab-6dd65b86ff-2zt57       1/1     Running   1          11m   10.244.2.3   node-2   <none>           <none>
postgresql-67cb84d6f8-cppqg   1/1     Running   0          29m   10.244.2.2   node-2   <none>           <none>
redis-f68fd858-bzc4s          1/1     Running   0          20m   10.244.1.3   node-1   <none>           <none>

[root@master-1 gitlab]#  kubectl get svc -o wide  
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                     AGE   SELECTOR
gitlab       NodePort    10.1.0.128     <none>        80:31746/TCP,22:30022/TCP   12m   name=gitlab
kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP                     28h   <none>
postgresql   ClusterIP   10.1.16.2      <none>        5432/TCP                    31m   name=postgresql
redis        ClusterIP   10.1.220.213   <none>        6379/TCP                    21m   name=redis


[root@master-1 gitlab]# netstat -antup|grep 31746
tcp        0      0 0.0.0.0:31746           0.0.0.0:*               LISTEN      9750/kube-proxy     
[root@master-1 gitlab]# netstat -antup|grep 30022
tcp        0      0 0.0.0.0:30022           0.0.0.0:*               LISTEN      9750/kube-proxy     
[root@master-1 gitlab]# 


10、通过IP:Port访问Gitlab服务，用户名 root，密码 admin123
http://172.16.201.134:31746/users/sign_in






kubectl exec -it gitlab-6dd65b86ff-2zt57  -- bash


[root@master-1 gitlab]# kubectl delete -f .
deployment.apps "gitlab" deleted
service "gitlab" deleted
persistentvolumeclaim "gitlab-postgresql-pvc" deleted
persistentvolumeclaim "gitlab-pvc" deleted
persistentvolume "pv001" deleted
persistentvolume "pv002" deleted
persistentvolume "pv003" deleted
persistentvolume "pv004" deleted
deployment.apps "postgresql" deleted
service "postgresql" deleted
deployment.apps "redis" deleted
service "redis" deleted
persistentvolumeclaim "gitlab-redis-pvc" deleted



[root@master-1 harbor]# kubectl get pod -o wide  
NAME                          READY   STATUS    RESTARTS   AGE    IP           NODE     NOMINATED NODE   READINESS GATES
gitlab-6dd65b86ff-2zt57       1/1     Running   0          2d5h   10.244.2.3   node-2   <none>           <none>
postgresql-67cb84d6f8-cppqg   1/1     Running   0          2d5h   10.244.2.2   node-2   <none>           <none>
redis-f68fd858-bzc4s          1/1     Running   0          2d5h   10.244.1.3   node-1   <none>           <none>
jenkins-77b9c47874-qjgfd      1/1     Running   1          13h    10.244.1.3   node-1   <none>           <none>
jenkins-slave-c07daa7b-31ef   1/1     Running   0          18s    10.244.2.2   node-2   <none>           <none>

[root@master-1 harbor]# docker-compose ps
      Name                     Command                  State                          Ports                   
---------------------------------------------------------------------------------------------------------------
harbor-core         /harbor/entrypoint.sh            Up (healthy)                                              
harbor-db           /docker-entrypoint.sh 96 13      Up (healthy)                                              
harbor-jobservice   /harbor/entrypoint.sh            Up (healthy)                                              
harbor-log          /bin/sh -c /usr/local/bin/ ...   Up (healthy)   127.0.0.1:1514->10514/tcp                  
harbor-portal       nginx -g daemon off;             Up (healthy)                                              
nginx               nginx -g daemon off;             Up (healthy)   0.0.0.0:80->8080/tcp, 0.0.0.0:443->8443/tcp
redis               redis-server /etc/redis.conf     Up (healthy)                                              
registry            /home/harbor/entrypoint.sh       Up (healthy)                                              
registryctl         /home/harbor/start.sh            Up (healthy)                                              
[root@master-1 harbor]# kubectl get pods -n k8s-ops
NAME                                                             READY   STATUS    RESTARTS   AGE



kubectl delete namespace kube-ops
kubectl create namespace devops
kubectl create -f .
kubectl delete -f .

kubectl apply -f 
kubectl apply -f redis.yaml 
kubectl get pod -o wide    
kubectl get svc -n devops 

kubectl delete -f 
kubectl apply -f tidb-cluster.yaml -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster

kubectl get pods -n mycluster -o wide
kubectl get pv,pvc -n mycluster -o wide
kubectl get svc  -n mycluster
kubectl get event  -n mycluster
kubectl describe pod  mycluster-pd-0 -n mycluster
kubectl exec -it   -- bash
kubectl patch pv pd-detailed-tidb-pd-0  -p '{"metadata":{"finalizers":null}}' -n mycluster                 
kubectl delete pvc pd-mycluster-pd-0 -n mycluster
kubectl delete pv tidb-cluster-node-1-pv05


kubectl describe pod   mycluster-tiflash-0 -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster

kubectl get event


###十、安装Jenkins
####1、nfs配置
[root@master-1 jenkins]# mkdir -p /nfs/data/jenkins
[root@master-1 jenkins]# vim /etc/exports
/nfs/data/jenkins   172.16.201.0/24(rw,no_root_squash)
[root@master-1 jenkins]# exportfs -r
[root@master-1 jenkins]# exportfs
/nfs/data/jenkins   172.16.201.0/24


mount -t nfs 172.16.201.134:/nfs/data/jenkins  /nfs/data/jenkins

[root@node-1 ~]# mkdir -p /nfs/data/jenkins
[root@node-1 ~]# mount -t nfs 172.16.201.134:/nfs/data/jenkins  /nfs/data/jenkins
[root@node-1 ~]# df -h|grep jenkins
172.16.201.134:/nfs/data/jenkins   17G  9.7G  7.4G  57% /nfs/data/jenkins
[root@node-1 ~]# 

####2、创建 Jenkins 用于存储的 PV、PVC
创建 Kubernetes 的 PV、PVC 资源，其中 PV 用于与 NFS 关联，需要设置 NFS Server 服务器地址和挂载的路径，修改占用空间大小。而 PVC 则是与应用关联，方便应用与 NFS 绑定挂载，下面是 PV、PVC 的资源对象 yaml 文件。

[root@master-1 jenkins]# vim jenkins-storage.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: jenkins
  labels:
    app: jenkins
spec:
  capacity:          
    storage: 5Gi
  accessModes:       
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain  
  mountOptions:
    - hard
    - nfsvers=4.1    
  nfs:
    path: /nfs/data/jenkins
    server: 172.16.201.134
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: jenkins
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  selector:
    matchLabels:
      app: jenkins

kubectl delete namespace mydlqcloud
[root@master-1 jenkins]# kubectl create namespace mydlqcloud
namespace/mydlqcloud created

[root@master-1 jenkins]# kubectl apply -f jenkins-storage.yaml -n mydlqcloud
persistentvolume/jenkins created
persistentvolumeclaim/jenkins created


[root@master-1 jenkins]# kubectl get pv,pvc -n mydlqcloud
NAME                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                           STORAGECLASS   REASON   AGE
persistentvolume/jenkins   5Gi        RWO            Retain           Bound       mydlqcloud/jenkins                                      44s
persistentvolume/pv001     1Gi        RWO,RWX        Retain           Bound       default/gitlab-postgresql-pvc                           2d11h
persistentvolume/pv002     1Gi        RWO,RWX        Retain           Bound       default/gitlab-redis-pvc                                2d11h
persistentvolume/pv003     1Gi        RWO,RWX        Retain           Bound       default/gitlab-pvc                                      2d11h
persistentvolume/pv004     1Gi        RWO,RWX        Retain           Available                                                           2d11h

NAME                            STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/jenkins   Bound    jenkins   5Gi        RWO                           30s
[root@master-1 jenkins]# 




####3、创建 ServiceAccount & ClusterRoleBinding
Kubernetes 集群一般情况下都默认开启了 RBAC 权限，所以需要创建一个角色和服务账户，设置角色拥有一定权限，然后将角色与 ServiceAccount 绑定，最后将 ServiceAccount 与 Jenkins 绑定，这样来赋予 Jenkins 一定的权限，使其能够执行一些需要权限才能进行的操作。这里为了方便，将 cluster-admin 绑定到 ServiceAccount 来保证 Jenkins 拥有足够的权限。
注意： 请修改下面的 Namespace 参数，改成部署的 Jenkins 所在的 Namespace。


[root@master-1 jenkins]# vim jenkins-rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: jenkins-admin
  namespace: mydlqcloud
  labels:
    name: jenkins
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: jenkins-admin
  labels:
    name: jenkins
subjects:
  - kind: ServiceAccount
    name: jenkins-admin
    namespace: mydlqcloud
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io


[root@master-1 jenkins]# kubectl apply -f jenkins-rbac.yaml -n mydlqcloud
serviceaccount/jenkins-admin created
clusterrolebinding.rbac.authorization.k8s.io/jenkins-admin created
[root@master-1 jenkins]# 

[root@master-1 jenkins]# kubectl get serviceaccount,ClusterRoleBinding -n mydlqcloud|grep jenkins
serviceaccount/jenkins-admin   1         39s
clusterrolebinding.rbac.authorization.k8s.io/jenkins-admin  ClusterRole/cluster-admin    39s

####4、创建 Service & Deployment
在 Kubernetes 中部署服务需要部署文件，这里部署 Jenkins 需要创建 Service 与 Deployment 对象，其中两个对象需要做一些配置，如下：
Service：Service 暴露两个接口 8080 与 50000，其中 8080 是 Jenkins API 和 UI 的端口，而 50000 则是供代理使用的端口。
Deployment： Deployment 中，需要设置容器安全策略为 runAsUser: 0 赋予容器以 Root 权限运行，并且暴露 8080 与 50000 两个端口与 Service 对应，而且还要注意的是，还要设置上之前创建的服务账户 “jenkins-admin”。

[root@master-1 jenkins]# vim jenkins-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: jenkins
  labels:
    app: jenkins
spec:
  type: NodePort
  ports:
  - name: http
    port: 8080
    targetPort: 8080
    nodePort: 32001
  - name: jnlp
    port: 50000
    targetPort: 50000
    nodePort: 32002
  selector:
    app: jenkins
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins
  labels:
    app: jenkins
spec:
  selector:
    matchLabels:
      app: jenkins
  replicas: 1
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      serviceAccountName: jenkins-admin
      containers:
      - name: jenkins
        image: jenkins/jenkins:2.204.6
        securityContext:                     
          runAsUser: 0
          privileged: true
        ports:
        - name: http
          containerPort: 8080
        - name: jnlp
          containerPort: 50000
        resources:
          limits:
            memory: 2Gi
            cpu: "2000m"
          requests:
            memory: 2Gi
            cpu: "2000m"
        env:
        - name: LIMITS_MEMORY
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
              divisor: 1Mi
        - name: "JAVA_OPTS"
          value: " 
                   -Xmx$(LIMITS_MEMORY)m 
                   -XshowSettings:vm 
                   -Dhudson.slaves.NodeProvisioner.initialDelay=0
                   -Dhudson.slaves.NodeProvisioner.MARGIN=50
                   -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85
                   -Duser.timezone=Asia/Shanghai
                 "    
        - name: "JENKINS_OPTS"
          value: "--prefix=/jenkins"
        volumeMounts:
        - name: data
          mountPath: /var/jenkins_home
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: jenkins


[root@master-1 jenkins]# kubectl create -f jenkins-deployment.yaml -n mydlqcloud
service/jenkins created
deployment.apps/jenkins created

[root@master-1 jenkins]# kubectl get  pod -n mydlqcloud --watch
NAME                       READY   STATUS              RESTARTS   AGE
jenkins-585687fb8b-dbnmc   0/1     ContainerCreating   0          19s

[root@master-1 jenkins]# kubectl get  pod -n mydlqcloud 
NAME                       READY   STATUS    RESTARTS   AGE
jenkins-585687fb8b-dbnmc   1/1     Running   0          2m21s



####5、获取 Jenkins 生成的 Token
在安装 Jenkins 时候，它默认生成一段随机字符串在控制台日志中，用于安装时验证。这里需要获取它输出在控制台中的日志信息，来获取 Token 字符串。
查看 Jenkins Pod 启动日志
-n：指定应用启动的 namespace

在日志中可以看到，默认给的token为：
[root@master-1 jenkins]# kubectl logs $(kubectl get pods -n mydlqcloud | awk '{print $1}' | grep jenkins) -n mydlqcloud
VM settings:
    Max. Heap Size: 1.00G
    Ergonomics Machine Class: server
    Using VM: OpenJDK 64-Bit Server VM

Running from: /usr/share/jenkins/jenkins.war
webroot: EnvVars.masterEnvVars.get("JENKINS_HOME")
2021-11-20 13:49:06.943+0000 [id=1]     INFO    org.eclipse.jetty.util.log.Log#initialized: Logging initialized @2098ms to org.eclipse.jetty.util.log.JavaUtilLog
2021-11-20 13:49:07.933+0000 [id=1]     INFO    winstone.Logger#logInternal: Beginning extraction from war file
2021-11-20 13:49:31.639+0000 [id=1]     INFO    org.eclipse.jetty.server.Server#doStart: jetty-9.4.z-SNAPSHOT; built: 2019-05-02T00:04:53.875Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 1.8.0_242-b08
2021-11-20 13:49:34.402+0000 [id=1]     INFO    o.e.j.w.StandardDescriptorProcessor#visitServlet: NO JSP Support for /jenkins, did not find org.eclipse.jetty.jsp.JettyJspServlet
2021-11-20 13:49:34.755+0000 [id=1]     INFO    o.e.j.s.s.DefaultSessionIdManager#doStart: DefaultSessionIdManager workerName=node0
2021-11-20 13:49:34.756+0000 [id=1]     INFO    o.e.j.s.s.DefaultSessionIdManager#doStart: No SessionScavenger set, using defaults
2021-11-20 13:49:34.763+0000 [id=1]     INFO    o.e.j.server.session.HouseKeeper#startScavenging: node0 Scavenging every 600000ms
2021-11-20 13:49:37.231+0000 [id=1]     INFO    hudson.WebAppMain#contextInitialized: Jenkins home directory: /var/jenkins_home found at: EnvVars.masterEnvVars.get("JENKINS_HOME")
2021-11-20 13:49:38.003+0000 [id=1]     INFO    o.e.j.s.handler.ContextHandler#doStart: Started w.@759fad4{Jenkins v2.204.6,/jenkins,file:///var/jenkins_home/war/,AVAILABLE}{/var/jenkins_home/war}
2021-11-20 13:49:38.116+0000 [id=1]     INFO    o.e.j.server.AbstractConnector#doStart: Started ServerConnector@19932c16{HTTP/1.1,[http/1.1]}{0.0.0.0:8080}
2021-11-20 13:49:38.116+0000 [id=1]     INFO    org.eclipse.jetty.server.Server#doStart: Started @33273ms
2021-11-20 13:49:38.121+0000 [id=20]    INFO    winstone.Logger#logInternal: Winstone Servlet Engine v4.0 running: controlPort=disabled
2021-11-20 13:49:43.243+0000 [id=26]    INFO    jenkins.InitReactorRunner$1#onAttained: Started initialization
2021-11-20 13:49:43.587+0000 [id=26]    INFO    jenkins.InitReactorRunner$1#onAttained: Listed all plugins
2021-11-20 13:49:51.846+0000 [id=25]    INFO    jenkins.InitReactorRunner$1#onAttained: Prepared all plugins
2021-11-20 13:49:51.925+0000 [id=25]    INFO    jenkins.InitReactorRunner$1#onAttained: Started all plugins
2021-11-20 13:49:52.102+0000 [id=26]    INFO    jenkins.InitReactorRunner$1#onAttained: Augmented all extensions
2021-11-20 13:49:56.414+0000 [id=26]    INFO    jenkins.InitReactorRunner$1#onAttained: Loaded all jobs
2021-11-20 13:49:58.156+0000 [id=39]    INFO    hudson.model.AsyncPeriodicWork#lambda$doRun$0: Started Download metadata
2021-11-20 13:49:58.401+0000 [id=39]    INFO    hudson.util.Retrier#start: Attempt #1 to do the action check updates server
2021-11-20 13:50:02.136+0000 [id=26]    INFO    o.s.c.s.AbstractApplicationContext#prepareRefresh: Refreshing org.springframework.web.context.support.StaticWebApplicationContext@698f8fdf: display name [Root WebApplicationContext]; startup date [Sat Nov 20 21:50:02 CST 2021]; root of context hierarchy
2021-11-20 13:50:02.137+0000 [id=26]    INFO    o.s.c.s.AbstractApplicationContext#obtainFreshBeanFactory: Bean factory for application context [org.springframework.web.context.support.StaticWebApplicationContext@698f8fdf]: org.springframework.beans.factory.support.DefaultListableBeanFactory@1ec3ee94
2021-11-20 13:50:02.232+0000 [id=26]    INFO    o.s.b.f.s.DefaultListableBeanFactory#preInstantiateSingletons: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@1ec3ee94: defining beans [authenticationManager]; root of factory hierarchy
2021-11-20 13:50:03.726+0000 [id=26]    INFO    o.s.c.s.AbstractApplicationContext#prepareRefresh: Refreshing org.springframework.web.context.support.StaticWebApplicationContext@6bd66070: display name [Root WebApplicationContext]; startup date [Sat Nov 20 21:50:03 CST 2021]; root of context hierarchy
2021-11-20 13:50:03.727+0000 [id=26]    INFO    o.s.c.s.AbstractApplicationContext#obtainFreshBeanFactory: Bean factory for application context [org.springframework.web.context.support.StaticWebApplicationContext@6bd66070]: org.springframework.beans.factory.support.DefaultListableBeanFactory@4a921220
2021-11-20 13:50:03.727+0000 [id=26]    INFO    o.s.b.f.s.DefaultListableBeanFactory#preInstantiateSingletons: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4a921220: defining beans [filter,legacy]; root of factory hierarchy
2021-11-20 13:50:06.309+0000 [id=26]    INFO    jenkins.install.SetupWizard#init: 

*************************************************************
*************************************************************
*************************************************************

Jenkins initial setup is required. An admin user has been created and a password generated.
Please use the following password to proceed to installation:

7d1041959cc54589b695c525f62776b9

This may also be found at: /var/jenkins_home/secrets/initialAdminPassword

*************************************************************
*************************************************************
*************************************************************

[root@master-1 jenkins]# 

#######查看初始密码
kubectl exec -it -n ci jenkins-585687fb8b-dbnmc – bash
more /var/jenkins_home/secrets/initialAdminPassword




####6、启动 Jenkins 进行初始化
输入 Kubernetes 集群地址和 Jenkins Service 设置的 NodePort 端口号，访问 Jenkins UI 界面进行初始化，按以下步骤执行：
[root@master-1 jenkins]# kubectl get svc -n mydlqcloud
NAME      TYPE       CLUSTER-IP   EXTERNAL-IP   PORT(S)                          AGE
jenkins   NodePort   10.1.68.67   <none>        8080:32001/TCP,50000:32002/TCP   4m42s


输入 Kubernetes 集群地址和上面设置的 Nodeport 方式的端口号 32001，然后输入上面获取的 Token 字符串。例如，本人 Kubernetes 集群 IP 为 172.16.201.134 ，所以就可以访问地址 http://172.16.201.134:32001/jenkins 进入 Jenkins 初始化界面。




#(十五)、k8s部署 LNMP

[root@master-1 nfs]# vim  /etc/exports
/nfs *(rw,no_root_squash,no_all_squash,sync)


[root@master-1 nfs]# mkdir -p /nfs/pv{1..5} 
[root@master-1 nfs]# ll
total 0
drwxr-xr-x 3 root root 21 Nov 23 02:34 data
drwxr-xr-x 2 root root  6 Nov 23 17:14 pv1
drwxr-xr-x 2 root root  6 Nov 23 17:14 pv2
drwxr-xr-x 2 root root  6 Nov 23 17:14 pv3
drwxr-xr-x 2 root root  6 Nov 23 17:14 pv4
drwxr-xr-x 2 root root  6 Nov 23 17:20 pv5

[root@master-1 gitlab]# exportfs -r
[root@master-1 gitlab]# exportfs
/nfs_dir/vol1   172.16.201.0/24
/nfs_dir/vol2   172.16.201.0/24
/nfs_dir/vol3   172.16.201.0/24
/nfs_dir/vol4   172.16.201.0/24


[root@node-1 ~]#  mount -t nfs 172.16.201.134:/nfs  /nfs
[root@node-1 ~]# df -h|grep 172.16.201.134
172.16.201.134:/nfs_dir/vol1       17G  9.3G  7.8G  55% /nfs_dir/vol1
172.16.201.134:/nfs_dir/vol2       17G  9.3G  7.8G  55% /nfs_dir/vol2
172.16.201.134:/nfs_dir/vol3       17G  9.3G  7.8G  55% /nfs_dir/vol3
172.16.201.134:/nfs_dir/vol4       17G  9.3G  7.8G  55% /nfs_dir/vol4
172.16.201.134:/nfs/data/jenkins   17G  9.3G  7.8G  55% /nfs/data/jenkins
172.16.201.134:/nfs                17G  9.3G  7.8G  55% /nfs

[root@node-2 ~]#  mount -t nfs 172.16.201.134:/nfs  /nfs
[root@node-2 ~]# df -h|grep 172.16.201.134
172.16.201.134:/nfs_dir/vol1       17G  9.3G  7.8G  55% /nfs_dir/vol1
172.16.201.134:/nfs_dir/vol2       17G  9.3G  7.8G  55% /nfs_dir/vol2
172.16.201.134:/nfs_dir/vol3       17G  9.3G  7.8G  55% /nfs_dir/vol3
172.16.201.134:/nfs_dir/vol4       17G  9.3G  7.8G  55% /nfs_dir/vol4
172.16.201.134:/nfs/data/jenkins   17G  9.3G  7.8G  55% /nfs/data/jenkins
172.16.201.134:/nfs                17G  9.3G  7.8G  55% /nfs



[root@master-1 lnmp]# vim pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001
spec:
  capacity:            
    storage: 1Gi
  volumeMode: Filesystem  
  accessModes:           
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs                
  nfs:
    path: /nfs/pv1
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0002
spec:
  capacity:            
    storage: 2Gi
  volumeMode: Filesystem  
  accessModes:           
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs
  nfs:
    path: /nfs/pv2
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:            
    storage: 3Gi
  volumeMode: Filesystem  
  accessModes:           
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-mysql
  nfs:
    path: /nfs/pv3
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0004
spec:
  capacity:            
    storage: 5Gi
  volumeMode: Filesystem  
  accessModes:           
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-nginx
  nfs:
    path: /nfs/pv4
    server: 172.16.201.134
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0005
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  storageClassName: nfs-php
  nfs:
    path: /nfs/pv5
    server: 172.16.201.134


[root@master-1 lnmp]# kubectl apply -f pv.yaml
persistentvolume/pv0001 created
persistentvolume/pv0002 created
persistentvolume/pv0003 created
persistentvolume/pv0004 created
persistentvolume/pv0005 created

[root@master-1 lnmp]# kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv0001   1Gi        RWX            Retain           Available           nfs                     13s
pv0002   2Gi        RWX            Retain           Available           nfs                     13s
pv0003   3Gi        RWX            Retain           Available           nfs-mysql               13s
pv0004   5Gi        RWX            Retain           Available           nfs-nginx               13s
pv0005   5Gi        RWX            Retain           Available           nfs-php                 13s
[root@master-1 lnmp]# 


[root@master-1 lnmp]# vim  storageclass.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-nginx
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-php
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: nfs-mysql
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"

[root@master-1 lnmp]# kubectl apply -f storageclass.yaml 
storageclass.storage.k8s.io/nfs created
storageclass.storage.k8s.io/nfs-nginx created
storageclass.storage.k8s.io/nfs-php created
storageclass.storage.k8s.io/nfs-mysql created

[root@master-1 lnmp]# kubectl get storageclass
NAME        PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
nfs         kubernetes.io/nfs   Retain          Immediate           false                  14s
nfs-mysql   kubernetes.io/nfs   Retain          Immediate           false                  14s
nfs-nginx   kubernetes.io/nfs   Retain          Immediate           false                  14s
nfs-php     kubernetes.io/nfs   Retain          Immediate           false                  14s


[root@master-1 lnmp]# kubectl create secret generic mysql-pass --from-literal=password=123456
secret/mysql-pass created

[root@master-1 lnmp]# kubectl get secret
NAME                  TYPE                                  DATA   AGE
db-user-pass          Opaque                                2      3d1h
default-token-q94x6   kubernetes.io/service-account-token   3      4d5h
mysql-pass            Opaque                                1      17s
[root@master-1 lnmp]# 

[root@master-1 lnmp]# vim mysql-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  name: blog-mysql
  labels:
    app: blog-mysql
spec:
  ports:
    - port: 3306
  selector:
    app: blog-mysql
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
spec:
  storageClassName: nfs-mysql
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 3Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blog-mysql
spec:
  selector:
    matchLabels:
      app: blog-mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: blog-mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.6
        imagePullPolicy: IfNotPresent
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports: 
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim

[root@master-1 lnmp]# kubectl apply -f mysql-deployment.yaml
service/blog-mysql created
persistentvolumeclaim/mysql-pv-claim created
deployment.apps/blog-mysql created

[root@master-1 lnmp]# kubectl get pods
NAME                          READY   STATUS    RESTARTS   AGE
blog-mysql-5cb7fd64f8-mx9x9   1/1     Running   0          6s


[root@master-1 lnmp]# vim nginx-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-wp-config
data:
  default.conf: |-
    server {
        listen 80;
        server_name localhost;
        root /usr/local/nginx/html;
        index index.html index.php;
 
        location ~ \.php$ {
            root /usr/local/nginx/html;
            fastcgi_pass blog-php:9000;#这里名称为blog-php svc名称
            fastcgi_param SCRIPT_FILENAME /usr/local/nginx/html$fastcgi_script_name;
            include fastcgi_params;
            fastcgi_connect_timeout 60s;
            fastcgi_read_timeout 300s;
            fastcgi_send_timeout 300s;
        }
    }
---
apiVersion: v1
kind: Service
metadata:
  name: blog-nginx
  labels:
    app: nginx
spec:
  ports:
    - port: 80
  selector:
    app: blog-nginx
    tier: frontend
  type: NodePort
  sessionAffinity: ClientIP
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nginx-pv-claim
spec:
  storageClassName: nfs-nginx
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blog-nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blog-nginx
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: blog-nginx
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: nginx
        volumeMounts:
        - name: nginx-persistent-storage
          mountPath: /usr/local/nginx/html
        - name: config
          mountPath: /etc/nginx/conf.d/default.conf
          subPath: default.conf
      volumes:
      - name: nginx-persistent-storage
        persistentVolumeClaim:
          claimName: nginx-pv-claim
      - name: config
        configMap:
          name: nginx-wp-config



[root@master-1 lnmp]# kubectl apply -f  nginx-deployment.yaml 
configmap/nginx-wp-config created
service/blog-nginx created
persistentvolumeclaim/nginx-pv-claim created
deployment.apps/blog-nginx created


[root@master-1 lnmp]# kubectl get pod 
NAME                          READY   STATUS    RESTARTS   AGE
blog-mysql-5cb7fd64f8-mx9x9   1/1     Running   0          45m
blog-nginx-5545fc6957-64dzg   1/1     Running   8          16m
blog-php-6cc8557468-zcm9h     1/1     Running   0          78s

#####问题：等php 建Running 之后，nginx 就不报错了。
[root@node-2 ~]# docker logs --tail="100"  75dad5697fa6
2021/11/21 05:04:01 [emerg] 1#1: host not found in upstream "blog-php" in /etc/nginx/conf.d/default.conf:9
nginx: [emerg] host not found in upstream "blog-php" in /etc/nginx/conf.d/default.conf:9

先kubectl apply -f nginx-deployment.yaml ，会出现host not found in upstream "blog-php"  找不到的情况。
#####问题：等php 建Running 之后，nginx 就不报错了。

kubectl delete pv pv0004
kubectl apply -f  pv0004.yaml


####
[root@master-1 v0]# vim index.php
<?php

phpinfo();

?>


[root@master-1 v0]# vim Dockerfile
FROM php:7.4-fpm
WORKDIR /app
COPY index.php /app

[root@master-1 v0]# docker build . -t php:0.1
Sending build context to Docker daemon  4.608kB
Step 1/3 : FROM php:7.4-fpm
 ---> 3788a3c06032
Step 2/3 : WORKDIR /app
 ---> Running in dba9ed8390e8
Removing intermediate container dba9ed8390e8
 ---> e11adf847ac2
Step 3/3 : COPY index.php /app
 ---> a6aa6610dc62
Successfully built a6aa6610dc62
Successfully tagged php:0.1
[root@master-1 v0]# 


[root@master-1 v0]# docker images 
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
php                                                               0.1                 a6aa6610dc62        21 seconds ago      460MB
php                                                               7.4-fpm             3788a3c06032        6 days ago          460MB



[root@master-1 lnmp]# vim php-deployment.yaml

apiVersion: v1
kind: Service
metadata:
  name: blog-php
  labels:
    app: php
spec:
  ports:
    - port: 9000
  selector:
    app: blog-php
    tier: frontend
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: php-pv-claim
spec:
  storageClassName: nfs-php
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: blog-php
  labels:
    app: php
spec:
  replicas: 1
  selector:
    matchLabels:
      app: blog-php
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: blog-php
        tier: frontend
    spec:
      containers:
      - name: php
        image: php:7.1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9000
          name: php
        volumeMounts:
        - name: php-persistent-storage
          mountPath: /usr/local/nginx/html
      imagePullSecrets:
      - name: regsecret
      volumes:
      - name: php-persistent-storage
        persistentVolumeClaim:
          claimName: php-pv-claim

[root@master-1 lnmp]# kubectl apply -f php-deployment.yaml
service/blog-php created
persistentvolumeclaim/php-pv-claim created
deployment.apps/blog-php created


[root@master-1 lnmp]# kubectl get pod 
NAME                          READY   STATUS    RESTARTS   AGE
blog-mysql-5cb7fd64f8-mx9x9   1/1     Running   0          45m
blog-nginx-5545fc6957-64dzg   1/1     Running   8          16m
blog-php-6cc8557468-zcm9h     1/1     Running   0          78s

[root@master-1 lnmp]# kubectl get pod  
NAME                          READY   STATUS    RESTARTS   AGE
blog-mysql-5cb7fd64f8-c844g   1/1     Running   0          7m19s
wordpress-5994d99f46-dx7m7    1/1     Running   0          70s




[root@master-1 v1]# kubectl get pv,pvc,svc,pod -o wide     
NAME                      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                    STORAGECLASS   REASON   AGE   VOLUMEMODE
persistentvolume/pv0001   1Gi        RWX            Retain           Available                            nfs                     88m   Filesystem
persistentvolume/pv0002   2Gi        RWX            Retain           Available                            nfs                     88m   Filesystem
persistentvolume/pv0003   3Gi        RWX            Retain           Bound       default/mysql-pv-claim   nfs-mysql               88m   Filesystem
persistentvolume/pv0004   5Gi        RWX            Retain           Bound       default/nginx-pv-claim   nfs-nginx               31m   Filesystem
persistentvolume/pv0005   5Gi        RWX            Retain           Bound       default/php-pv-claim     nfs-php                 43m   Filesystem

NAME                                   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE
persistentvolumeclaim/mysql-pv-claim   Bound    pv0003   3Gi        RWX            nfs-mysql      39m   Filesystem
persistentvolumeclaim/nginx-pv-claim   Bound    pv0004   5Gi        RWX            nfs-nginx      31m   Filesystem
persistentvolumeclaim/php-pv-claim     Bound    pv0005   5Gi        RWX            nfs-php        42m   Filesystem

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR
service/blog-mysql   ClusterIP   10.1.71.85     <none>        3306/TCP       39m   app=blog-mysql
service/blog-nginx   NodePort    10.1.112.104   <none>        80:30189/TCP   31m   app=blog-nginx,tier=frontend
service/blog-php     ClusterIP   10.1.208.199   <none>        9000/TCP       42m   app=blog-php,tier=frontend
service/kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP        11h   <none>

NAME                              READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
pod/blog-mysql-5cb7fd64f8-qlksz   1/1     Running   0          39m   10.244.2.11   node-2   <none>           <none>
pod/blog-nginx-5545fc6957-lcfrq   1/1     Running   0          31m   10.244.1.13   node-1   <none>           <none>
pod/blog-php-5cbddf56b7-brvss     1/1     Running   0          42m   10.244.2.10   node-2   <none>           <none>
[root@master-1 v1]# 



[root@master-1 v1]# kubectl exec -it blog-nginx-5545fc6957-lcfrq  --  cat /etc/nginx/conf.d/default.conf 
server {
    listen 80;
    server_name localhost;
    root /usr/local/nginx/html;
    index index.html index.php;

    location ~ \.php$ {
        root /usr/local/nginx/html;
        fastcgi_pass blog-php:9000;#这里名称为blog-php svc名称
        fastcgi_param SCRIPT_FILENAME /usr/local/nginx/html$fastcgi_script_name;
        include fastcgi_params;
        fastcgi_connect_timeout 60s;
        fastcgi_read_timeout 300s;
        fastcgi_send_timeout 300s;
    }
}[root@master-1 v1]# 



[root@master-1 v1]# kubectl exec -it blog-php-5cbddf56b7-brvss -- php -v
PHP 7.4.26 (cli) (built: Nov 18 2021 16:23:37) ( NTS )
Copyright (c) The PHP Group
Zend Engine v3.4.0, Copyright (c) Zend Technologies


http://172.16.201.134:30189/index.php



[root@master-1 pv4]# cd /nfs/pv4
[root@master-1 pv4]# wget https://wordpress.org/latest.tar.gz
--2021-11-25 04:01:59--  https://wordpress.org/latest.tar.gz
Resolving wordpress.org (wordpress.org)... 198.143.164.252
Connecting to wordpress.org (wordpress.org)|198.143.164.252|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 15085301 (14M) [application/octet-stream]
Saving to: ‘latest.tar.gz’

100%[==================================================================================================================================>] 15,085,301  1.70MB/s   in 8.5s   

2021-11-25 04:02:12 (1.70 MB/s) - ‘latest.tar.gz’ saved [15085301/15085301]

[root@master-1 pv4]#tar -xvf latest.tar.gz
[root@master-1 pv4]#cp -rp wordpress/* ../pv5/
[root@master-1 pv4]# chown -R nobody /nfs/pv4/


[root@master-1 v1]# kubectl exec -it blog-mysql-5cb7fd64f8-qlksz  -- bash
root@blog-mysql-5cb7fd64f8-qlksz:/# 
root@blog-mysql-5cb7fd64f8-qlksz:/# mysql -uroot -p123456
Warning: Using a password on the command line interface can be insecure.
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 1
Server version: 5.6.51 MySQL Community Server (GPL)

Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> create database wp;
Query OK, 1 row affected (0.01 sec)

mysql> grant all on wp.* to 'root'@'%' identified by '123456';
Query OK, 0 rows affected (0.01 sec)

mysql> FLUSH PRIVILEGES;
Query OK, 0 rows affected (0.00 sec)

mysql> 


http://172.16.201.134:30189/wp-admin/setup-config.php
WordPress
Before getting started

Welcome to WordPress. Before getting started, we need some information on the database. You will need to know the following items before proceeding.

    Database name
    Database username
    Database password
    Database host
    Table prefix (if you want to run more than one WordPress in a single database)

We’re going to use this information to create a wp-config.php file. If for any reason this automatic file creation doesn’t work, don’t worry. All this does is fill in the database information to a configuration file. You may also simply open wp-config-sample.php in a text editor, fill in your information, and save it as wp-config.php. Need more help? We got it.

In all likelihood, these items were supplied to you by your Web Host. If you don’t have this information, then you will need to contact them before you can continue. If you’re all ready…

Let’s go!

点Let’s go!

WordPress

Set up your database connection
Below you should enter your database connection details. If you’re not sure about these, contact your host.

Database Name	
wp
The name of the database you want to use with WordPress.
Username	
root
Your database username.
Password	
123456
Your database password.
Database Host	
blog-mysql
You should be able to get this info from your web host, if localhost doesn’t work.
Table Prefix	
wp_
If you want to run multiple WordPress installations in a single database, change this.

数据库信息参考上面。

wp
root
123456
blog-mysql




kubectl apply -f php-deployment.yaml
kubectl delete -f php-deployment.yaml
docker logs --tail="100"  96b86bf51c03

kubectl delete pv pv0003
kubectl delete pv pv0004
kubectl delete pv pv0005
kubectl apply -f pv.yaml 
kubectl get pv,pvc,svc,pod -o wide


kubectl apply -f 0003.yaml 
kubectl apply -f 0004.yaml 
kubectl apply -f 0005.yaml 

kubectl describe pod blog-php-65bd88456b-ztd25


kubectl get event
kubectl get event
kubectl exec -it blog-nginx-5545fc6957-lcfrq  --  cat /etc/nginx/conf.d/default.conf 
kubectl exec -it blog-nginx-5545fc6957-lcfrq  -- bash
kubectl get pods -o wide
kubectl get pv,pvc -o wide
kubectl get svc
kubectl exec -it blog-mysql-5cb7fd64f8-qlksz  -- bash

kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD
kubectl create -f https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
kubectl create -f https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml



安装php-pdo扩展(更好的办法是你应该制作一个运行php环境的容器)
kubectl exec -it blog-php-5cbddf56b7-brvss -- /usr/local/bin/docker-php-ext-install pdo_mysql
kubectl exec -it blog-php-5cbddf56b7-dhtth -- /usr/local/bin/docker-php-ext-install php_mysqli
kubectl exec -it blog-php-5cbddf56b7-dhtth  -- php -m
kubectl exec -it blog-nginx-5545fc6957-nw4qm  -- bash

#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

最小化安装php
[root@master-1 v0]# vim index.php
<?php

phpinfo();

?>


[root@master-1 v0]# vim Dockerfile
FROM php:7.4-fpm
WORKDIR /app
COPY index.php /app
RUN /usr/local/bin/docker-php-ext-install pdo_mysql
RUN /usr/local/bin/docker-php-ext-install php_mysqli

[root@master-1 v0]# docker build . -t php:0.1
Sending build context to Docker daemon  4.608kB
Step 1/3 : FROM php:7.4-fpm
 ---> 3788a3c06032
Step 2/3 : WORKDIR /app
 ---> Running in dba9ed8390e8
Removing intermediate container dba9ed8390e8
 ---> e11adf847ac2
Step 3/3 : COPY index.php /app
 ---> a6aa6610dc62
Successfully built a6aa6610dc62
Successfully tagged php:0.1
[root@master-1 v0]# 


[root@master-1 v0]# docker images 
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
php                                                               0.1                 a6aa6610dc62        21 seconds ago      460MB
php                                                               7.4-fpm             3788a3c06032        6 days ago          460MB


#######因为没有仓库，需要把php:0.1 ，倒入到所有节点。


[root@master-1 v0]# vim php-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-server
  labels:
    name: php-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: php-server
  template:
    metadata:
      labels:
        app: php-server
    spec:
      containers:
      - name: php-server
        image: php:0.1
        volumeMounts:
        - mountPath: /var/www/html/
          name: nginx-data
        ports:
        - containerPort: 9000
      volumes:
      - name: nginx-data
        hostPath:
         path: /root/k8s/html
---
apiVersion: v1
kind: Service
metadata:
  name: php
spec:
  ports:
  - name: php
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app: php-server



[root@master-1 v0]kubectl apply -f php-deployment.yaml  
deployment.apps/php-server created
service/php created

[root@master-1 v0]# kubectl get pods -o wide 
NAME                               READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES
php-server-75d49d4444-2gk5j        1/1     Running   0          19s   10.244.1.10   node-1   <none>           <none>
wordpress-mysql-76468dd4bb-h6sr4   1/1     Running   0          8h    10.244.2.8    node-2   <none>           <none>


[root@master-1 v0]# kubectl exec -it php-server-75d49d4444-2gk5j  -- bash
root@php-server-75d49d4444-2gk5j:/app# 
root@php-server-75d49d4444-2gk5j:/app# ls -al
total 4
drwxr-xr-x 1 root root 23 Nov 24 18:21 .
drwxr-xr-x 1 root root 17 Nov 24 18:23 ..
-rw-r--r-- 1 root root 24 Nov 24 18:20 index.php

root@php-server-75d49d4444-2gk5j:/# php -v
PHP 7.4.26 (cli) (built: Nov 18 2021 16:23:37) ( NTS )
Copyright (c) The PHP Group
Zend Engine v3.4.0, Copyright (c) Zend Technologies
root@php-server-75d49d4444-2gk5j:/# 


用v1版本的nginx 试试
[root@master-1 v1]# kubectl get pod 
NAME                               READY   STATUS    RESTARTS   AGE
blog-nginx-5545fc6957-72c6b        1/1     Running   0          70s
php-server-75d49d4444-2gk5j        1/1     Running   0          9m11s
wordpress-mysql-76468dd4bb-h6sr4   1/1     Running   0          8h



[root@master-1 v0]# kubectl exec -it blog-nginx-5545fc6957-72c6b  -- bash

http://172.16.201.134:31447/



二、检查网站日志
使用FTP登录到的站点并检查错误日志。如果那没有帮助，请尝试启用调试模式。
启用调试的方法：使用FTP登录到的站点，然后 在文本编辑器中从WordPress根文件夹中打开wp-config.php文件。找到以下几行
true
define( 'WP_DEBUG', false ); 
define( 'WP_DEBUG_DISPLAY', false );
define( 'SCRIPT_DEBUG', false );
define( 'WP_DEBUG_LOG', false );
将false值更改为true并保存。现在，当刷新站点时，可能会看到其他错误，这些错误可能使更好地了解错误的来源。也可以通过FTP查看调试日志。完成后，请确保将其重新设置为false。


kubectl exec -it blog-php-5cbddf56b7-brvss   -- php -a


#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
[root@master-1 v0]# vim Dockerfile           
FROM php:7.4-fpm
WORKDIR /app
COPY index.php /app
RUN /usr/local/bin/docker-php-ext-install pdo_mysql
RUN /usr/local/bin/docker-php-ext-install mysqli

[root@master-1 v0]# docker build . -t php:0.1
...............
PATH="$PATH:/sbin" ldconfig -n /usr/src/php/ext/mysqli/modules
----------------------------------------------------------------------
Libraries have been installed in:
   /usr/src/php/ext/mysqli/modules

If you ever happen to want to link against installed libraries
in a given directory, LIBDIR, you must either use libtool, and
specify the full pathname of the library, or use the `-LLIBDIR'
flag during linking and do at least one of the following:
   - add LIBDIR to the `LD_LIBRARY_PATH' environment variable
     during execution
   - add LIBDIR to the `LD_RUN_PATH' environment variable
     during linking
   - use the `-Wl,--rpath -Wl,LIBDIR' linker flag
   - have your system administrator add LIBDIR to `/etc/ld.so.conf'

See any operating system documentation about shared libraries for
more information, such as the ld(1) and ld.so(8) manual pages.
----------------------------------------------------------------------

Build complete.
Don't forget to run 'make test'.

+ strip --strip-all modules/mysqli.so
Installing shared extensions:     /usr/local/lib/php/extensions/no-debug-non-zts-20190902/
Installing header files:          /usr/local/include/php/
find . -name \*.gcno -o -name \*.gcda | xargs rm -f
find . -name \*.lo -o -name \*.o | xargs rm -f
find . -name \*.la -o -name \*.a | xargs rm -f
find . -name \*.so | xargs rm -f
find . -name .libs -a -type d|xargs rm -rf
rm -f libphp.la      modules/* libs/*
Removing intermediate container d51e033928f1
 ---> 28a7fc339081
Successfully built 28a7fc339081
Successfully tagged php:0.1






[root@master-1 v2]# cat configmap.yaml
kind: ConfigMap # 对象类型
apiVersion: v1 # api 版本
metadata: # 元数据
  name: nginx-config # 对象名称
data: # key-value 数据集合
  nginx.conf: | # 将 nginx config 配置写入 ConfigMap 中，经典的 php-fpm 代理设置，这里就不再多说了
    events {
    }
    http {
      server {
        listen 80 default_server;
        listen [::]:80 default_server;
        root /var/www/html;
        index index.php;
        server_name _;
        location / {
          try_files $uri $uri/ =404;
        }
        location ~ \.php$ {
          include fastcgi_params;
          fastcgi_param REQUEST_METHOD $request_method;
          fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;
          fastcgi_pass 127.0.0.1:9000;
        }
      }
    }
[root@master-1 v2]# 


[root@master-1 v2]# cat php-fpm-nginx-deployment-and-service.yaml
kind: Deployment # 对象类型
apiVersion: apps/v1 # api 版本
metadata: # 元数据
  name: php-fpm-nginx # Deployment 对象名称
spec: # Deployment 对象规约
  selector: # 选择器
    matchLabels: # 标签匹配
      app: php-fpm-nginx
  replicas: 1 # 副本数量
  template: # 模版
    metadata: # Pod 对象的元数据
      labels: # Pod 对象的标签
        app: php-fpm-nginx
    spec: # Pod 对象规约
      containers: # 这里设置了两个容器
        - name: php-fpm # 第一个容器名称
          image: php:0.1 # 容器镜像
          ports:
            - containerPort: 9000 # php-fpm 端口
          volumeMounts: # 挂载数据卷
            - mountPath: /var/www/html # 挂载两个容器共享的 volume 
              name: nginx-www
          lifecycle: # 生命周期
            postStart: # 当容器处于 postStart 阶段时，执行一下命令
              exec:
                command: ["/bin/sh", "-c", "cp -r /app/. /var/www/html"] # 将 /app/index.php 复制到挂载的 volume 
        - name: nginx # 第二个容器名称
          image: nginx # 容器镜像
          ports:
            - containerPort: 80 # nginx 端口
          volumeMounts: # nginx 容器挂载了两个 volume，一个是与 php-fpm 容器共享的 volume，另外一个是配置了 nginx.conf 的 volume
            - mountPath: /var/www/html # 挂载两个容器共享的 volume 
              name: nginx-www
            - mountPath: /etc/nginx/nginx.conf #  挂载配置了 nginx.conf 的 volume
              subPath: nginx.conf
              name: nginx-config
      volumes:
        - name: nginx-www # 这个 volume 是 php-fpm 容器 和 nginx 容器所共享的，两个容器都 volumeMounts 了
          emptyDir: {}
        - name: nginx-config 
          configMap: # 有人好奇，这里为啥可以将 configMap 对象通过 volumeMounts 的方式注入到容器中呢，因为本质上 configMap 是一类特殊的 volume
            name: nginx-config
---
kind: Service # 对象类型
apiVersion: v1 # api 版本
metadata: # 元数据
  name: php-fpm-nginx
spec:
  selector:
    app: php-fpm-nginx
  ports:
    - port: 80 
      targetPort: 80 # Service 将 nginx 容器的 80 端口暴露出来
  type: NodePort
[root@master-1 v2]# 



[root@master-1 v2]# kubectl apply -f configmap.yaml # 配置对象，本示例存放 nginx.config
[root@master-1 v2]# kubectl apply -f php-fpm-nginx-deployment-and-service.yaml
kubectl delete -f php-fpm-nginx-deployment-and-service.yaml



[root@master-1 v2]# kubectl get pod  
NAME                             READY   STATUS    RESTARTS   AGE
blog-mysql-5cb7fd64f8-qlksz      1/1     Running   0          6h53m
php-fpm-nginx-5fdcb6c57d-7v2pb   2/2     Running   0          62s

kubectl exec -it php-fpm-nginx-5fdcb6c57d-7v2pb  -- bash

用之前v1版本的数据库blog-mysql，内部访问直接用blog-mysql名字即可


查找代码路径：节点在node-1上

[root@node-1 wordpress]#  docker ps -a|grep php
c4b2f6c24a04        nginx                                               "/docker-entrypoint.…"   11 minutes ago      Up 11 minutes                                 k8s_nginx_php-fpm-nginx-5fdcb6c57d-jjtgr_default_2bedd26d-96ba-48ab-ad99-484b32a3bdf4_0
a469d1feccb9        3f4e80124011                                        "docker-php-entrypoi…"   12 minutes ago      Up 12 minutes                                 k8s_php-fpm_php-fpm-nginx-5fdcb6c57d-jjtgr_default_2bedd26d-96ba-48ab-ad99-484b32a3bdf4_0
9e80ecebfd33        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 12 minutes ago      Up 12 minutes                                 k8s_POD_php-fpm-nginx-5fdcb6c57d-jjtgr_default_2bedd26d-96ba-48ab-ad99-484b32a3bdf4_0


直接进本地目录：
[root@node-1 nginx-www]# docker inspect fa9dd6e943c1|grep nginx-www
                "/var/lib/kubelet/pods/46e72359-61d8-4540-a834-80c16244b12f/volumes/kubernetes.io~empty-dir/nginx-www:/var/www/html",
                "Source": "/var/lib/kubelet/pods/46e72359-61d8-4540-a834-80c16244b12f/volumes/kubernetes.io~empty-dir/nginx-www",
[root@node-1 nginx-www]# 



[root@node-1 nginx-www]#cd /var/lib/kubelet/pods/46e72359-61d8-4540-a834-80c16244b12f/volumes/kubernetes.io~empty-dir/nginx-www/
[root@node-1 nginx-www]# ll
total 8
-rw-r--r-- 1 root root  3 Nov 25 11:45 index.html
-rw-r--r-- 1 root root 24 Nov 25 10:11 index.php

复制代码：
[root@node-1 nginx-www]#cp /nfs/wordpress-5.4-zh_CN.zip /var/lib/kubelet/pods/2bedd26d-96ba-48ab-ad99-484b32a3bdf4/volumes/kubernetes.io~empty-dir/nginx-www/

[root@node-1 nginx-www]#cd /var/lib/kubelet/pods/2bedd26d-96ba-48ab-ad99-484b32a3bdf4/volumes/kubernetes.io~empty-dir/nginx-www/
[root@node-1 nginx-www]#unzip wordpress-5.4-zh_CN.zip

访问：
http://172.16.201.134:32321/wordpress/wp-admin/

中文版：
[root@node-1 nginx-www]# wget https://cn.wordpress.org/latest-zh_CN.zip
--2021-11-25 12:50:30--  https://cn.wordpress.org/latest-zh_CN.zip
Resolving cn.wordpress.org (cn.wordpress.org)... 198.143.164.252
Connecting to cn.wordpress.org (cn.wordpress.org)|198.143.164.252|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 17270810 (16M) [application/zip]
Saving to: ‘latest-zh_CN.zip’

100%[=====================================================================================================================================================>] 17,270,810  2.65MB/s   in 7.0s   

2021-11-25 12:50:40 (2.36 MB/s) - ‘latest-zh_CN.zip’ saved [17270810/17270810]

[root@node-1 nginx-www]# 




数据库信息参考下面。

wp
root
123456
blog-mysql



chmod -R 777 wordpress
admin
oISycS@L4lE#7vEfrX
4gjSLcCDBie!e$iWru
http://172.16.201.134:32321/wordpress/wp-admin/



[root@master-1 v2]# vim server-ingress.yaml
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx

[root@master-1 v2]# kubectl apply -f server-ingress.yaml
service/ingress-nginx created
 # ingress 路由规则
ingress控制器会将所有监听到的ingress-svc(nginx-web)的绑定域名，www.shooter.com 写入到ingress-nginx控制器中,然后重启nginx来实现负载和转发
 
重点：ingress controller 会感知所有namespace的ingress资源文件
 
用户执行
kubectl apply ingress.yaml文件后 
k8s感知到变化就调用 ingress controller 把它翻译成nginx.conf
 
并写到ingress-nginx的pod中，pod的名字默认nginx-ingress-controller开头


[root@master-1 v2]# vim ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-web
spec:
  rules:
  - host: www.shooter.com
    http:
      paths:
      - path: /
        backend:
          serviceName: php-fpm-nginx   #这里对应要代理nginx svc名称
          servicePort: 80

[root@master-1 v2]# kubectl apply -f ingress.yaml
ingress.extensions/nginx-web created
[root@master-1 v2]# 
[root@master-1 v2]# kubectl get ingress
NAME        CLASS    HOSTS             ADDRESS   PORTS   AGE
nginx-web   <none>   www.shooter.com             80      15s

[root@master-1 v2]# kubectl get svc
NAME            TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                      AGE
blog-mysql      ClusterIP   10.1.71.85    <none>        3306/TCP                     10h
ingress-nginx   NodePort    10.1.62.225   <none>        80:31286/TCP,443:31419/TCP   2m39s
kubernetes      ClusterIP   10.1.0.1      <none>        443/TCP                      20h
php-fpm-nginx   NodePort    10.1.45.21    <none>        80:32321/TCP                 76m
[root@master-1 v2]# 


##(十六)、k8s部署 Tomcat

tomcat部署要点总结: 

利用configmap实现对server.xml的挂载
利用tomcat-ingress实现对tomcat对外网络暴露 
利用pv实现对tomcat日志存储
利用statefulset创建tomcat pod。 
利用mt-math.yaml创建mt-math命名空间
注:默认已安装ingress-nginx，core-dns，NFS


1.NFS共享目录下创建tomcat 日志存放目录及项目存放目录;这里手动创建pv，当然pv也可以进行动态创建

mkdir -p /nfs/tomcat-wwwroot/ROOT/;mkdir -p /nfs/pv-tomcat-log/{01,02,03}  #创建日志目录及项目目录
echo "k8s-ingress-tomcat-nfs-pv -Wyltest"> /nfs/tomcat-wwwroot/ROOT/index.jsp #创建测试首页文件jsp


[root@master-1 tomcat]# vim tomcat-configmap.yaml 
apiVersion: v1
data:
  server.xml: |
    <?xml version='1.0' encoding='utf-8'?>
    <!--
      Licensed to the Apache Software Foundation (ASF) under one or more
      contributor license agreements.  See the NOTICE file distributed with
      this work for additional information regarding copyright ownership.
      The ASF licenses this file to You under the Apache License, Version 2.0
      (the "License"); you may not use this file except in compliance with
      the License.  You may obtain a copy of the License at

          http://wangyunlong/licenses/LICENSE-2.0

      Unless required by applicable law or agreed to in writing, software
      distributed under the License is distributed on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
      See the License for the specific language governing permissions and
      limitations under the License.
    -->
    <!-- Note:  A "Server" is not itself a "Container", so you may not
         define subcomponents such as "Valves" at this level.
         Documentation at /docs/config/server.html
     -->
    <Server port="8005" shutdown="SHUTDOWN">
      <Listener className="org.apache.catalina.startup.VersionLoggerListener" />
      <!-- Security listener. Documentation at /docs/config/listeners.html
      <Listener className="org.apache.catalina.security.SecurityListener" />
      -->
      <!--APR library loader. Documentation at /docs/apr.html -->
      <Listener className="org.apache.catalina.core.AprLifecycleListener" SSLEngine="on" />
      <!--Initialize Jasper prior to webapps are loaded. Documentation at /docs/jasper-howto.html -->
      <Listener className="org.apache.catalina.core.JasperListener" />
      <!-- Prevent memory leaks due to use of particular java/javax APIs-->
      <Listener className="org.apache.catalina.core.JreMemoryLeakPreventionListener" />
      <Listener className="org.apache.catalina.mbeans.GlobalResourcesLifecycleListener" />
      <Listener className="org.apache.catalina.core.ThreadLocalLeakPreventionListener" />

      <!-- Global JNDI resources
           Documentation at /docs/jndi-resources-howto.html
      -->
      <GlobalNamingResources>
        <!-- Editable user database that can also be used by
             UserDatabaseRealm to authenticate users
        -->
        <Resource name="UserDatabase" auth="Container"
                  type="org.apache.catalina.UserDatabase"
                  description="User database that can be updated and saved"
                  factory="org.apache.catalina.users.MemoryUserDatabaseFactory"
                  pathname="conf/tomcat-users.xml" />
      </GlobalNamingResources>

      <!-- A "Service" is a collection of one or more "Connectors" that share
           a single "Container" Note:  A "Service" is not itself a "Container",
           so you may not define subcomponents such as "Valves" at this level.
           Documentation at /docs/config/service.html
       -->
      <Service name="Catalina">

        <!--The connectors can use a shared executor, you can define one or more named thread pools-->
        <!--
        <Executor name="tomcatThreadPool" namePrefix="catalina-exec-"
            maxThreads="150" minSpareThreads="4"/>
        -->


        <!-- A "Connector" represents an endpoint by which requests are received
             and responses are returned. Documentation at :
             Java HTTP Connector: /docs/config/http.html (blocking & non-blocking)
             Java AJP  Connector: /docs/config/ajp.html
             APR (HTTP/AJP) Connector: /docs/apr.html
             Define a non-SSL HTTP/1.1 Connector on port 8080
        -->
        <Connector port="8080" protocol="HTTP/1.1"
                   connectionTimeout="20000"
                   redirectPort="8443" />
        <!-- A "Connector" using the shared thread pool-->
        <!--
        <Connector executor="tomcatThreadPool"
                   port="8080" protocol="HTTP/1.1"
                   connectionTimeout="20000"
                   redirectPort="8443" />
        -->
        <!-- Define a SSL HTTP/1.1 Connector on port 8443
             This connector uses the BIO implementation that requires the JSSE
             style configuration. When using the APR/native implementation, the
             OpenSSL style configuration is required as described in the APR/native
             documentation -->
        <!--
        <Connector port="8443" protocol="org.apache.coyote.http11.Http11Protocol"
                   maxThreads="150" SSLEnabled="true" scheme="https" secure="true"
                   clientAuth="false" sslProtocol="TLS" />
        -->

        <!-- Define an AJP 1.3 Connector on port 8009 -->
        <Connector port="8009" protocol="AJP/1.3" redirectPort="8443" />


        <!-- An Engine represents the entry point (within Catalina) that processes
             every request.  The Engine implementation for Tomcat stand alone
             analyzes the HTTP headers included with the request, and passes them
             on to the appropriate Host (virtual host).
             Documentation at /docs/config/engine.html -->

        <!-- You should set jvmRoute to support load-balancing via AJP ie :
        <Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1">
        -->
        <Engine name="Catalina" defaultHost="localhost">

          <!--For clustering, please take a look at documentation at:
              /docs/cluster-howto.html  (simple how to)
              /docs/config/cluster.html (reference documentation) -->
          <!--
          <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster"/>
          -->

          <!-- Use the LockOutRealm to prevent attempts to guess user passwords
               via a brute-force attack -->
          <Realm className="org.apache.catalina.realm.LockOutRealm">
            <!-- This Realm uses the UserDatabase configured in the global JNDI
                 resources under the key "UserDatabase".  Any edits
                 that are performed against this UserDatabase are immediately
                 available for use by the Realm.  -->
            <Realm className="org.apache.catalina.realm.UserDatabaseRealm"
                   resourceName="UserDatabase"/>
          </Realm>

          <Host name="localhost"  appBase="webapps"
                unpackWARs="true" autoDeploy="true">

            <!-- SingleSignOn valve, share authentication between web applications
                 Documentation at: /docs/config/valve.html -->
            <!--
            <Valve className="org.apache.catalina.authenticator.SingleSignOn" />
            -->

            <!-- Access log processes all example.
                 Documentation at: /docs/config/valve.html
                 Note: The pattern used is equivalent to using pattern="common" -->
            <Valve className="org.apache.catalina.valves.AccessLogValve" directory="logs"
                   prefix="localhost_access_log." suffix=".txt"
                   pattern="%h %l %u %t &quot;%r&quot; %s %b" />

          </Host>
        </Engine>
      </Service>
    </Server>
kind: ConfigMap
metadata:
  creationTimestamp: 2019-06-05T03:04:53Z
  name: tomcat-configmap
  namespace: mt-math

[root@master-1 tomcat]# kubectl get configmap -n mt-math
NAME               DATA   AGE
tomcat-configmap   1      29s




[root@master-1 tomcat]# vim tomcat-pv-nfs.yaml 

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-tomcat01                     #创建的pv名称可创建多个.
  namespace: mt-math                    #属于的命名空间
spec:
  capacity:
    storage: 200M                        #创建的pv容量为1G
  accessModes:
  - ReadWriteMany                       #pv的访问模式:可读可写可挂在多个节点
  persistentVolumeReclaimPolicy: Recycle #回收策略
  storageClassName: sc-nfs-tomcat01
  nfs:                                  #创建的pv数据来源
    path: /nfs/pv-tomcat-log/01                     #数据源目录
    server: 172.16.201.134                #数据源ip
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-tomcat02                     #创建的pv名称可创建多个.
  namespace: mt-math                    #属于的命名空间
spec:
  capacity:
    storage: 200M                        #创建的pv容量为1G
  accessModes:
  - ReadWriteMany                       #pv的访问模式:可读可写可挂在多个节点
  persistentVolumeReclaimPolicy: Recycle #回收策略 由于是持久化日志没必要删除pod保留日志。只是为了方便查看日志内容。
  storageClassName: sc-nfs-tomcat02
  nfs:                                  #创建的pv数据来源
    path: /nfs/pv-tomcat-log/02                     #数据源目录
    server: 172.16.201.134                #数据源ip
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-tomcat03                     #创建的pv名称可创建多个.
  namespace: mt-math                    #属于的命名空间
spec:
  capacity:
    storage: 200M                        #创建的pv容量为1G
  accessModes:
  - ReadWriteMany                       #pv的访问模式:可读可写可挂在多个节点
  persistentVolumeReclaimPolicy: Recycle #回收策略
  storageClassName: sc-nfs-tomcat03
  nfs:                                  #创建的pv数据来源
    path: /nfs/pv-tomcat-log/03                     #数据源目录
    server: 172.16.201.134                #数据源ip

kubectl delete -f  tomcat-pv-nfs.yaml 
[root@master-1 tomcat]# kubectl apply -f  tomcat-pv-nfs.yaml 
persistentvolume/pv-nfs-tomcat01 created
persistentvolume/pv-nfs-tomcat02 created
persistentvolume/pv-nfs-tomcat03 created


[root@master-1 tomcat]# kubectl get pv -n mt-math   
NAME              CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                    STORAGECLASS      REASON   AGE
pv-nfs-tomcat01   200M       RWX            Recycle          Available                            sc-nfs-tomcat01            14s
pv-nfs-tomcat02   200M       RWX            Recycle          Available                            sc-nfs-tomcat02            14s
pv-nfs-tomcat03   200M       RWX            Recycle          Available                            sc-nfs-tomcat03            14s


[root@master-1 v1]# vim storageclass.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: sc-nfs-tomcat01
  namespace: mt-math
provisioner: kubernetes.io/nfs
reclaimPolicy: Retain
parameters:
  archiveOnDelete: "false"
[root@master-1 v1]# 

kubectl delete -f storageclass.yaml     
[root@master-1 tomcat]# kubectl apply -f storageclass.yaml
storageclass.storage.k8s.io/sc-nfs-tomcat01 created
storageclass.storage.k8s.io/sc-nfs-tomcat02 created
storageclass.storage.k8s.io/sc-nfs-tomcat03 created



[root@master-1 tomcat]# kubectl get storageclass -n mt-math   
NAME              PROVISIONER         RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
sc-nfs-tomcat01   kubernetes.io/nfs   Retain          Immediate           false                  1s
sc-nfs-tomcat02   kubernetes.io/nfs   Retain          Immediate           false                  1s
sc-nfs-tomcat03   kubernetes.io/nfs   Retain          Immediate           false                  1s





[root@master-1 tomcat]# kubectl delete pvc log-pvc-web-0 -n mt-math                
persistentvolumeclaim "log-pvc-web-0" deleted




[root@master-1 tomcat]# vim tomcat-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  namespace: mt-math
  name: tomcat
  labels:
    app: tomcat
spec:
  ports:
  - port: 8080
    name: web
  clusterIP: None
  selector:
    app: tomcat
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  namespace: mt-math
  name: web
spec:
  serviceName: "tomcat"
  replicas: 3
  selector:
    matchLabels:
      app: tomcat
  template:
    metadata:
      namespace: mt-math
      labels:
        app: tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat:7
        ports:
        - containerPort: 8080
          name: web
        volumeMounts:
        - name: log-pvc
          mountPath: /opt/tomcat/logs
        - name: tomcat-nfs-webapps
          mountPath: /opt/tomcat/webapps
        - name: tomcat-serverxml
          mountPath: /opt/tomcat/conf/server.xml
          subPath: server.xml

      volumes:                       
      - name: tomcat-nfs-webapps             
        nfs:                         
          server: 172.16.201.134       
          path: /NFS/tomcat-wwwroot      
      - name: tomcat-serverxml
        configMap:
          name: tomcat-configmap
          items:
          - key: server.xml
            path: server.xml
  volumeClaimTemplates:
  - metadata:
      name: log-pvc
      namespace: mt-math
    spec:
      accessModes: [ "ReadWriteMany" ]
      resources:
        requests:
          storage: 200M
		  storageClassName: sc-nfs-tomcat01


kubectl delete -f tomcat-statefulset.yaml
[root@master-1 tomcat]# kubectl apply -f tomcat-statefulset.yaml
service/tomcat created
statefulset.apps/web created


[root@master-1 tomcat]#  kubectl get pv,pvc -n mt-math  

NAME                               CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                    STORAGECLASS      REASON   AGE
persistentvolume/pv-nfs-tomcat01   200M       RWX            Recycle          Bound       mt-math/log-pvc-web-0    sc-nfs-tomcat01            6h19m
persistentvolume/pv-nfs-tomcat02   200M       RWX            Recycle          Bound       mt-math/log-pvc-web-1    sc-nfs-tomcat01            6h19m
persistentvolume/pv-nfs-tomcat03   200M       RWX            Recycle          Bound       mt-math/log-pvc-web-2    sc-nfs-tomcat01            6h19m
persistentvolume/pv0001            1Gi        RWX            Retain           Available                            nfs                        20h
persistentvolume/pv0002            2Gi        RWX            Retain           Available                            nfs                        20h
persistentvolume/pv0003            3Gi        RWX            Retain           Bound       default/mysql-pv-claim   nfs-mysql                  20h
persistentvolume/pv0004            5Gi        RWX            Retain           Released    default/nginx-pv-claim   nfs-nginx                  17h
persistentvolume/pv0005            5Gi        RWX            Retain           Released    default/php-pv-claim     nfs-php                    17h

NAME                                  STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS      AGE
persistentvolumeclaim/log-pvc-web-0   Bound    pv-nfs-tomcat01   200M       RWX            sc-nfs-tomcat01   64m
persistentvolumeclaim/log-pvc-web-1   Bound    pv-nfs-tomcat02   200M       RWX            sc-nfs-tomcat01   15m
persistentvolumeclaim/log-pvc-web-2   Bound    pv-nfs-tomcat03   200M       RWX            sc-nfs-tomcat01   3m3s


[root@master-1 tomcat]#  kubectl get pod -n mt-math -o wide 
NAME    READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
web-0   1/1     Running   0          66m     10.244.2.18   node-2   <none>           <none>
web-1   1/1     Running   0          17m     10.244.1.27   node-1   <none>           <none>
web-2   1/1     Running   0          4m49s   10.244.2.19   node-2   <none>           <none>



[root@node-1 ~]# docker exec -it  k8s_tomcat_tomcat01-95fc6cd5d-p85g6_test-ns_500aacb5-62be-4879-9c14-1ede8dbec968_0 /bin/bash
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat#mkdir webapps/ROOT;cd webapps
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat/webapps# echo "######  K8S  #######" > ROOT/index.jsp  

echo "node-2 -1" > ROOT/index.jsp  




[root@master-1 v1]# kubectl get svc -n mt-math -o wide
NAME     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE    SELECTOR
tomcat   ClusterIP   None         <none>        8080/TCP   100m   app=tomcat


[root@master-1 v1]#  kubectl describe service/tomcat -n mt-math
Name:              tomcat
Namespace:         mt-math
Labels:            app=tomcat
Annotations:       <none>
Selector:          app=tomcat
Type:              ClusterIP
IP:                None
Port:              web  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.1.27:8080,10.244.2.18:8080,10.244.2.19:8080
Session Affinity:  None
Events:            <none>
You have new mail in /var/spool/mail/root
[root@master-1 v1]# 




[root@master-1 tomcat]# vim tomcat-ingress.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-tomcat-wyl
  namespace: mt-math
  annotations: 
    kubernets.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/affinity: "cookie"
    nginx.ingress.kubernetes.io/session-cookie-name: "route"
    nginx.ingress.kubernetes.io/session-cookie-hash: "sha1"
spec:
  rules:
  - host: tomcat.bestyunyan.com
    http:
      paths:
      - path: 
        backend:
          serviceName: tomcat
          servicePort: 8080



[root@master-1 tomcat]# ll
total 28
-rw-r--r-- 1 root root   57 Nov 25 15:29 mt-math.yaml
-rw-r--r-- 1 root root  536 Nov 25 15:29 storageclass.yaml
-rw-r--r-- 1 root root 7269 Nov 25 15:35 tomcat-configmap.yaml
-rw-r--r-- 1 root root  538 Nov 25 15:36 tomcat-ingress.yaml
-rw-r--r-- 1 root root 2088 Nov 25 15:29 tomcat-pv-nfs.yaml
-rw-r--r-- 1 root root 1458 Nov 25 15:34 tomcat-statefulset.yaml

[root@master-1 tomcat]# kubectl apply -f tomcat-ingress.yaml 
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
ingress.extensions/ingress-tomcat-wyl created


[root@master-1 tomcat]# kubectl get ingress -n mt-math 
Warning: extensions/v1beta1 Ingress is deprecated in v1.14+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress
NAME                 CLASS    HOSTS                   ADDRESS   PORTS   AGE
ingress-tomcat-wyl   <none>   tomcat.bestyunyan.com             80      29s


#@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

k8s启动的tomcat会话保持配置
问题：k8s启动的tomcat服务，每次访问都带了session会话，每次调整不同的pod导致session实效，系统登录失败。
处理：在server中添加session保持:sessionAffinity: ClientIP

[root@master-1 v2]# vim deploy.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spring-k8s
spec:
  replicas: 4
  selector:
    matchLabels:
      app: spring-k8s
  template:
    metadata:
      labels:
        app: spring-k8s
    spec:
      containers:
        - name: spring-k8s
          image: tomcat:7
          ports:
            - containerPort: 8080
[root@master-1 v2]# 

[root@master-1 v2]# vim service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: spring-k8s
spec:
  selector:
    app: spring-k8s
  type: NodePort
  ports:
    - nodePort: 30008
      port: 80
      protocol: TCP
      targetPort: 8080
  sessionAffinity: ClientIP


景象是原始的，去各个节点容器里增加测试页面：
[root@node-1 ~]# docker exec -it  d107a8d3ecf0 /bin/bash
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat#mkdir webapps/ROOT;cd webapps
root@tomcat01-95fc6cd5d-p85g6:/usr/local/tomcat/webapps# echo "node-2 -1" > ROOT/index.jsp  

4个pod ，4个页面：
echo "node-1 -1" > ROOT/index.jsp  
echo "node-1 -2" > ROOT/index.jsp  
echo "node-2 -1" > ROOT/index.jsp  
echo "node-2 -2" > ROOT/index.jsp  



访问测试：http://172.16.201.134:30008/
[root@master-1 v2]# for i in {1..10};do curl http://172.16.201.134:30008/;echo ;done
node-2 -2

node-1 -2

node-1 -2

node-2 -1

node-1 -2

node-2 -1

node-1 -2

node-2 -2

node-1 -2

node-1 -1


添加session亲和性参数：sessionAffinity: ClientIP ,绘画保持
[root@master-1 v2]# vim service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: spring-k8s
spec:
  selector:
    app: spring-k8s
  type: NodePort
  ports:
    - nodePort: 30008
      port: 80
      protocol: TCP
      targetPort: 8080
  sessionAffinity: ClientIP




[root@master-1 v2]#  kubectl apply -f  service.yaml 
service/spring-k8s configured

[root@master-1 v1]#  kubectl describe service/tomcat -n mt-math
Name:              tomcat
Namespace:         mt-math
Labels:            app=tomcat
Annotations:       <none>
Selector:          app=tomcat
Type:              ClusterIP
IP:                None
Port:              web  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.1.27:8080,10.244.2.18:8080,10.244.2.19:8080
Session Affinity:  None
Events:            <none>
[root@master-1 v1]# 


[root@master-1 v1]#  kubectl get svc,pod
NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/blog-mysql      ClusterIP   10.1.71.85     <none>        3306/TCP       20h
service/kubernetes      ClusterIP   10.1.0.1       <none>        443/TCP        31h
service/php-fpm-nginx   NodePort    10.1.97.87     <none>        80:30071/TCP   10h
service/spring-k8s      NodePort    10.1.114.102   <none>        80:30008/TCP   9m36s

NAME                                 READY   STATUS    RESTARTS   AGE
pod/blog-mysql-5cb7fd64f8-qlksz      1/1     Running   0          20h
pod/php-fpm-nginx-5fdcb6c57d-z5vs8   2/2     Running   0          10h
pod/spring-k8s-66ffd64dbb-bh9pd      1/1     Running   0          10m
pod/spring-k8s-66ffd64dbb-dgmdj      1/1     Running   0          10m
pod/spring-k8s-66ffd64dbb-wgq79      1/1     Running   0          10m
pod/spring-k8s-66ffd64dbb-xwlz5      1/1     Running   0          10m
[root@master-1 v1]# 


[root@master-1 v1]# for i in {1..10};do curl http://172.16.201.134:30008/;echo ;done
node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

[root@master-1 v1]# 


[root@master-1 v1]# kubectl get pod -o wide 
NAME                             READY   STATUS             RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
blog-mysql-5cb7fd64f8-qlksz      1/1     Running            0          24h     10.244.2.11   node-2   <none>           <none>
php-fpm-nginx-5fdcb6c57d-z5vs8   2/2     Running            0          13h     10.244.1.25   node-1   <none>           <none>
nexus3-5c6f988994-hbvd2          1/1     Running            0          2m58s   10.244.1.50   node-1   <none>           <none>
spring-k8s-66ffd64dbb-2rxpr      1/1     Running            0          107s    10.244.2.24   node-2   <none>           <none>
spring-k8s-66ffd64dbb-4rgpg      1/1     Running            0          107s    10.244.2.23   node-2   <none>           <none>
spring-k8s-66ffd64dbb-75vxd      1/1     Running            0          71s     10.244.2.29   node-2   <none>           <none>
spring-k8s-66ffd64dbb-7chks      1/1     Running            0          107s    10.244.1.31   node-1   <none>           <none>
spring-k8s-66ffd64dbb-7scl5      1/1     Running            0          71s     10.244.1.33   node-1   <none>           <none>
spring-k8s-66ffd64dbb-86h6x      1/1     Running            0          71s     10.244.1.35   node-1   <none>           <none>
spring-k8s-66ffd64dbb-b2z96      1/1     Running            0          107s    10.244.1.32   node-1   <none>           <none>
spring-k8s-66ffd64dbb-bh9pd      1/1     Running            0          3h39m   10.244.2.21   node-2   <none>           <none>
spring-k8s-66ffd64dbb-ck8q9      1/1     Running            0          71s     10.244.1.36   node-1   <none>           <none>
spring-k8s-66ffd64dbb-dgmdj      1/1     Running            0          3h39m   10.244.2.20   node-2   <none>           <none>
spring-k8s-66ffd64dbb-htdp2      1/1     Running            0          71s     10.244.2.25   node-2   <none>           <none>
spring-k8s-66ffd64dbb-lb5hs      1/1     Running            0          71s     10.244.2.26   node-2   <none>           <none>
spring-k8s-66ffd64dbb-mkbj5      1/1     Running            0          107s    10.244.1.30   node-1   <none>           <none>
spring-k8s-66ffd64dbb-pcnqg      1/1     Running            0          107s    10.244.2.22   node-2   <none>           <none>
spring-k8s-66ffd64dbb-pnk44      1/1     Running            0          71s     10.244.2.28   node-2   <none>           <none>
spring-k8s-66ffd64dbb-s75bp      1/1     Running            0          71s     10.244.1.34   node-1   <none>           <none>
spring-k8s-66ffd64dbb-wgq79      1/1     Running            0          3h39m   10.244.1.29   node-1   <none>           <none>
spring-k8s-66ffd64dbb-xwlz5      1/1     Running            0          3h39m   10.244.1.28   node-1   <none>           <none>
spring-k8s-66ffd64dbb-z6w58      1/1     Running            0          71s     10.244.2.27   node-2   <none>           <none>


[root@master-1 v1]# for i in {1..20};do curl http://172.16.201.134:30008/;echo ;done
node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

node-2 -1

[root@master-1 v1]# 





kubectl apply -f tomcat-pv-nfs.yaml
kubectl apply -f storageclass.yaml
kubectl apply -f tomcat-configmap.yaml
kubectl apply -f tomcat-statefulset.yaml
kubectl apply -f tomcat-ingress.yaml



kubectl patch pvc test  -p '{"metadata":{"finalizers":null}}' -n k8s-test
kubectl patch pvc log-pvc-web-0 -p '{"metadata":{"finalizers":null}}' -n mt-math         

kubectl apply -f php-deployment.yaml
kubectl delete -f php-deployment.yaml
docker logs --tail="100"  96b86bf51c03

kubectl delete pv pv0003
kubectl delete pv pv0004
kubectl delete pv pv0005
kubectl apply -f pv.yaml 
kubectl get pv,pvc,svc,pod -o wide


kubectl apply -f 0003.yaml 
kubectl apply -f 0004.yaml 
kubectl apply -f 0005.yaml 

kubectl describe pod blog-php-65bd88456b-ztd25


kubectl get event -n mt-math 
kubectl get event
kubectl exec -it blog-nginx-5545fc6957-lcfrq  --  cat /etc/nginx/conf.d/default.conf 
kubectl exec -it blog-nginx-5545fc6957-lcfrq  -- bash
kubectl get pods -o wide
kubectl get pv,pvc -o wide
kubectl get svc
kubectl exec -it blog-mysql-5cb7fd64f8-qlksz  -- bash

kubectl create secret generic mysql-pass --from-literal=password=YOUR_PASSWORD
kubectl create -f https://k8s.io/examples/application/wordpress/mysql-deployment.yaml
kubectl create -f https://k8s.io/examples/application/wordpress/wordpress-deployment.yaml



安装php-pdo扩展(更好的办法是你应该制作一个运行php环境的容器)
kubectl exec -it blog-php-5cbddf56b7-brvss -- /usr/local/bin/docker-php-ext-install pdo_mysql
kubectl exec -it blog-php-5cbddf56b7-dhtth -- /usr/local/bin/docker-php-ext-install php_mysqli
kubectl exec -it blog-php-5cbddf56b7-dhtth  -- php -m
kubectl exec -it blog-nginx-5545fc6957-nw4qm  -- bash


#####测试自愈：
[root@node-1 pv-tomcat-log]# ps -ef|grep tomcat
root       1549 110748  0 03:56 pts/0    00:00:00 grep --color=auto tomcat
root      19377  19359  0 Nov25 ?        00:00:32 /usr/local/openjdk-8/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start
root      38398  38363  0 Nov25 ?        00:00:32 /usr/local/openjdk-8/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start
root      38409  38377  0 Nov25 ?        00:00:31 /usr/local/openjdk-8/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start

干掉任何一个tomcat猫
[root@node-1 pv-tomcat-log]# kill -9 38409

剩余2个了
[root@node-1 pv-tomcat-log]# ps -ef|grep tomcat
root       1742 110748  0 03:56 pts/0    00:00:00 grep --color=auto tomcat
root      19377  19359  0 Nov25 ?        00:00:32 /usr/local/openjdk-8/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start
root      38398  38363  0 Nov25 ?        00:00:32 /usr/local/openjdk-8/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start

出现错误，立即重启了一个
[root@master-1 v1]# kubectl get pod -o wide --watch
spring-k8s-66ffd64dbb-xwlz5      0/1     Error     0          4h8m   10.244.1.28   node-1   <none>           <none>
spring-k8s-66ffd64dbb-xwlz5      1/1     Running   1          4h8m   10.244.1.28   node-1   <none>           <none>


[root@master-1 v1]# kubectl get pod -o wide --watch
NAME                             READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
blog-mysql-5cb7fd64f8-qlksz      1/1     Running   5          24h     10.244.2.11   node-2   <none>           <none>
php-fpm-nginx-5fdcb6c57d-z5vs8   2/2     Running   0          14h     10.244.1.25   node-1   <none>           <none>
spring-k8s-66ffd64dbb-bh9pd      1/1     Running   0          4h10m   10.244.2.21   node-2   <none>           <none>
spring-k8s-66ffd64dbb-dgmdj      1/1     Running   0          4h10m   10.244.2.20   node-2   <none>           <none>
spring-k8s-66ffd64dbb-wgq79      1/1     Running   0          4h10m   10.244.1.29   node-1   <none>           <none>
spring-k8s-66ffd64dbb-xwlz5      1/1     Running   1          4h10m   10.244.1.28   node-1   <none>           <none>

spring-k8s相关状态恢复正常


##(十七)、k8s部署 nginx弹性伸缩
1、什么是K8s的弹性伸缩？

答：Hpa（全称叫做Horizontal Pod Autoscaler），Horizontal Pod Autoscaler的操作对象是Replication Controller、ReplicaSet或者Deployment对应的Pod（k8s中可以控制Pod的是rc、rs、deployment），根据观察到的CPU使用量与用户的阈值进行比对，做出是否需要增加或者减少实例数量的决策。controller目前使用heapSter来检测CPU使用量，检测周期默认是30秒。


2、K8s的弹性伸缩的工作原理？
答：Horizontal Pod Autoscaler的工作原理，主要是监控一个Pod，监控这个Pod的资源CPU使用率，一旦达到了设置的阈值，就做策略来决定它是否需要增加，做策略的时候还需要一个周期，比如，持续五分钟都发现CPU使用率高，就抓紧增加Pod的数量来减轻它的压力。当然也有一个策略，就是持续五分钟之后，压力一直都很低，那么会减少Pod的数量。这就是k8s的弹性伸缩的工作原理，主要是监控CPU的使用率，然后来决定是否增加或者减少Pod的数量。



##(十八)、k8s部署Maven Nexus私服
[root@node-1 maven]# wget https://sonatype-download.global.ssl.fastly.net/repository/downloads-prod-group/3/nexus-3.37.0-01-unix.tar.gz
[root@node-1 maven]# tar -xvf nexus-3.37.0-01-unix.tar.gz
[root@node-1 maven]# vim  nexus-3.37.0-01/bin/nexus.vmoptions 
-Xms1600m
-Xmx1800m
-XX:MaxDirectMemorySize=2099m
-XX:+UnlockDiagnosticVMOptions
-XX:+LogVMOutput
-XX:LogFile=/data/maven-nexus/log/jvm.log
-XX:-OmitStackTraceInFastThrow
-Djava.net.preferIPv4Stack=true
-Dkaraf.home=.
-Dkaraf.base=.
-Dkaraf.etc=etc/karaf
-Djava.util.logging.config.file=etc/karaf/java.util.logging.properties
-Dkaraf.data=/data/maven-nexus/data
-Dkaraf.log=../sonatype-work/nexus3/log
-Djava.io.tmpdir=/data/maven-nexus/tmp
-Dkaraf.startLocalConsole=false
-Djdk.tls.ephemeralDHKeySize=2048


主要是就是下面这些：
-XX:LogFile=/data/maven-nexus/log/jvm.log   # 日志文件生成位置
-Dkaraf.data=/data/maven-nexus/data         # 仓库数据存放位置(上传的jar包)
-Djava.io.tmpdir=/data/maven-nexus/tmp      # 临时文件存放位置
[root@node-1 maven]# tar -czvf nexus-3.37.0-01-my.tar.gz nexus-3.37.0-01 sonatype-work


[root@node-1 maven]# docker login
Login with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.
Username: lex921
Password: 

[root@node-1 maven]# docker pull halcom/oracle-jdk8:151
151: Pulling from halcom/oracle-jdk8
8d30e94188e7: Pull complete 
837109e7db31: Pull complete 
9fa483f86897: Pull complete 
258a46ee0c37: Pull complete 
fb10d2832630: Pull complete 
Digest: sha256:7cb312302573114d20233a06cd51c7618e7bcac46a33e63c2c88914f11a89e26
Status: Downloaded newer image for halcom/oracle-jdk8:151
docker.io/halcom/oracle-jdk8:151
[root@node-1 maven]# 


[root@node-1 maven]# vim Dockerfile
FROM halcom/oracle-jdk8:151
ADD nexus-3.37.0-01-my.tar.gz /opt
ENTRYPOINT ["/opt/nexus-3.37.0-01/bin/nexus", "run"]

[root@node-1 maven]# docker build . -t nexus:0.1
Sending build context to Docker daemon  672.4MB
Step 1/3 : FROM halcom/oracle-jdk8:151
 ---> 96f4574f5640
Step 2/3 : ADD nexus-3.37.0-01-my.tar.gz /opt
 ---> 616c4771f796
Step 3/3 : ENTRYPOINT ["/opt/nexus-3.37.0-01/bin/nexus", "run"]
 ---> Running in f32ffeea0feb
Removing intermediate container f32ffeea0feb
 ---> 9a3a76ba9535
Successfully built 9a3a76ba9535
Successfully tagged nexus:0.1
[root@node-1 maven]#  



kubectl create -f nexus-pv-pvc.yaml
kubectl delete -f nexus-pv-pvc.yaml
[root@master-1 maven]# cat nexus-pv-pvc.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nexus3-data-pv
  labels:
    app: nexus3-data-pv
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  hostPath:
    path: /data/maven-nexus

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nexus3-data-pvc
  labels:
    app: nexus3-data-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  selector:
    matchLabels:
      app: nexus3-data-pv

[root@master-1 maven]# 


[root@master-1 maven]# cat nexus-deployment.yaml
kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    app: nexus3
  name: nexus3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nexus3
  template:
    metadata:
      labels:
        app: nexus3
    spec:
      containers:
        - name: nexus3
          image: nexus:0.1
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8081
            protocol: TCP
          volumeMounts:
          - name: nexus-data
            mountPath: /data/maven-nexus
      volumes:
        - name: nexus-data
          persistentVolumeClaim:
            claimName: nexus3-data-pvc
      nodeSelector:
        kubernetes.io/hostname: node-1
[root@master-1 maven]# 
[root@master-1 maven]# kubectl get pod |grep nexus3
nexus3-654b948474-m75r6          1/1     Running   0          16s
kubectl delete -f nexus-deployment.yaml




[root@master-1 maven]# cat nexus-svc.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nexus3
  name: nexus3
spec:
  type: NodePort
  ports:
  - port: 8081
    targetPort: 8081
    nodePort: 8081
    name: web-ui
  - port: 5000
    targetPort: 5000
    nodePort: 5000
    name: docker-group
  - port: 8889
    targetPort: 8889
    nodePort: 8889
    name: docker-push
  selector:
    app: nexus3
[root@master-1 maven]# kubectl create -f nexus-svc.yaml
service/nexus3 created

kubectl delete -f nexus-svc.yaml
[root@master-1 maven]# kubectl get svc
NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                     AGE
blog-mysql      ClusterIP   10.1.71.85     <none>        3306/TCP                                    25h
kubernetes      ClusterIP   10.1.0.1       <none>        443/TCP                                     36h
nexus3          NodePort    10.1.64.97     <none>        8081:8081/TCP,5000:5000/TCP,8889:8889/TCP   4s
php-fpm-nginx   NodePort    10.1.97.87     <none>        80:30071/TCP                                15h
spring-k8s      NodePort    10.1.114.102   <none>        80:30008/TCP                                5h5m

http://172.16.201.134:8081/#browse/welcome
Your admin user password is located in
/data/maven-nexus/data/admin.password on the server.


[root@master-1 maven]# kubectl exec -it nexus3-5c6f988994-hbvd2 -- bash
[root@nexus3-5c6f988994-hbvd2 /]# cat /data/maven-nexus/data/admin.password
d1e33304-09b8-40c9-8348-63f160ee48e2[root@nexus3-5c6f988994-hbvd2 /]# 
进去改密码：admin/11111111
bi xu

kubectl get pod
kubectl get svc
kubectl get pv
kubectl describe pod blog-php-65bd88456b-ztd25
kubectl get event -n mt-math 
kubectl get event

docker logs --tail="100"  d62c524204dc
kubectl logs --tail 200 -f nexus3-5c6f988994-fctk5


升级
这里再额外补充点内容，由于nexus3更新算是比较频繁的，我们如何无缝升级呢？
这里就借用k8s Deployment的升级方式就好了。

第一步：从官网下载最新的nexus安装包；
第二步：修改nexus配置文件，将上面旧版本的配置覆盖过来就行了；
第三步：修改Dockerfile文件，构建新的Docker镜像，将新打包的nexus放入镜像中。
如：docker build . -t nexus:0.1
Ps: 不要忘记启动命令路径也要调整哟
第四步：使用k8s命令升级Deployment：
如：kubectl set image deployment/nexus3 nexus:v3.38
第五步：回滚升级，如果发现升级了的不好用，或者出现问题，也可以回滚哦：
如：kubectl rollout undo deployment/nexus3

##()、k8s部署MysqL 集群


1.2.1高性能架构
1)读写分离架构(读性能较高)
代码级别
MySQL proxy (Atlas,mysql router,proxySQL(percona),maxscale)、
amoeba(taobao)
xx-dbproxy等。
2)分布式架构（读写性能都提高）
分库分表——cobar--->TDDL(头都大了),DRDS
Mycat--->DBLE自主研发等。
TiDB

1.2.2高可用架构
（3）单活:MMM架构——mysql-mmm（google）
（4）单活:MHA架构——mysql-master-ha（日本DeNa）,T-MHA
（5）多活:MGR ——5.7 新特性 MySQL Group replication(5.7.17) --->Innodb Cluster
（6）多活:MariaDB Galera Cluster架构,(PXC)Percona XtraDB Cluster、MySQL Cluster(Oracle rac)架构





##(二十)、k8s部署 ActiveMQ集群
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.201.134:6443 --token 9fjk8q.tqi4qa6fixuoe050 \
    --discovery-token-ca-cert-hash sha256:57bb6a95af7a6d7a49f8e4ed7226a33dd3589b70b965519924b76dd79e74a48a



下载地址：
http://archive.apache.org/dist/activemq/


JDK : lex921@163.com / 000#
https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html



###1、做景象
[root@master1 ~]# vim dockerfile_activemq 
FROM centos:latest

MAINTAINER lyd

ADD jdk-8u251-linux-x64.tar.gz /usr/local/
ADD apache-activemq-5.16.0-bin.tar.gz /usr/local/
RUN cd /usr/local && ls && mv apache-activemq-5.16.0 activemq
ENV JAVA_HOME=/usr/local/jdk1.8.0_251
ENV CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
ENV PATH=$JAVA_HOME/bin:$PATH

EXPOSE 61616 8161

ENTRYPOINT  /usr/local/activemq/bin/activemq start && /bin/bash


###2、执行
[root@master-1 activemq]#docker build -f dockerfile_activemq -t activemq:0.1 .

mq使用的端口较多，启动时主要是端口映射
AMQ使用默认61616提供JMS服务，8161提供web管控台服务。
docker run -dit --name activemq -p 61616:61616 -p 8161:8161 activemq
docker run -dit --name activemq -p 61616:61616 -p 8161:8161 -p 30081:8161 -p 30061:61616 activemq
netstat -nltp|grep -E '(61616|8161)' 查看映射端口

###3、查看
[root@node-1 activemq]# docker images|grep activemq
activemq                                             0.1                 25d66bad1665        About an hour ago   741MB
[root@node-1 activemq]# 


###4、准备丫猫脚本
[root@master-1 activemq]# vim activemq-service.yml
[root@master1 ~]# cat activemq-service.yml 
apiVersion: v1
kind: Service
metadata:
  name: activemq-service #名称
  labels:
    app: activemq
spec:
  type: NodePort
  ports:
  - name: admin
    port: 8161
    targetPort: 8161
    nodePort: 30081
  - name: tcp
    port: 61616
    targetPort: 61616
    protocol: TCP
    nodePort: 30061
  selector:
    app: activemq


[root@master-1 activemq]# kubectl apply -f activemq-service.yml 
service/activemq-service created

[root@master-1 activemq]#  kubectl get svc
NAME               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
activemq-service   NodePort    10.1.113.149   <none>        8161:30081/TCP,61616:30061/TCP   3s
kubernetes         ClusterIP   10.1.0.1       <none>        443/TCP                          93m
nginx              NodePort    10.1.54.17     <none>        80:32758/TCP                     24m





[root@master-1 activemq]# vim activemq-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: activemq-deployment
  labels:
    app: activemq
spec:
  replicas: 1
  selector:
    matchLabels:
      app: activemq
  template:
    metadata:
      labels:
        app: activemq
    spec:
      containers:
      - name: activemq
        image: activemq:latest
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-ce", "tail -f /dev/null" ]
        ports:
        - containerPort: 61616
        - containerPort: 8161
      nodeName: node-2

 kubectl delete -f activemq-deployment.yml
 kubectl delete pod activemq-deployment-77966fd658-nglgd --force
 
[root@master-1 activemq]#  kubectl apply -f activemq-deployment.yml 
deployment.apps/activemq-deployment created

[root@master-1 activemq]# kubectl get pods
NAME                                  READY   STATUS    RESTARTS   AGE
activemq-deployment-c9b57bd56-9spvq   0/1     Pending   0          9s



kubectl describe pod activemq-deployment-76b64874f-wv99l
kubectl get event


###5、启动服务
###activemq配置需要改一下：
[root@master-1 activemq]# kubectl exec -it activemq-deployment-68b8596b6b-mn9fw -- bash
[root@activemq-deployment-68b8596b6b-mn9fw activemq]# cd /usr/local/activemq
[root@activemq-deployment-68b8596b6b-mn9fw activemq]# vim conf/jetty.xml
<property name="host" value="0.0.0.0"/>
默认是127.0.0.1，改成0.0.0.0
[root@activemq-deployment-68b8596b6b-mn9fw activemq]#./bin/activemq start
[root@activemq-deployment-68b8596b6b-mn9fw activemq]# tail -100f ./data/activemq.log
...
2021-12-21 05:52:52,387 | WARN  | ServletContext@o.e.j.s.ServletContextHandler@74f5ce22{/,null,STARTING} has uncovered http methods for path: / | org.eclipse.jetty.security.SecurityHandler | main
2021-12-21 05:52:52,486 | INFO  | Listening for connections at ws://activemq-deployment-68b8596b6b-mn9fw:61614?maximumConnections=1000&wireFormat.maxFrameSize=104857600 | org.apache.activemq.transport.ws.WSTransportServer | main
2021-12-21 05:52:52,496 | INFO  | Connector ws started | org.apache.activemq.broker.TransportConnector | main
2021-12-21 05:52:52,499 | INFO  | Apache ActiveMQ 5.16.0 (localhost, ID:activemq-deployment-68b8596b6b-mn9fw-33550-1640065971778-0:1) started | org.apache.activemq.broker.BrokerService | main
2021-12-21 05:52:52,501 | INFO  | For help or more information please see: http://activemq.apache.org | org.apache.activemq.broker.BrokerService | main
2021-12-21 05:52:52,503 | WARN  | Store limit is 102400 mb (current store usage is 0 mb). The data directory: /usr/local/activemq/data/kahadb only has 98832 mb of usable space. - resetting to maximum available disk space: 98832 mb | org.apache.activemq.broker.BrokerService | main
2021-12-21 05:52:53,958 | INFO  | ActiveMQ WebConsole available at http://0.0.0.0:8161/ | org.apache.activemq.web.WebConsoleStarter | main
2021-12-21 05:52:53,959 | INFO  | ActiveMQ Jolokia REST API available at http://0.0.0.0:8161/api/jolokia/ | org.apache.activemq.web.WebConsoleStarter | main


###6、访问：
访问：默认admin/admin
http://172.16.201.134:30081/admin/queues.jsp



[root@master-1 activemq]#  kubectl get pods
NAME                                   READY   STATUS    RESTARTS   AGE
activemq-deployment-68b8596b6b-2xjgd   1/1     Running   0          2m49s
activemq-deployment-68b8596b6b-6gxvf   1/1     Running   0          20s
activemq-deployment-68b8596b6b-8589n   1/1     Running   0          2m49s
activemq-deployment-68b8596b6b-fkxj6   1/1     Running   0          20s
activemq-deployment-68b8596b6b-mn9fw   1/1     Running   0          61m
activemq-deployment-68b8596b6b-nmmzx   1/1     Running   0          20s
activemq-deployment-68b8596b6b-p958t   1/1     Running   0          20s
activemq-deployment-68b8596b6b-qdp4f   1/1     Running   0          20s
activemq-deployment-68b8596b6b-swwr4   1/1     Running   0          20s
nginx-6799fc88d8-xccsn                 1/1     Running   1          86m



######问题1：
CrashLoopBackOff 错误
CrashLoopBackOff的含义是，Kubernetes试图启动该Pod，但是过程中出现错误，导致容器启动失败或者正在被删除。
[root@master1 ~]# kubectl get pods -o wide
NAME                                   READY   STATUS             RESTARTS   AGE   IP            NODE    NOMINATED NODE   READINESS GATES
activemq-deployment-5fb4cccb97-2zmxh   1/1     Running            0          16h   10.244.3.8    node2   <none>           <none>
connect-deployment-74d6746f47-hv7sv    0/1     CrashLoopBackOff   5          6m    10.244.3.17   node2   <none>           <none>


############解决：
deployment中镜像位置，追加指令command
command: ["/bin/bash", “-ce”, “tail -f /dev/null” ] 这样导致结果actvemq可能应用程序未启动起来。启动了空服务。
这个如果应用程序未启动，登录进docker中查看日志，执行./bin/activemq start，可以作为调试。
最简单办法就是在Dockerfile中追加空文件 java -jar xxx.jar && tail -f test 文件

pod报错"Back-off restarting failed container"解决办法
链接地址：https://blog.csdn.net/sqhren626232/article/details/101013390

解决方法：在deployment申明镜像的后面加上命令
command: [ “/bin/bash”, “-ce”, “tail -f /dev/null” 



##(二十一)、k8s部署 RocketMQ集群
###一、基本知识简要说明
RocketMQ主要有四大组成部分：NameServer、Broker、Producer、Consumer。
 
Nameserver作用：
NameServer 可以说是 Broker 的注册中心，Broker 在启动的时候，会根据配置信息向所有的 NameServer 进行注册，NameServer 会和每次前来注册的 Broker 保持长连接，并每 30s 检查 Broker 是否还存活，对于宕机的 Broker，NameServer 会将其从列表中剔除。当生产者需要向 Broker 发送消息的时候，就会先从 NameServer 里面获取 Broker 的地址列表，然后负载均衡，选择一台消息服务器进行发送。
 
RocketMQ的部署方式有多种：
2m-noslave： 多Master模式，无Slave。[双主模式]
2m-2s-sync： 多Master多Slave模式，同步双写 [双主双从+同步模式]
2m-2s-async：多Master多Slave模式，异步复制 [双主双从+异步模式]
RocketMQ 提供了三种方式发送消息：同步、异步和单向：
同步发送: 指消息发送方发出数据后会在收到接收方发回响应之后才发下一个数据包。
异步发送: 指发送方发出数据后，不等接收方发回响应，接着发送下个数据包, 异步方式也需要Broker返回确认信息。
单向发送: 指只负责发送消息而不等待服务器回应且没有回调函数触发。
 
RocketMQ 三种消息发送模式的使用场景：
具体使用哪种模式，这主要是看应用场景。
同步发送：主要运用在比较重要一点消息传递/通知等业务：
异步发送：通常用于对发送消息响应时间要求更高/更快的场景：
单向发送：适用于某些耗时非常短，但对可靠性要求并不高的场景，例如日志收集。只发送消息，不等待服务器响应，只发送请求不等待应答。此方式发送消息的过程耗时非常短，一般在微秒级别。
 
RocketMQ端口：
rocketmq 默认端口：9876（即nameserver端口）
非vip通道端口：10911
vip通道端口：10909
10909是VIP通道对应的端口，在JAVA中的消费者对象或者是生产者对象中关闭VIP通道即可无需开放10909端口
 
本案例部署的是RocketMQ的"双主双从+同步模式"，涉及6个pod：
broker-a 主1
broker-b 主2
broker-a-s 从1
broker-a-s 从2
NameServer（注册中心）
rocketmq-externals（可视化web界面）

###二、部署过程记录

####1、使用NFS配置StatefulSet的动态持久化存储
StatefulSet使用volumeClaimTemplates

#####1.1）在NFS服务器端（172.16.60.238）通过nfs创建rocketmq的共享目录
使用NFS作为StatefulSet持久化存储的操作记录，分别需要创建nfs-provisioner的rbac、storageclass、nfs-client-provisioner和statefulset的pod。

[root@master-1 rocketmq]# mkdir -p /data/storage/k8s/rocketmq
[root@master-1 rocketmq]# echo '/data/storage/k8s/rocketmq *(rw,no_root_squash)' >> /etc/exports
[root@master-1 rocketmq]# exportfs -r
[root@master-1 rocketmq]# exportfs 
/data/storage/k8s/rocketmq
                <world>
[root@master-1 rocketmq]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/data/storage/k8s/rocketmq *

[root@node-1 ~]# mount -t nfs 172.16.201.134:/data/storage/k8s/rocketmq /data/storage/k8s/rocketmq
[root@node-2 ~]# mount -t nfs 172.16.201.134:/data/storage/k8s/rocketmq /data/storage/k8s/rocketmq

[root@node-1 ~]#  df -h|grep 134
172.16.201.134:/data/storage/k8s/rocketmq  100G  4.2G   96G   5% /data/storage/k8s/rocketmq

[root@node-2 ~]# df -h|grep 134
172.16.201.134:/data/storage/k8s/rocketmq  100G  4.2G   96G   5% /data/storage/k8s/rocketmq     



#####1.2）创建nfs的rbac
[root@master-1 activemq]# kubectl create namespace wiseco
namespace/wiseco created




[root@k8s-master01 rocketmq]# vim nfs-rbac.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
  namespace: wiseco
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
   name: nfs-provisioner-runner
   namespace: wiseco
rules:
   -  apiGroups: [""]
      resources: ["persistentvolumes"]
      verbs: ["get", "list", "watch", "create", "delete"]
   -  apiGroups: [""]
      resources: ["persistentvolumeclaims"]
      verbs: ["get", "list", "watch", "update"]
   -  apiGroups: ["storage.k8s.io"]
      resources: ["storageclasses"]
      verbs: ["get", "list", "watch"]
   -  apiGroups: [""]
      resources: ["events"]
      verbs: ["watch", "create", "update", "patch"]
   -  apiGroups: [""]
      resources: ["services", "endpoints"]
      verbs: ["get","create","list", "watch","update"]
   -  apiGroups: ["extensions"]
      resources: ["podsecuritypolicies"]
      resourceNames: ["nfs-provisioner"]
      verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: wiseco
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io

[root@master-1 rocketmq]# kubectl apply -f nfs-rbac.yaml
serviceaccount/nfs-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created


[root@master-1 rocketmq]# kubectl get sa -n wiseco|grep nfs
nfs-provisioner   1         10s
[root@master-1 rocketmq]# kubectl get clusterrole -n wiseco|grep nfs
nfs-provisioner-runner                                                 2021-12-21T10:25:32Z
[root@master-1 rocketmq]# kubectl get clusterrolebinding -n wiseco|grep nfs
run-nfs-provisioner                                    ClusterRole/nfs-provisioner-runner                                                 18s
[root@master-1 rocketmq]# 


#####1.3）创建rocketmq集群的storageclass

[root@master-1 rocketmq]# vim rocketmq-nfs-class.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: rocketmq-nfs-storage
  namespace: wiseco
provisioner: rocketmq/nfs
reclaimPolicy: Retain


[root@master-1 rocketmq]# kubectl apply -f rocketmq-nfs-class.yaml
Warning: storage.k8s.io/v1beta1 StorageClass is deprecated in v1.19+, unavailable in v1.22+; use storage.k8s.io/v1 StorageClass
storageclass.storage.k8s.io/rocketmq-nfs-storage created

[root@master-1 rocketmq]# kubectl get sc -n wiseco
NAME                   PROVISIONER    RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rocketmq-nfs-storage   rocketmq/nfs   Retain          Immediate           false                  8s
[root@master-1 rocketmq]# 


#####1.4）创建rocketmq集群的nfs-client-provisioner
PROVISIONER_NAME的值一定要和StorageClass中的provisioner相等。


[root@master-1 rocketmq]# vim rocketmq-nfs.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rocketmq-nfs-client-provisioner
  namespace: wiseco
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rocketmq-nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rocketmq-nfs-client-provisioner
    spec:
      serviceAccount: nfs-provisioner
      containers:
        - name: rocketmq-nfs-client-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root
              mountPath:  /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: rocketmq/nfs
            - name: NFS_SERVER
              value: 172.16.201.134
            - name: NFS_PATH
              value: /data/storage/k8s/rocketmq
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.201.134
            path: /data/storage/k8s/rocketmq

[root@master-1 rocketmq]# kubectl apply -f rocketmq-nfs.yml
deployment.apps/rocketmq-nfs-client-provisioner created
[root@master-1 rocketmq]# kubectl get pods -n wiseco|grep nfs
rocketmq-nfs-client-provisioner-5bc9679c86-9xrws   1/1     Running   0          13s



####2、创建RocketMQ集群（双主双从同步模式）

本案例采用jdk1.8.0_192的基础镜像

[root@master-1 rocketmq]# ll
total 12
-rw-r--r-- 1 root root 1216 Dec 21 18:25 nfs-rbac.yaml
-rw-r--r-- 1 root root  161 Dec 21 18:26 rocketmq-nfs-class.yaml
-rw-r--r-- 1 root root 1029 Dec 21 18:28 rocketmq-nfs.yml



#####2.1）制作rocketmq的image镜像
rocketmq下载地址：https://mirrors.tuna.tsinghua.edu.cn/apache/rocketmq/

需要注意一点：
pod可能会启动报错：/usr/local/rocketmq-4.8.0/bin/runbroker.sh: line 90: 18 Killed $JAVA ${JAVA_OPT} $@
原因分析：/usr/local/rocketmq-4.8.0/bin/runbroker.sh脚本文件里的内存设置的太大, pod因内存不足导致启动失败！
解决办法：将runbroker.sh脚本文件里的内存设置调整小点：

将JAVA_OPT="${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g"
改为
JAVA_OPT="${JAVA_OPT} -server -Xms1g -Xmx1g -Xmn512m"
[root@master-1 rocketmq_image]# vim rocketmq-4.9.2/bin/runbroker.sh 
...
JAVA_OPT="${JAVA_OPT} -server -Xms512m -Xmx512m"
...

打包：
[root@master-1 rocketmq_image]# tar -czvf rocketmq-4.9.2.tar.gz rocketmq-4.9.2



[root@master-1 rocketmq_image]# vim Dockerfile
FROM centos:latest

RUN rm -f /etc/localtime \
&& ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
&& echo "Asia/Shanghai" > /etc/timezone

ENV LANG en_US.UTF-8
ADD jdk-8u311-linux-x64.tar.gz /usr/local/
ADD rocketmq-4.9.2.tar.gz /usr/local/
RUN mkdir -p /data/rocketmq/store
ENV JAVA_HOME=/usr/local/jdk1.8.0_311
ENV CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
ENV PATH=$JAVA_HOME/bin:$PATH

CMD ["/bin/bash"]


制作镜像
[root@master-1 rocketmq_image]# docker build -f ./Dockerfile -t rocketmq:0.1 . 



#####2.2）制作nameserver的image镜像

[root@master-1 image]# mkdir nameserver_image
[root@master-1 image]# ll
total 0
drwxr-xr-x 2 root root   6 Dec 21 18:52 nameserver_image
drwxr-xr-x 3 root root 151 Dec 21 18:51 rocketmq_image
[root@master-1 image]# cd nameserver_image/
[root@master-1 nameserver_image]# cp ../rocketmq_image/rocketmq-4.9.2.tar.gz ./
[root@master-1 nameserver_image]# vim namesrv.properties
listenPort=20901
[root@master-1 nameserver_image]# vim Dockerfile
FROM FROM centos:latest
 
RUN rm -f /etc/localtime \
&& ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
&& echo "Asia/Shanghai" > /etc/timezone
 
ENV LANG en_US.UTF-8
ADD jdk-8u311-linux-x64.tar.gz /usr/local/
ADD rocketmq-4.9.2.tar.gz /usr/local/
COPY namesrv.properties /usr/local/rocketmq-4.9.2/conf/
ENV JAVA_HOME=/usr/local/jdk1.8.0_311
ENV CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
ENV PATH=$JAVA_HOME/bin:$PATH
WORKDIR /usr/local/rocketmq-4.9.2
CMD ["/usr/local/rocketmq-4.9.2/bin/mqnamesrv","-c","/usr/local/rocketmq-4.9.2/conf/namesrv.properties"]

制作镜像
[root@master-1 nameserver_image]#docker build -f ./Dockerfile -t rocketmq_namesrv:0.1 . 



#####2.3）制作rocketmq的web可视化界面的image镜像
下载地址：https://pan.baidu.com/s/19PROiNivWBiOIV5NkOBBqA
提取密码：r1cf

[root@master-1 image]# mkdir rocketmq-externals_image
[root@master-1 rocketmq-externals_image]# wget https://dlcdn.apache.org/maven/maven-3/3.8.4/binaries/apache-maven-3.8.4-bin.tar.gz

[root@master-1 rocketmq-externals_image]# ll
total 29900
-rw-r--r--  1 root root  9046177 Nov 14 21:25 apache-maven-3.8.4-bin.tar.gz
drwxr-xr-x 17 root root     4096 May  9  2018 rocketmq-externals
-rw-r--r--  1 root root 21562027 Dec 22 00:03 rocketmq-externals.tar.gz

[root@master-1 rocketmq-externals_image]#tar -zvxf rocketmq-externals.tar.gz
[root@master-1 rocketmq-externals_image]#vim rocketmq-externals/rocketmq-console/src/main/resources/application.properties
server.contextPath=
server.port=8080                 #默认访问端口是8080
spring.application.name=rocketmq-console
spring.http.encoding.charset=UTF-8
spring.http.encoding.enabled=true
spring.http.encoding.force=true
logging.config=classpath:logback.xml
rocketmq.config.namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901          #如果nameserver是多台集群机器，则后面就配置多个ip+port，即"rocketmq.config.namesrvAddr=ip1:port;ip2:port"
rocketmq.config.isVIPChannel=true               #注意这个参数，是否设置为false或true取决于rocketmq的版本号。rocketmq低于3.5.8版本,设置为false，默认为true！
rocketmq.config.dataPath=/tmp/rocketmq-console/data
rocketmq.config.enableDashBoardCollect=true

删除之前的包，重新打包
[root@master-1 rocketmq-externals_image]#rm -rf rocketmq-externals.tar.gz
[root@master-1 rocketmq-externals_image]# tar -zvcf rocketmq-externals.tar.gz rocketmq-externals
[root@master-1 rocketmq-externals_image]# rm -rf rocketmq-externals
[root@master-1 rocketmq-externals_image]# ll
total 29896
-rw-r--r-- 1 root root  9046177 Nov 14 21:25 apache-maven-3.8.4-bin.tar.gz
-rw-r--r-- 1 root root 21562564 Dec 22 00:09 rocketmq-externals.tar.gz

Dockerfile镜像文件内容：
[root@master-1 rocketmq-externals_image]# vim Dockerfile
FROM centos:latest
 
RUN rm -f /etc/localtime \
&& ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
&& echo "Asia/Shanghai" > /etc/timezone
 
ENV LANG en_US.UTF-8
ENV MAVEN_HOME /usr/local/maven
ENV PATH $PATH:$MAVEN_HOME/bin
ENV JAVA_HOME=/usr/local/jdk1.8.0_311
ENV CLASSPATH=.$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
ENV PATH=$JAVA_HOME/bin:$PATH
 
ADD jdk-8u311-linux-x64.tar.gz /usr/local/
ADD rocketmq-externals.tar.gz /usr/local
ADD apache-maven-3.8.4-bin.tar.gz /usr/local
RUN mv /usr/local/apache-maven-3.8.4 /usr/local/maven
WORKDIR /usr/local/rocketmq-externals/rocketmq-console/
RUN mvn clean package -Dmaven.test.skip=true
 
 
WORKDIR /usr/local/rocketmq-externals/rocketmq-console/target
EXPOSE 8080
CMD ["nohup","java","-jar","rocketmq-console-ng-1.0.0.jar","&"]


制作镜像(时间很长)
[root@master-1 rocketmq_image]# docker build -f ./Dockerfile -t rocketmq-externals:0.1 . 
...............
Downloaded from central: https://repo.maven.apache.org/maven2/org/apache/maven/shared/maven-shared-utils/3.0.1/maven-shared-utils-3.0.1.jar (154 kB at 6.3 kB/s)
[INFO] Building jar: /usr/local/rocketmq-externals/rocketmq-console/target/rocketmq-console-ng-1.0.0-sources.jar
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  30:40 min
[INFO] Finished at: 2021-12-22T00:50:23+08:00
[INFO] ------------------------------------------------------------------------
Removing intermediate container bb66475d73ac
 ---> 352f959f6a96
Step 15/17 : WORKDIR /usr/local/rocketmq-externals/rocketmq-console/target
 ---> Running in 4b7344848b93
Removing intermediate container 4b7344848b93
 ---> abdd7154e6d4
Step 16/17 : EXPOSE 8080
 ---> Running in 4d4e944f7299
Removing intermediate container 4d4e944f7299
 ---> 01f6dd084366
Step 17/17 : CMD ["nohup","java","-jar","rocketmq-console-ng-1.0.0.jar","&"]
 ---> Running in 9e4a4195c4b8
Removing intermediate container 9e4a4195c4b8
 ---> b9a3d21b08c9
Successfully built b9a3d21b08c9
Successfully tagged rocketmq-externals:0.1
[root@master-1 rocketmq-externals_image]# 


3个景象完成：
[root@master-1 rocketmq-externals_image]# docker images|grep rocketmq
rocketmq-externals                                                0.1                 b9a3d21b08c9        About a minute ago   787MB
rocketmq_namesrv                                                  0.1                 d7f093a52ae4        6 hours ago          619MB
rocketmq                                                          0.1                 6392bb133d05        6 hours ago          619MB
[root@master-1 rocketmq-externals_image]# 

[root@master-1 rocketmq-externals_image]# ll
total 173260
-rw-r--r-- 1 root root   9046177 Nov 14 21:25 apache-maven-3.8.4-bin.tar.gz
-rw-r--r-- 1 root root       793 Dec 22 00:19 Dockerfile
-rw-r--r-- 1 root root 146799982 Dec 21 00:38 jdk-8u311-linux-x64.tar.gz
-rw-r--r-- 1 root root  21562564 Dec 22 00:09 rocketmq-externals.tar.gz


#######注意：如果没有仓库，每个节点都要做景象


####3、准备配置文件，创建configmap
RocketMQ默认提供的配置文件都是最基本的，很多配置都是默认值，在生产环境中我们需要根据实际情况进行修改。
本案例中使用的Pod的namespace命名空间是 wiseco。
broker的端口是20911、nameserver的端口是20901


[root@master-1 rocketmq]# ll
total 12
drwxr-xr-x 5 root root   84 Dec 22 00:25 image
-rw-r--r-- 1 root root 1216 Dec 21 18:25 nfs-rbac.yaml
-rw-r--r-- 1 root root  161 Dec 21 18:26 rocketmq-nfs-class.yaml
-rw-r--r-- 1 root root 1029 Dec 21 18:28 rocketmq-nfs.yml

[root@master-1 rocketmq]# mkdir config
[root@master-1 rocketmq]# cd config/
[root@master-1 rocketmq]# ll
total 12
drwxr-xr-x 2 root root    6 Dec 22 00:25 config
drwxr-xr-x 5 root root   84 Dec 22 00:25 image
-rw-r--r-- 1 root root 1216 Dec 21 18:25 nfs-rbac.yaml
-rw-r--r-- 1 root root  161 Dec 21 18:26 rocketmq-nfs-class.yaml
-rw-r--r-- 1 root root 1029 Dec 21 18:28 rocketmq-nfs.yml
[root@master-1 rocketmq]# 

在config目录下编辑：
RocketMQ"双主双从集群模式"的四个配置文件。
以及nameserver配置文件

[root@master-1 config]# ll
total 16
-rw-r--r-- 1 root root 438 Dec 22 00:26 broker-a.properties
-rw-r--r-- 1 root root 462 Dec 22 00:26 broker-a-s.properties
-rw-r--r-- 1 root root 463 Dec 22 00:27 broker-b.properties
-rw-r--r-- 1 root root 462 Dec 22 00:27 broker-b-s.properties

broker-a.properties 配置文件：
[root@k8s-master01 config]# cat broker-a.properties
brokerClusterName=rocketmq-cluster
brokerName=broker-a
brokerId=0
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=MASTER
　　

broker-a-s.properties 配置文件：
[root@k8s-master01 config]# cat broker-a-s.properties           
brokerClusterName=rocketmq-cluster
brokerName=broker-a
brokerId=1
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=SLAVE
flushDiskType=SYNC_FLUSH
　　

broker-b.properties 配置文件：
[root@k8s-master01 config]# cat broker-b.properties
brokerClusterName=rocketmq-cluster
brokerName=broker-b
brokerId=0
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=MASTER
flushDiskType=SYNC_FLUSH
　　

broker-b-s.properties 配置文件：
[root@k8s-master01 config]# cat broker-b-s.properties
brokerClusterName=rocketmq-cluster
brokerName=broker-b
brokerId=1
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=SLAVE
flushDiskType=SYNC_FLUSH


依据上面rocketmq的4个配置文件创建configmap存储卷
[root@master-1 config]# pwd
/root/mq/rocketmq/config
[root@master-1 config]#  
[root@master-1 config]# ll
total 16
-rw-r--r-- 1 root root 438 Dec 22 00:26 broker-a.properties
-rw-r--r-- 1 root root 462 Dec 22 00:26 broker-a-s.properties
-rw-r--r-- 1 root root 463 Dec 22 00:27 broker-b.properties
-rw-r--r-- 1 root root 462 Dec 22 00:27 broker-b-s.properties
[root@master-1 config]# 


[root@master-1 config]# kubectl create configmap rocketmq-config --from-file=broker-a.properties --from-file=broker-b.properties --from-file=broker-a-s.properties --from-file=broker-b-s.properties  -n wiseco
configmap/rocketmq-config created

[root@master-1 config]# kubectl get cm -n wiseco|grep rocketmq
rocketmq-config   4      11s

[root@master-1 config]#  kubectl describe cm rocketmq-config -n wiseco
Name:         rocketmq-config
Namespace:    wiseco
Labels:       <none>
Annotations:  <none>

Data
====
broker-b-s.properties:
----
brokerClusterName=rocketmq-cluster
brokerName=broker-b
brokerId=1
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=SLAVE
flushDiskType=SYNC_FLUSH

broker-b.properties:
----
brokerClusterName=rocketmq-cluster
brokerName=broker-b
brokerId=0
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=MASTER
flushDiskType=SYNC_FLUSH

broker-a-s.properties:
----
brokerClusterName=rocketmq-cluster
brokerName=broker-a
brokerId=1
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=SLAVE
flushDiskType=SYNC_FLUSH

broker-a.properties:
----
brokerClusterName=rocketmq-cluster
brokerName=broker-a
brokerId=0
namesrvAddr=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
defaultTopicQueueNums=4
autoCreateTopicEnable=true
autoCreateSubscriptionGroup=true
listenPort=20911
deleteWhen=04
fileReservedTime=120
mapedFileSizeCommitLog=1073741824
mapedFileSizeConsumeQueue=300000
diskMaxUsedSpaceRatio=88
storePathRootDir=/data/rocketmq/store
maxMessageSize=65536
brokerRole=MASTER

Events:  <none>
[root@master-1 config]# 


####4、部署rocketmq容器实例
启动命令-c可以指定配置文件，NameSvrAddr使用配置文件内指定的，配置文件使用前面的configmap挂载进来，subPath可以指定具体文件和目录。使用StatefulSet控制器部署rocketmq-a和rocketmq-b 两个master的pod。

[root@master-1 rocketmq]# pwd
/root/mq/rocketmq
[root@master-1 rocketmq]# ll
total 12
drwxr-xr-x 2 root root  118 Dec 22 00:27 config
drwxr-xr-x 5 root root   84 Dec 22 00:25 image
-rw-r--r-- 1 root root 1216 Dec 21 18:25 nfs-rbac.yaml
-rw-r--r-- 1 root root  161 Dec 21 18:26 rocketmq-nfs-class.yaml
-rw-r--r-- 1 root root 1029 Dec 21 18:28 rocketmq-nfs.yml
[root@master-1 rocketmq]# 

[root@master-1 rocketmq]# mkdir deploy
[root@master-1 rocketmq]# cd deploy/
[root@master-1 deploy]# ll
total 24
-rw-r--r-- 1 root root 2261 Dec 22 00:34 broker-a-deployment.yaml
-rw-r--r-- 1 root root 2380 Dec 22 00:35 broker-a-s-deployment.yaml
-rw-r--r-- 1 root root 2260 Dec 22 00:36 broker-b-deployment.yaml
-rw-r--r-- 1 root root 2380 Dec 22 00:38 broker-b-s-deployment.yaml
-rw-r--r-- 1 root root 1419 Dec 22 00:39 namesrv-deployment.yaml
-rw-r--r-- 1 root root 1352 Dec 22 00:49 rocketmq-externals-deployment.yaml
[root@master-1 deploy]# 


broker-a-deployment.yaml 文件内容:
[root@master-1 deploy]# vim broker-a-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: broker-a
  name: broker-a
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 20911
    targetPort: 20911
    name: broker-port
    nodePort: 30911
  selector:
    app: broker-a
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: broker-a
  namespace: wiseco
spec:
  serviceName: broker-a
  replicas: 1
  selector:
    matchLabels:
      app: broker-a
  template:
    metadata:
     labels:
       app: broker-a
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - broker-a
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: broker-a
        image: lex921/rocketmq:0.1
        imagePullPolicy: Always
        command: ["sh","-c","/usr/local/rocketmq-4.9.2/bin/mqbroker  -c /usr/local/rocketmq-4.9.2/conf/broker-a.properties"]
        volumeMounts:
          - mountPath: /root/logs
            name: rocketmq-data
            subPath: mq-brokeroptlogs
          - mountPath: /data/rocketmq
            name: rocketmq-data
            subPath: mq-brokeroptstore
          - name: broker-config
            mountPath: /usr/local/rocketmq-4.9.2/conf/broker-a.properties
            subPath: broker-a.properties
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 20911
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
      volumes:
      - name: broker-config
        configMap:
          name: rocketmq-config
  volumeClaimTemplates:
  - metadata:
      name: rocketmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: "rocketmq-nfs-storage"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 10Gi

broker-a-s-deployment.yaml 文件内容:
[root@master-1 deploy]# vim broker-a-s-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: broker-a-s
  name: broker-a-s
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 20911
    targetPort: 20911
    name: broker-port
    nodePort: 30912
  selector:
    app: broker-a-s
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: broker-a-s
  namespace: wiseco
spec:
  serviceName: broker-a-s
  replicas: 1
  selector:
    matchLabels:
      app: broker-a-s
  template:
    metadata:
     labels:
       app: broker-a-s
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - broker-a-s
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: broker-a-s
        image: lex921/rocketmq:0.1
        imagePullPolicy: Always
        command: ["sh","-c","/usr/local/rocketmq-4.9.2/bin/mqbroker  -c /usr/local/rocketmq-4.9.2/conf/broker-a-s.properties"]
        volumeMounts:
          - mountPath: /root/logs
            name: rocketmq-data
            subPath: mq-brokeroptlogs
          - mountPath: /data/rocketmq
            name: rocketmq-data
            subPath: mq-brokeroptstore
          - name: broker-config
            mountPath: /usr/local/rocketmq-4.9.2/conf/broker-a-s.properties
            subPath: broker-a-s.properties
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 20911
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
      volumes:
      - name: broker-config
        configMap:
          name: rocketmq-config
          items:
          - key: broker-a-s.properties
            path: broker-a-s.properties
  volumeClaimTemplates:
  - metadata:
      name: rocketmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: "rocketmq-nfs-storage"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 10Gi

broker-b-deployment.yaml 文件内容:
[root@master-1 deploy]# vim broker-b-deployment.yaml  
apiVersion: v1
kind: Service
metadata:
  labels:
    app: broker-b
  name: broker-b
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 20911
    targetPort: 20911
    name: broker-port
    nodePort: 30913
  selector:
    app: broker-b
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: broker-b
  namespace: wiseco
spec:
  serviceName: broker-b
  replicas: 1
  selector:
    matchLabels:
      app: broker-b
  template:
    metadata:
     labels:
       app: broker-b
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - broker-b
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: broker-b
        image: lex921/rocketmq:0.1
        imagePullPolicy: Always
        command: ["sh","-c","/usr/local/rocketmq-4.9.2/bin/mqbroker  -c /usr/local/rocketmq-4.9.2/conf/broker-b.properties"]
        volumeMounts:
          - mountPath: /root/logs
            name: rocketmq-data
            subPath: mq-brokeroptlogs
          - mountPath: /data/rocketmq
            name: rocketmq-data
            subPath: mq-brokeroptstore
          - name: broker-config
            mountPath: /usr/local/rocketmq-4.9.2/conf/broker-b.properties
            subPath: broker-b.properties
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 20911
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
      volumes:
      - name: broker-config
        configMap:
          name: rocketmq-config
  volumeClaimTemplates:
  - metadata:
      name: rocketmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: "rocketmq-nfs-storage"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 10Gi

broker-b-s-deployment.yaml 文件内容:
[root@master-1 deploy]# vim broker-b-s-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: broker-b-s
  name: broker-b-s
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 20911
    targetPort: 20911
    name: broker-port
    nodePort: 30914
  selector:
    app: broker-b-s
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: broker-b-s
  namespace: wiseco
spec:
  serviceName: broker-b-s
  replicas: 1
  selector:
    matchLabels:
      app: broker-b-s
  template:
    metadata:
     labels:
       app: broker-b-s
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - broker-b-s
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: broker-b-s
        image: lex921/rocketmq:0.1
        imagePullPolicy: Always
        command: ["sh","-c","/usr/local/rocketmq-4.9.2/bin/mqbroker  -c /usr/local/rocketmq-4.9.2/conf/broker-b-s.properties"]
        volumeMounts:
          - mountPath: /root/logs
            name: rocketmq-data
            subPath: mq-brokeroptlogs
          - mountPath: /data/rocketmq
            name: rocketmq-data
            subPath: mq-brokeroptstore
          - name: broker-config
            mountPath: /usr/local/rocketmq-4.9.2/conf/broker-b-s.properties
            subPath: broker-b-s.properties
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 20911
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
      volumes:
      - name: broker-config
        configMap:
          name: rocketmq-config
          items:
          - key: broker-b-s.properties
            path: broker-b-s.properties
  volumeClaimTemplates:
  - metadata:
      name: rocketmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: "rocketmq-nfs-storage"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 10Gi

namesrv-deployment.yaml文件内容:
[root@master-1 deploy]# vim namesrv-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: mq-namesrv
  name: mq-namesrv
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 20901
    targetPort: 20901
    name: namesrv-port
    nodePort: 30915
  selector:
    app: mq-namesrv
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mq-namesrv
  namespace: wiseco
spec:
  serviceName: mq-namesrv
  replicas: 1
  selector:
    matchLabels:
      app: mq-namesrv
  template:
    metadata:
     labels:
       app: mq-namesrv
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - mq-namesrv
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: mq-namesrv
        image: lex921/rocketmq_namesrv:0.1
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 20901
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20

rocketmq-externals-deployment.yaml 文件内容:
[root@master-1 deploy]# vim rocketmq-externals-deployment.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: mq-externals
  name: mq-externals
  namespace: wiseco
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 8080
    name: console-port
    nodePort: 30916
  selector:
    app: mq-externals
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mq-externals
  namespace: wiseco
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mq-externals
  template:
    metadata:
     labels:
       app: mq-externals
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - mq-externals
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: mq-externals
        image: lex921/rocketmq-externals:0.1
        imagePullPolicy: Always
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: 8080
          initialDelaySeconds: 15


[root@master-1 deploy]# ll
total 24
-rw-r--r-- 1 root root 2261 Dec 22 00:34 broker-a-deployment.yaml
-rw-r--r-- 1 root root 2380 Dec 22 00:35 broker-a-s-deployment.yaml
-rw-r--r-- 1 root root 2260 Dec 22 00:36 broker-b-deployment.yaml
-rw-r--r-- 1 root root 2380 Dec 22 00:38 broker-b-s-deployment.yaml
-rw-r--r-- 1 root root 1419 Dec 22 00:39 namesrv-deployment.yaml
-rw-r--r-- 1 root root 1352 Dec 22 00:49 rocketmq-externals-deployment.yaml
[root@master-1 deploy]# 


部署rockermq相应pod
kubectl delete -f .
[root@master-1 deploy]# kubectl apply -f .
service/broker-a created
statefulset.apps/broker-a created
service/broker-a-s created
statefulset.apps/broker-a-s created
service/broker-b created
statefulset.apps/broker-b created
service/broker-b-s created
statefulset.apps/broker-b-s created
service/mq-namesrv created
statefulset.apps/mq-namesrv created
service/mq-externals created
deployment.apps/mq-externals created
[root@master-1 deploy]# 




https://hub.docker.com/repositories

lex921/cicd


上传景象
[root@master-1 deploy]# docker tag rocketmq_namesrv:0.1 lex921/rocketmq_namesrv:0.1
[root@master-1 deploy]# docker push  lex921/rocketmq_namesrv:0.1
The push refers to repository [docker.io/lex921/rocketmq_namesrv]
0b9262c74c5a: Pushed 
d83460144beb: Pushed 
6010b0dafdb6: Pushed 
541300130327: Pushed 
74ddd0ec08fa: Pushed 
0.1: digest: sha256:fb1c0478997be5ca4b9494597632d38108eb7c910de84e72e99cb41099226b40 size: 1368
[root@master-1 deploy]# 


[root@node-1 rocketmq-externals_image]# docker tag rocketmq:0.1 lex921/rocketmq:0.1
[root@node-1 rocketmq-externals_image]# docker push  lex921/rocketmq:0.1
The push refers to repository [docker.io/lex921/rocketmq]
098cfcd3710c: Pushed 
cd706737122a: Pushed 
43984db81f91: Pushed 
f798b2957a9d: Pushed 
74ddd0ec08fa: Pushed 
0.1: digest: sha256:57a477df020a7db65e6a48986d59963bf3d6be6ce930501c1fb3186d7a539de0 size: 1368



[root@master-1 deploy]# docker tag rocketmq-externals:0.1 lex921/rocketmq-externals:0.1
[root@master-1 deploy]# docker push  lex921/rocketmq-externals:0.1
The push refers to repository [docker.io/lex921/rocketmq-externals]
2cb9e1568757: Pushed 
ec8d84e06198: Pushed 
d2508531e85b: Pushed 
dcb9a972b7e0: Pushed 
cd8b1ce8fe99: Pushed 
541300130327: Mounted from lex921/rocketmq_namesrv 
74ddd0ec08fa: Mounted from lex921/rocketmq_namesrv 
0.1: digest: sha256:d908a07c681fc05c928bf02598eff45b50730b4d0d37e439ce728405e5e87350 size: 1795
[root@master-1 deploy]# 


kubectl delete -f broker-a-deployment.yaml
kubectl apply -f broker-a-deployment.yaml
kubectl delete -f 
kubectl apply -f
kubectl  describe pod  -n wiseco mq-namesrv-0




查看pod、svc
[root@k8s-master01 deploy]# kubectl get pods -n wiseco

[root@master-1 deploy]# kubectl get svc -n wiseco    
NAME         TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE
broker-a     NodePort   10.1.143.46    <none>        20911:30911/TCP   21m
broker-a-s   NodePort   10.1.101.224   <none>        20911:30912/TCP   16m
broker-b     NodePort   10.1.158.168   <none>        20911:30913/TCP   12m
broker-b-s   NodePort   10.1.225.22    <none>        20911:30914/TCP   12m

[root@master-1 deploy]# kubectl get pods -A
NAMESPACE     NAME                                               READY   STATUS    RESTARTS   AGE
default       activemq-deployment-68b8596b6b-mn9fw               1/1     Running   1          24h
default       nginx-6799fc88d8-xccsn                             1/1     Running   2          24h
kube-system   coredns-6d56c8448f-42ldb                           1/1     Running   2          25h
kube-system   coredns-6d56c8448f-w2cls                           1/1     Running   2          25h
kube-system   etcd-master-1                                      1/1     Running   4          25h
kube-system   kube-apiserver-master-1                            1/1     Running   4          25h
kube-system   kube-controller-manager-master-1                   1/1     Running   2          24h
kube-system   kube-flannel-ds-8bhd2                              1/1     Running   2          24h
kube-system   kube-flannel-ds-s7s8w                              1/1     Running   2          24h
kube-system   kube-flannel-ds-xw49t                              1/1     Running   6          24h
kube-system   kube-proxy-m7wcs                                   1/1     Running   4          25h
kube-system   kube-proxy-p8zrt                                   1/1     Running   3          25h
kube-system   kube-proxy-tr7wx                                   1/1     Running   3          24h
kube-system   kube-scheduler-master-1                            1/1     Running   2          24h
wiseco        broker-a-0                                         1/1     Running   1          34m
wiseco        broker-a-s-0                                       1/1     Running   1          30m
wiseco        broker-b-0                                         1/1     Running   1          26m
wiseco        broker-b-s-0                                       1/1     Running   0          25m
wiseco        rocketmq-nfs-client-provisioner-5bc9679c86-gsqws   1/1     Running   1          13h
[root@master-1 deploy]# 


[root@master-1 deploy]# kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS           REASON   AGE
pvc-0d973e7f-679a-44e7-9bae-7a7910149098   10Gi       RWX            Delete           Bound    wiseco/rocketmq-data-broker-a-s-0   rocketmq-nfs-storage            12h
pvc-b504d16b-d25f-47ed-a0ec-dbd571be4d20   10Gi       RWX            Delete           Bound    wiseco/rocketmq-data-broker-b-0     rocketmq-nfs-storage            12h
pvc-cfd76063-fd78-41e1-a216-eea00ac02ddd   10Gi       RWX            Delete           Bound    wiseco/rocketmq-data-broker-b-s-0   rocketmq-nfs-storage            12h
pvc-f7380bb2-9585-4c65-86e3-4ec17458c0b9   10Gi       RWX            Delete           Bound    wiseco/rocketmq-data-broker-a-0     rocketmq-nfs-storage            12h

[root@master-1 deploy]# kubectl get pvc -n wiseco
NAME                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS           AGE
rocketmq-data-broker-a-0     Bound    pvc-f7380bb2-9585-4c65-86e3-4ec17458c0b9   10Gi       RWX            rocketmq-nfs-storage   12h
rocketmq-data-broker-a-s-0   Bound    pvc-0d973e7f-679a-44e7-9bae-7a7910149098   10Gi       RWX            rocketmq-nfs-storage   12h
rocketmq-data-broker-b-0     Bound    pvc-b504d16b-d25f-47ed-a0ec-dbd571be4d20   10Gi       RWX            rocketmq-nfs-storage   12h
rocketmq-data-broker-b-s-0   Bound    pvc-cfd76063-fd78-41e1-a216-eea00ac02ddd   10Gi       RWX            rocketmq-nfs-storage   12h

[root@k8s-master01 deploy]# 


查看NFS共享存储
NFS服务器（172.16.60.238），查看共享目录/data/storage/k8s/rocketmq
[root@master-1 deploy]# cd /data/storage/k8s/rocketmq/
[root@master-1 rocketmq]# ll
total 0
drwxrwxrwx 4 root root 55 Feb  3 00:02 wiseco-rocketmq-data-broker-a-0-pvc-77d7358e-ee27-49f2-bf29-7c31fa6250b1
drwxrwxrwx 4 root root 55 Feb  3 00:02 wiseco-rocketmq-data-broker-a-s-0-pvc-4d31cdfb-94c3-4973-b3c3-e62ce88ae29e
drwxrwxrwx 4 root root 55 Feb  3 00:02 wiseco-rocketmq-data-broker-b-0-pvc-d1d06c6b-2b3d-43e3-88cf-0d56e2eb8bd7
drwxrwxrwx 4 root root 55 Feb  3 00:02 wiseco-rocketmq-data-broker-b-s-0-pvc-f05d79c8-c2b0-4f00-b300-7db23a3a0c28

[root@master-1 rocketmq]#  ll wiseco-rocketmq-data-broker-a-0-pvc-77d7358e-ee27-49f2-bf29-7c31fa6250b1/
total 0
drwxrwxrwx 3 root root 26 Feb  3 00:02 mq-brokeroptlogs
drwxrwxrwx 3 root root 19 Feb  3 00:02 mq-brokeroptstore

[root@master-1 rocketmq]#  ll wiseco-rocketmq-data-broker-a-0-pvc-77d7358e-ee27-49f2-bf29-7c31fa6250b1/mq-brokeroptstore/
total 0
drwxr-xr-x 3 root root 63 Feb  3 00:02 store

[root@master-1 rocketmq]#  ll wiseco-rocketmq-data-broker-a-0-pvc-77d7358e-ee27-49f2-bf29-7c31fa6250b1/mq-brokeroptstore/store/
total 8
-rw-r--r-- 1 root root    0 Feb  3 00:02 abort
-rw-r--r-- 1 root root 4096 Feb  3 00:43 checkpoint
drwxr-xr-x 2 root root  193 Feb  3 00:44 config
-rw-r--r-- 1 root root    4 Feb  3 00:02 lock


####5、rockermq的可视化界面
访问http://node节点ip:30916

点点点：
Cluster --> OPS 可以看到域名


####6、tool.sh工具测试收发消息
自动创建开启后，可以利用mq自动的tool.sh工具测试收发消息。「 注意是开启了自动创建topic后才行 」
[root@master-1 rocketmq]# kubectl get pods -n wiseco
NAME                                               READY   STATUS    RESTARTS   AGE
broker-a-0                                         1/1     Running   0          9h
broker-a-s-0                                       1/1     Running   0          9h
broker-b-0                                         1/1     Running   0          9h
broker-b-s-0                                       1/1     Running   0          9h
mq-externals-57c9ddddcb-cffj5                      1/1     Running   0          9h
mq-namesrv-0                                       1/1     Running   0          9h
rocketmq-nfs-client-provisioner-59d8bbd947-k294r   1/1     Running   1          46h
 
[root@master-1 rocketmq]# kubectl exec -ti broker-a-0 -n wiseco -- /bin/bash
[root@broker-a-0 /]# cd /usr/local/rocketmq-4.9.2/bin/
 
[root@broker-a-0 bin]# export NAMESRV_ADDR=mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
[root@broker-a-0 bin]# echo $NAMESRV_ADDR
mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901

生产者发送消息
执行命令后，稍微等一下，等到出现下面的输出结果即为正常
[root@broker-a-0 bin]# ./tools.sh org.apache.rocketmq.example.quickstart.Producer
10:00:21.965 [main] DEBUG i.n.u.i.l.InternalLoggerFactory - Using SLF4J as the default logging framework
RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).
RocketMQLog:WARN Please initialize the logger system properly.
SendResult [sendStatus=SEND_OK, msgId=7F00000148894DC639960C7260600000, offsetMsgId=AC1E55D0000051AF0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-a, queueId=0], queueOffset=0]
SendResult [sendStatus=SEND_OK, msgId=7F00000148894DC639960C7262B70001, offsetMsgId=AC1E55D0000051AF00000000000000CB, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-a, queueId=1], queueOffset=0]
SendResult [sendStatus=SEND_OK, msgId=7F00000148894DC639960C7262CE0002, offsetMsgId=AC1E55D0000051AF0000000000000196, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-a, queueId=2], queueOffset=0]
SendResult [sendStatus=SEND_OK, msgId=7F00000148894DC639960C7262E80003, offsetMsgId=AC1E55D0000051AF0000000000000261, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-a, queueId=3], queueOffset=0]
SendResult [sendStatus=SEND_OK, msgId=7F00000148894DC639960C7262F60004, offsetMsgId=AC1ED946000051AF0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=broker-b, queueId=0], queueOffset=0]
................
................
 
消费者接受消息：
执行命令后，稍微等一下，等到出现下面的输出结果即为正常
[root@broker-a-0 bin]# ./tools.sh org.apache.rocketmq.example.quickstart.Consumer
10:04:02.870 [main] DEBUG i.n.u.i.l.InternalLoggerFactory - Using SLF4J as the default logging framework
Consumer Started.
ConsumeMessageThread_1 Receive New Messages: [MessageExt [brokerName=broker-b, queueId=0, storeSize=203, queueOffset=8, sysFlag=0, bornTimestamp=1612317805388, bornHost=/172.16.201.134:42326, storeTimestamp=1612317805389, storeHost=/172.30.217.70:20911, msgId=AC1ED946000051AF00000000000010B6, commitLogOffset=4278, bodyCRC=1032136437, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=20, CONSUME_START_TIME=1612317843435, UNIQ_KEY=7F00000149474DC639960C752B4C0003, CLUSTER=rocketmq-cluster, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 51], transactionId='null'}]]
ConsumeMessageThread_3 Receive New Messages: [MessageExt [brokerName=broker-b, queueId=0, storeSize=204, queueOffset=10, sysFlag=0, bornTimestamp=1612317806220, bornHost=/172.16.201.134:42326, storeTimestamp=1612317806222, storeHost=/172.30.217.70:20911, msgId=AC1ED946000051AF0000000000001712, commitLogOffset=5906, bodyCRC=1918600882, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=20, CONSUME_START_TIME=1612317843435, UNIQ_KEY=7F00000149474DC639960C752E8C0013, CLUSTER=rocketmq-cluster, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 49, 57], transactionId='null'}]]
ConsumeMessageThread_2 Receive New Messages: [MessageExt [brokerName=broker-b, queueId=0, storeSize=204, queueOffset=9, sysFlag=0, bornTimestamp=1612317805826, bornHost=/172.16.201.134:42326, storeTimestamp=1612317805828, storeHost=/172.30.217.70:20911, msgId=AC1ED946000051AF00000000000013E2, commitLogOffset=5090, bodyCRC=2088767104, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=20, CONSUME_START_TIME=1612317843435, UNIQ_KEY=7F00000149474DC639960C752D02000B, CLUSTER=rocketmq-cluster, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 49, 49], transactionId='null'}]]
ConsumeMessageThread_4 Receive New Messages: [MessageExt [brokerName=broker-b, queueId=0, storeSize=204, queueOffset=11, sysFlag=0, bornTimestamp=1612317806563, bornHost=/172.16.201.134:42326, storeTimestamp=1612317806565, storeHost=/172.30.217.70:20911, msgId=AC1ED946000051AF0000000000001A42, commitLogOffset=6722, bodyCRC=1053751414, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic='TopicTest', flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=20, CONSUME_START_TIME=1612317843437, UNIQ_KEY=7F00000149474DC639960C752FE3001B, CLUSTER=rocketmq-cluster, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 50, 55], transactionId='null'}]]
................
................
 

手动测试新增topic
[root@broker-a-0 bin]# sh mqadmin updateTopic -c rocketmq-cluster -n mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901 -t testtopic
RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).
RocketMQLog:WARN Please initialize the logger system properly.
create topic to 172.16.201.134:20911 success.
create topic to 172.30.217.70:20911 success.
TopicConfig [topicName=testtopic, readQueueNums=8, writeQueueNums=8, perm=RW-, topicFilterType=SINGLE_TAG, topicSysFlag=0, order=false]
 

查看topic列表：
[root@broker-a-0 bin]# sh mqadmin topicList -n mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901
RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0).
RocketMQLog:WARN Please initialize the logger system properly.
RMQ_SYS_TRANS_HALF_TOPIC
rocketmq-cluster
%RETRY%please_rename_unique_group_name_4
BenchmarkTest
OFFSET_MOVED_EVENT
TBW102
SELF_TEST_TOPIC
SCHEDULE_TOPIC_XXXX
testtopic
rocketmq-cluster_REPLY_TOPIC
broker-b
TopicTest
broker-a

再次查看可视化界面
点 Dasboard  表格数据出来了


点Topic : testtopic 显示存在


####7、客户端连接RockerMQ集群
RocketMQ可以令客户端找到Name Server, 然后通过Name Server再找到Broker。如下所示有多种配置方式，优先级由高到低，高优先级会覆盖低优先级；
代码中指定Name Server地址，多个namesrv地址之间用分号分割：
producer.setNamesrvAddr("mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901");
consumer.setNamesrvAddr("mq-namesrv-0.mq-namesrv.wiseco.svc.cluster.local:20901");
　　

####8、参考
https://www.cnblogs.com/kevingrace/p/14366671.html


##(二十二)、k8s部署RabbitMQ

参考
https://www.cnblogs.com/kevingrace/p/14412360.html


RabbitMQ 简介
以熟悉的电商场景为例，如果商品服务和订单服务是两个不同的微服务，在下单的过程中订单服务需要调用商品服务进行扣库存操作。按照传统的方式，下单过程要等到调用完毕之后才能返回下单成功，如果网络产生波动等原因使得商品服务扣库存延迟或者失败，会带来较差的用户体验，如果在高并发的场景下，这样的处理显然是不合适的，那怎么进行优化呢？这就需要消息队列登场了。
 
消息队列提供一个异步通信机制，消息的发送者不必一直等待到消息被成功处理才返回，而是立即返回。消息中间件负责处理网络通信，如果网络连接不可用，消息被暂存于队列当中，当网络畅通的时候在将消息转发给相应的应用程序或者服务，当然前提是这些服务订阅了该队列。如果在商品服务和订单服务之间使用消息中间件，既可以提高并发量，又降低服务之间的耦合度。
 
RabbitMQ就是这样一款我们苦苦追寻的消息队列。RabbitMQ是一个开源的消息代理的队列服务器，用来通过普通协议在完全不同的应用之间共享数据。
 
RabbitMQ 的特点
开源、性能优秀，速度快，稳定性保障提供可靠性消息投递模式、返回模式与Spring AMQP完美整合，API丰富集群模式丰富，表达式配置，HA模式，镜像队列模型保证数据不丢失的前提做到高可靠性、可用性
 
RabbitMQ 典型应用场景
异步处理：把消息放入消息中间件中，等到需要的时候再去处理。
流量削峰：例如秒杀活动，在短时间内访问量急剧增加，使用消息队列，当消息队列满了就拒绝响应，跳转到错误页面，这样就可以使得系统不会因为超负载而崩溃。
日志处理；（不过一般日志处理都使用Kafka这种消息队列）
应用解耦：假设某个服务A需要给许多个服务(B、C、D)发送消息，当某个服务(例如B)不需要发送消息了，服务A需要改代码再次部署；当新加入一个服务(服务E)需要服务A的消息的时候，也需要改代码重新部署；另外服务A也要考虑其他服务挂掉，没有收到消息怎么办？要不要重新发送呢？是不是很麻烦，使用MQ发布订阅模式，服务A只生产消息发送到MQ，B、C、D从MQ中读取消息，需要A的消息就订阅，不需要了就取消订阅，服务A不再操心其他的事情，使用这种方式可以降低服务或者系统之间的耦合。
RabbitMQ集群节点之间是如何相互认证的：
通过Erlang Cookie，相当于共享秘钥的概念，长度任意，只要所有节点都一致即可。
rabbitmq server在启动的时候，erlang VM会自动创建一个随机的cookie文件。cookie文件的位置是/var/lib/rabbitmq/.erlang.cookie 或者 /root/.erlang.cookie，为保证cookie的完全一致，采用从一个节点copy的方式。
Erlang Cookie是保证不同节点可以相互通信的密钥，要保证集群中的不同节点相互通信必须共享相同的Erlang Cookie。具体的目录存放在/var/lib/rabbitmq/.erlang.cookie。
 
说明：这就要从rabbitmqctl命令的工作原理说起，RabbitMQ底层是通过Erlang架构来实现的，所以rabbitmqctl会启动Erlang节点，并基于Erlang节点来使用Erlang系统连接RabbitMQ节点，在连接过程中需要正确的Erlang Cookie和节点名称，Erlang节点通过交换Erlang Cookie以获得认证。
 
RabbitMQ集群模式

单机模式
普通集群模式（无高可用性）
镜像集群模式（高可用性），最常用的集群模式。
 

RabbitMQ集群故障处理机制：
rabbitmq broker集群允许个体节点down机，
对应集群的的网络分区问题（ network partitions）
RabbitMQ集群推荐用于LAN环境，不适用WAN环境；
要通过WAN连接broker，Shovel or Federation插件是最佳的解决方案；Shovel or Federation不同于集群。
 
 
RabbitMQ节点类型
RAM node：只保存状态到内存。内存节点将所有的队列、交换机、绑定、用户、权限和vhost的元数据定义存储在内存中，好处是可以使得像交换机和队列声明等操作更加的快速。
Disk node：将元数据存储在磁盘中。单节点系统只允许磁盘类型的节点，防止重启RabbitMQ的时候，丢失系统的配置信息。
内存节点虽然不写入磁盘，但是它执行比磁盘节点要好。RabbitMQ集群中，只需要一个磁盘节点来保存状态就足够了；如果集群中只有内存节点，那么不能停止它们，否则所有的状态，消息等都会丢失。
 
问题说明：
RabbitMQ要求在集群中至少有一个磁盘节点，所有其他节点可以是内存节点，当节点加入或者离开集群时，必须要将该变更通知到至少一个磁盘节点。
如果集群中唯一的一个磁盘节点崩溃的话，集群仍然可以保持运行，但是无法进行其他操作（增删改查），直到节点恢复。
解决方案：设置两个磁盘节点，至少有一个是可用的，可以保存元数据的更改。
 
RabbitMQ集群的节点运行模式：
为保证数据持久性，当前所有node节点跑在disk模式。
如果今后压力大，需要提高性能，考虑采用ram模式。
 
RabbitMQ集群记录
本案例采用 "镜像模式"，即队列为镜像队列，队列消息存在集群的每个节点上。
 
###1、版本说明
因为考虑到较早版本rabbitmq在k8s上的集群部署是使用autocluster插件去调用kubernetes apiserver来获取rabbitmq服务的endpoints，进而获取node节点信息，并自动加入集群，但是现在autocluster已不再更新了，并且只支持3.6.x版本，故而放弃这种方式。
 
对于3.7.x或更新的版本，现在市场主流是使用 peer discovery subsystem来构建rabbitmq-cluster，参考这里。
 
###2、部署方式
在Kubernetes上搭建RabbitMQ有4种部署方法（IP模式、Pod与Server的DNS模式、Statefulset 与Headless Service模式、hostname模式），这里选择StatefulSet与Headless Service模式部署有状态的RabbitMQ集群。
 
###3、使用NFS配置StatefulSet的动态持久化存储
#####1）在NFS服务器端（172.16.60.238）通过nfs创建RabbitMQ集群的共享目录
[root@master-1 rocketmq]# mkdir -p /data/storage/k8s/rabbitmq
[root@master-1 rocketmq]# echo '/data/storage/k8s/rabbitmq *(rw,no_root_squash)' >> /etc/exports
[root@master-1 rocketmq]# exportfs -r
[root@master-1 rabbitmq]# exportfs 
/data/storage/k8s/rocketmq
                <world>
/data/storage/k8s/rabbitmq
                <world>
[root@master-1 rabbitmq]# showmount -e 172.16.201.134
Export list for 172.16.201.134:
/data/storage/k8s/rabbitmq *
/data/storage/k8s/rocketmq *

[root@node-1 ~]# mount -t nfs 172.16.201.134:/data/storage/k8s/rabbitmq /data/storage/k8s/rabbitmq
[root@node-2 ~]# mount -t nfs 172.16.201.134:/data/storage/k8s/rabbitmq /data/storage/k8s/rabbitmq

[root@node-1 ~]#  df -h|grep 134
172.16.201.134:/data/storage/k8s/rabbitmq  100G  4.2G   96G   5% /data/storage/k8s/rabbitmq

[root@node-2 ~]# df -h|grep 134
172.16.201.134:/data/storage/k8s/rabbitmq  100G  4.2G   96G   5% /data/storage/k8s/rabbitmq     



#####2）创建nfs的rbac
[root@master-1 activemq]# kubectl create namespace wiseco
namespace/wiseco created




[root@k8s-master01 rocketmq]# vim nfs-rbac.yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-provisioner
  namespace: wiseco
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
   name: nfs-provisioner-runner
   namespace: wiseco
rules:
   -  apiGroups: [""]
      resources: ["persistentvolumes"]
      verbs: ["get", "list", "watch", "create", "delete"]
   -  apiGroups: [""]
      resources: ["persistentvolumeclaims"]
      verbs: ["get", "list", "watch", "update"]
   -  apiGroups: ["storage.k8s.io"]
      resources: ["storageclasses"]
      verbs: ["get", "list", "watch"]
   -  apiGroups: [""]
      resources: ["events"]
      verbs: ["watch", "create", "update", "patch"]
   -  apiGroups: [""]
      resources: ["services", "endpoints"]
      verbs: ["get","create","list", "watch","update"]
   -  apiGroups: ["extensions"]
      resources: ["podsecuritypolicies"]
      resourceNames: ["nfs-provisioner"]
      verbs: ["use"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-provisioner
    namespace: wiseco
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io

[root@master-1 rocketmq]# kubectl apply -f nfs-rbac.yaml
serviceaccount/nfs-provisioner created
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created


[root@master-1 rocketmq]# kubectl get sa -n wiseco|grep nfs
nfs-provisioner   1         10s
[root@master-1 rocketmq]# kubectl get clusterrole -n wiseco|grep nfs
nfs-provisioner-runner                                                 2021-12-21T10:25:32Z
[root@master-1 rocketmq]# kubectl get clusterrolebinding -n wiseco|grep nfs
run-nfs-provisioner                                    ClusterRole/nfs-provisioner-runner                                                 18s
[root@master-1 rocketmq]# 


#####3）创建RabbitMQ集群的storageclass

[root@master-1 rocketmq]# vim rabbitmq-nfs-class.yaml
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: rabbitmq-nfs-storage
  namespace: wiseco
provisioner: rabbitmq/nfs
reclaimPolicy: Retain


[root@master-1 rabbitmq]# kubectl apply -f rabbitmq-nfs-class.yaml
storageclass.storage.k8s.io/rabbitmq-nfs-storage created

[root@master-1 rabbitmq]#  kubectl get sc
NAME                   PROVISIONER    RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
rabbitmq-nfs-storage   rabbitmq/nfs   Retain          Immediate           false                  14m
rocketmq-nfs-storage   rocketmq/nfs   Retain          Immediate           false                  28h
[root@master-1 rabbitmq]# 


#####4）创建MongoDB集群的nfs-client-provisioner
PROVISIONER_NAME的值一定要和StorageClass中的provisioner相等。


[root@master-1 rabbitmq]# vim rabbitmq-nfs.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq-nfs-client-provisioner
  namespace: wiseco
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq-nfs-client-provisioner
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: rabbitmq-nfs-client-provisioner
    spec:
      serviceAccount: nfs-provisioner
      containers:
        - name: rabbitmq-nfs-client-provisioner
          image: registry.cn-hangzhou.aliyuncs.com/open-ali/nfs-client-provisioner
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: nfs-client-root
              mountPath:  /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: rabbitmq/nfs
            - name: NFS_SERVER
              value: 172.16.201.134
            - name: NFS_PATH
              value: /data/storage/k8s/rabbitmq
      volumes:
        - name: nfs-client-root
          nfs:
            server: 172.16.201.134
            path: /data/storage/k8s/rabbitmq

[root@master-1 rabbitmq]# kubectl apply -f rabbitmq-nfs.yml
deployment.apps/rabbitmq-nfs-client-provisioner created

[root@master-1 rabbitmq]# kubectl get pods -n wiseco
NAME                                               READY   STATUS    RESTARTS   AGE
rabbitmq-nfs-client-provisioner-5b9bb74974-fqd66   1/1     Running   0          23s
rocketmq-nfs-client-provisioner-5bc9679c86-gsqws   1/1     Running   1          29h
[root@master-1 rabbitmq]# 


###4、部署RabbitMQ基于镜像模式的集群
[root@k8s-master01 rabbitmq]# ll
total 12
-rw-r--r-- 1 root root 1216 Feb  7 17:33 nfs-rbac.yaml
-rw-r--r-- 1 root root  161 Feb  7 17:37 rabbitmq-nfs-class.yaml
-rw-r--r-- 1 root root 1027 Feb  7 17:46 rabbitmq-nfs.yml
[root@k8s-master01 rabbitmq]# mkdir deployment
[root@k8s-master01 rabbitmq]# cd deployment
[root@k8s-master01 deployment]#


采用StatefulSet与Headless Service模式部署有状态的RabbitMQ集群。
rabbitmq.yml文件内容：

[root@master-1 deployment]# vim rabbitmq.yml
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq-management
  namespace: wiseco
  labels:
    app: rabbitmq
spec:
  ports:
  - port: 15672
    name: http
  selector:
    app: rabbitmq
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  namespace: wiseco
  labels:
    app: rabbitmq
spec:
  ports:
  - port: 5672
    name: amqp
  - port: 4369
    name: epmd
  - port: 25672
    name: rabbitmq-dist
  clusterIP: None
  selector:
    app: rabbitmq
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  namespace: wiseco
  name: rabbitmq
spec:
  serviceName: "rabbitmq"
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - rabbitmq
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: rabbitmq
        image: rabbitmq
        lifecycle:
          postStart:
            exec:
              command:
              - /bin/sh
              - -c
              - >
                if [ -z "$(grep rabbitmq /etc/resolv.conf)" ]; then
                  sed "s/^search \([^ ]\+\)/search rabbitmq.\1 \1/" /etc/resolv.conf > /etc/resolv.conf.new;
                  cat /etc/resolv.conf.new > /etc/resolv.conf;
                  rm /etc/resolv.conf.new;
                fi;
                until rabbitmqctl node_health_check; do sleep 1; done;
                if [ -z "$(rabbitmqctl cluster_status | grep rabbitmq-0)" ]; then
                  touch /gotit
                  rabbitmqctl stop_app;
                  rabbitmqctl reset;
                  rabbitmqctl join_cluster rabbit@rabbitmq-0;
                  rabbitmqctl start_app;
                else
                  touch /notget
                fi;
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: RABBITMQ_ERLANG_COOKIE
          value: "YZSDHWMFSMKEMBDHSGGZ"
        - name: RABBITMQ_NODENAME
          value: "rabbit@$(MY_POD_NAME)"
        ports:
        - name: http
          protocol: TCP
          containerPort: 15672
        - name: amqp
          protocol: TCP
          containerPort: 5672
        livenessProbe:
          tcpSocket:
            port: amqp
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: amqp
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
        volumeMounts:
        - name: rabbitmq-data
          mountPath: /var/lib/rabbitmq
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-data
      annotations:
        volume.beta.kubernetes.io/storage-class: "rabbitmq-nfs-storage"
    spec:
      accessModes:
        - ReadWriteMany
      resources:
        requests:
          storage: 10Gi


[root@master-1 deployment]# kubectl apply -f rabbitmq.yml
service/rabbitmq-management created
service/rabbitmq created
statefulset.apps/rabbitmq created

[root@master-1 deployment]# kubectl get pods -n wiseco -o wide|grep rabbitmq
rabbitmq-0                                        1/1     Running   0          11m     172.30.85.206    k8s-node01   <none>           <none>
rabbitmq-1                                        1/1     Running   0          9m9s    172.30.217.69    k8s-node04   <none>           <none>
rabbitmq-2                                        1/1     Running   0          7m59s   172.30.135.145   k8s-node03   <none>           <none>
rabbitmq-nfs-client-provisioner-c4f95d479-xvm8r   1/1     Running   0          20h     172.30.217.122   k8s-node04   <none>           <none>
 


[root@master-1 ~]# kubectl get svc -n wiseco|grep rabbitmq
rabbitmq              ClusterIP   None         <none>        5672/TCP,4369/TCP,25672/TCP   62s
rabbitmq-management   NodePort    10.1.170.1   <none>        15672:32336/TCP               63s


kubectl apply -f rabbitmq.yml
kubectl delete -f rabbitmq.yml

kubectl delete -f 
kubectl apply -f
kubectl  describe pod  -n wiseco rabbitmq-0


kubectl get event -n wiseco
kubectl logs -f rabbitmq-0 -n wiseco

kubectl delete pvc rabbitmq-data-rabbitmq-0 -n wiseco
kubectl delete pvc rabbitmq-data-rabbitmq-1 -n wiseco

####5、验证RabbitMQ集群

登录rabbitmq-0容器查看集群状态
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl cluster_status
Cluster status of node rabbit@rabbitmq-0 ...
[{nodes,[{disc,['rabbit@rabbitmq-0','rabbit@rabbitmq-1',
                'rabbit@rabbitmq-2']}]},
 {running_nodes,['rabbit@rabbitmq-2','rabbit@rabbitmq-1','rabbit@rabbitmq-0']},
 {cluster_name,<<"rabbit@rabbitmq-0.rabbitmq.wiseco.svc.cluster.local">>},
 {partitions,[]},
 {alarms,[{'rabbit@rabbitmq-2',[]},
          {'rabbit@rabbitmq-1',[]},
          {'rabbit@rabbitmq-0',[]}]}]
 
 
登录rabbitmq-1容器查看集群状态
[root@master-1 deployment]# kubectl exec -ti rabbitmq-1 -n wiseco -- rabbitmqctl cluster_status
Cluster status of node rabbit@rabbitmq-1 ...
[{nodes,[{disc,['rabbit@rabbitmq-0','rabbit@rabbitmq-1',
                'rabbit@rabbitmq-2']}]},
 {running_nodes,['rabbit@rabbitmq-2','rabbit@rabbitmq-0','rabbit@rabbitmq-1']},
 {cluster_name,<<"rabbit@rabbitmq-0.rabbitmq.wiseco.svc.cluster.local">>},
 {partitions,[]},
 {alarms,[{'rabbit@rabbitmq-2',[]},
          {'rabbit@rabbitmq-0',[]},
          {'rabbit@rabbitmq-1',[]}]}]
 
 
登录rabbitmq-2容器查看集群状态
[root@master-1 deployment]# kubectl exec -ti rabbitmq-2 -n wiseco -- rabbitmqctl cluster_status
Cluster status of node rabbit@rabbitmq-2 ...
[{nodes,[{disc,['rabbit@rabbitmq-0','rabbit@rabbitmq-1',
                'rabbit@rabbitmq-2']}]},
 {running_nodes,['rabbit@rabbitmq-0','rabbit@rabbitmq-1','rabbit@rabbitmq-2']},
 {cluster_name,<<"rabbit@rabbitmq-0.rabbitmq.wiseco.svc.cluster.local">>},
 {partitions,[]},
 {alarms,[{'rabbit@rabbitmq-0',[]},
          {'rabbit@rabbitmq-1',[]},
          {'rabbit@rabbitmq-2',[]}]}]　


###6、访问RabbitMQ的Web界面，查看集群状态
[root@master-1 deployment]# kubectl get svc -n wiseco|grep rabbitmq
rabbitmq              ClusterIP   None             <none>        5672/TCP,4369/TCP,25672/TCP   23m
rabbitmq-management   NodePort    10.254.128.136   <none>        15672:32513/TCP               23m


通过K8S的node节点的32513访问web页面，用户名和密码都是guest
http://172.16.201.134:32513


###7、RabbitMQ的日常操作命令
1）用户管理
=====================================================================================================
新增一个用户
rabbitmqctl add_user Username Password
 
删除一个用户
rabbitmqctl delete_user Username
 
修改用户的密码
rabbitmqctl change_password Username Newpassword
 
查看当前用户列表
rabbitmqctl list_users
 
比如：修改guest用户密码、新增或删除一个用户
查看当前用户列表
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
 
修改guest用户密码为 guest@123
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl change_password guest guest@123
Changing password for user "guest" ..
 
新增一个用户，用户名为kevin，密码为 kevin@123
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl add_user kevin kevin@123
Adding user "kevin" ...
 
查看当前用户列表
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
kevin   []
 
设置kevin用户角色为administrator
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl set_user_tags kevin administrator
Setting tags for user "kevin" to [administrator] ...
 
查看当前用户列表
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
kevin   [administrator]
 
修改kevin用户角色为monitoring、policymaker
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl set_user_tags kevin monitoring policymaker
Setting tags for user "kevin" to [monitoring, policymaker] ...
 
查看当前用户列表
[root@master-1 deployment]#  kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
kevin   [monitoring, policymaker]
 
删除kevin用户
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl delete_user kevin
Deleting user "kevin" ...
 
查看当前用户列表
[root@master-1 deployment]# kubectl exec -ti rabbitmq-0 -n wiseco -- rabbitmqctl list_users
Listing users ...
user    tags
guest   [administrator]
 
 
 
2）用户角色
=====================================================================================================
用户角色分类
用户角色可分为五类：超级管理员、监控者、策略制定者、普通管理者以及其他。
超级管理员 (administrator)
可登陆管理控制台(启用management plugin的情况下)，可查看所有的信息，并且可以对用户，策略(policy)进行操作。
监控者 (monitoring)
可登陆管理控制台(启用management plugin的情况下)，同时可以查看rabbitmq节点的相关信息(进程数，内存使用情况，磁盘使用情况等)
策略制定者 (policymaker)
可登陆管理控制台(启用management plugin的情况下), 同时可以对policy进行管理。但无法查看节点的相关信息
普通管理者 (management)
仅可登陆管理控制台(启用management plugin的情况下)，无法看到节点信息，也无法对策略进行管理。
其他
无法登陆管理控制台，通常就是普通的生产者和消费者。
 
相关操作命令：
设置用户角色的命令为：
rabbitmqctl set_user_tags User Tag
 
其中：
User为用户名
Tag为角色名 (对应于上面的administrator，monitoring，policymaker，management，或其他自定义名称)。
 
也可以给同一用户设置多个角色，例如：
rabbitmqctl set_user_tags kevin monitoring policymaker
 
 
 
3）用户权限
=====================================================================================================
用户权限指的是用户对exchange，queue的操作权限，包括配置权限，读写权限。
配置权限会影响到exchange，queue的声明和删除。
读写权限影响到从queue里取消息，向exchange发送消息以及queue和exchange的绑定(bind)操作。
 
例如：
 将queue绑定到某exchange上，需要具有queue的可写权限，以及exchange的可读权限；
向exchange发送消息需要具有exchange的可写权限；
从queue里取数据需要具有queue的可读权限。
 
相关操作命令：
设置用户权限
rabbitmqctl set_permissions -p VHostPath User ConfP WriteP ReadP
 
查看(指定hostpath)所有用户的权限信息
rabbitmqctl list_permissions [-p VHostPath]
 
查看指定用户的权限信息
rabbitmqctl list_user_permissions User
 
清除用户的权限信息
rabbitmqctl clear_permissions [-p VHostPath] User
 
设置节点类型
RabbitMQ节点类型分为内存节点和硬盘节点。
如果你想更换节点类型可以通过命令修改：
rabbitmqctl stop_app
rabbitmqctl change_cluster_node_type dist
rabbitmqctl change_cluster_node_type ram
rabbitmqctl start_app

###8、模拟RabbitMQ节点故障
模拟故障，重启其中的一个node节点，比如rabbitmq-0，然后观察集群状态：
[root@master-1 deployment]#kubectl get pods -n wiseco -o wide|grep rabbitmq
rabbitmq-0                                        1/1     Running   0          71m   172.30.85.206    k8s-node01   <none>           <none>
rabbitmq-1                                        1/1     Running   0          68m   172.30.217.69    k8s-node04   <none>           <none>
rabbitmq-2                                        1/1     Running   0          67m   172.30.135.145   k8s-node03   <none>           <none>
rabbitmq-nfs-client-provisioner-c4f95d479-xvm8r   1/1     Running   0          21h   172.30.217.122   k8s-node04   <none>           <none>
 
删除rabbitmq-0节点
[root@master-1 deployment]#kubectl delete pods rabbitmq-0 -n wiseco
pod "rabbitmq-0" deleted
 
查看pod，发现rabbitmq-0节点删除后，重启需要耗费一段时间
[root@master-1 deployment]#kubectl get pods -n wiseco -o wide|grep rabbitmq
rabbitmq-0                                        0/1     ContainerCreating   0          44s   <none>           k8s-node01   <none>           <none>
rabbitmq-1                                        1/1     Running             0          70m   172.30.217.69    k8s-node04   <none>           <none>
rabbitmq-2                                        1/1     Running             0          69m   172.30.135.145   k8s-node03   <none>           <none>
rabbitmq-nfs-client-provisioner-c4f95d479-xvm8r   1/1     Running             0          21h   172.30.217.122   k8s-node04   <none>           <none>
 
此时，查看RabbitMQ集群状态
发现此时，rabbit@rabbitmq-0节点还没有恢复，running的node节点只有rabbit@rabbitmq-2、rabbit@rabbitmq-1
[root@master-1 deployment]# kubectl exec -ti rabbitmq-1 -n wiseco -- rabbitmqctl cluster_status
Cluster status of node rabbit@rabbitmq-1 ...
[{nodes,[{disc,['rabbit@rabbitmq-0','rabbit@rabbitmq-1',
                'rabbit@rabbitmq-2']}]},
 {running_nodes,['rabbit@rabbitmq-2','rabbit@rabbitmq-1']},
 {cluster_name,<<"rabbit@rabbitmq-0.rabbitmq.wiseco.svc.cluster.local">>},
 {partitions,[]},
 {alarms,[{'rabbit@rabbitmq-2',[]},{'rabbit@rabbitmq-1',[]}]}]
　　
此时，查看web界面的集群状态，先后经历了下面三个状态：
红色表示 节点故障。
黄色表示 节点恢复中，暂不可用。
绿色表示 点运行正常。


####9、客户端访问RabbitMQ集群地址
客户端连接RabbitMQ集群地址：
rabbitmq-0.rabbitmq.wiseco.svc.cluster.local:5672
rabbitmq-0.rabbitmq.wiseco.svc.cluster.local:5672
rabbitmq-0.rabbitmq.wiseco.svc.cluster.local:5672

连接方式：
客户端可以连接RabbitMQ集群中的任意一个节点。如果一个节点故障，客户端自行重新连接到其他的可用节点；
也就是说，RabbitMQ集群有"重连"机制，但是这种集群连接方式对客户端不透明，不太建议这种连接方式。
推荐方式：给客户端提供一个统一的透明的集群连接地址
做法：在前面部署LVS或Haproxy，通过四层负载均衡代理后RabbitMQ的三个node节点的5672端口。


##(二十三)、k8s部署Nacos
参考：
https://www.cnblogs.com/kevingrace/p/14412064.html

一、什么是Nacos
英文全称Dynamic Naming and Configuration Service，Na为naming/nameServer即注册中心,co为configuration即注册中心，service是指该注册/配置中心都是以服务为核心。服务在nacos是一等公民

二、Nacos原理
Nacos注册中心分为server与client，server采用Java编写，为client提供注册发现服务与配置服务。而client可以用多语言实现，client与微服务嵌套在一起，nacos提供sdk和openApi，如果没有sdk也可以根据openApi手动写服务注册与发现和配置拉取的逻辑

Nacos服务领域模型主要分为命名空间、集群、服务。在下图的分级存储模型可以看到，在服务级别，保存了健康检查开关、元数据、路由机制、保护阈值等设置，而集群保存了健康检查模式、元数据、同步机制等数据，实例保存了该实例的ip、端口、权重、健康检查状态、下线状态、元数据、响应时间。

服务注册方法：以Java nacos client v1.0.1 为例子，服务注册的策略的是每5秒向nacos server发送一次心跳，心跳带上了服务名，服务ip，服务端口等信息。同时 nacos server也会向client 主动发起健康检查，支持tcp/http检查。如果15秒内无心跳且健康检查失败则认为实例不健康，如果30秒内健康检查失败则剔除实例。


三、Nacos 的关键特性包括:

服务发现和服务健康监测
Nacos 支持基于 DNS 和基于 RPC 的服务发现。服务提供者使用 原生SDK、OpenAPI、或一个独立的Agent TODO注册 Service 后，服务消费者可以使用DNS TODO 或HTTP&API查找和发现服务。
Nacos 提供对服务的实时的健康检查，阻止向不健康的主机或服务实例发送请求。Nacos 支持传输层 (PING 或 TCP)和应用层 (如 HTTP、MySQL、用户自定义）的健康检查。 对于复杂的云环境和网络拓扑环境中（如 VPC、边缘网络等）服务的健康检查，Nacos 提供了 agent 上报模式和服务端主动检测2种健康检查模式。Nacos 还提供了统一的健康检查仪表盘，帮助您根据健康状态管理服务的可用性及流量。

动态配置服务
动态配置服务可以让您以中心化、外部化和动态化的方式管理所有环境的应用配置和服务配置。
动态配置消除了配置变更时重新部署应用和服务的需要，让配置管理变得更加高效和敏捷。
配置中心化管理让实现无状态服务变得更简单，让服务按需弹性扩展变得更容易。
Nacos 提供了一个简洁易用的UI (控制台样例 Demo) 帮助您管理所有的服务和应用的配置。Nacos 还提供包括配置版本跟踪、金丝雀发布、一键回滚配置以及客户端配置更新状态跟踪在内的一系列开箱即用的配置管理特性，帮助您更安全地在生产环境中管理配置变更和降低配置变更带来的风险。

动态 DNS 服务
动态 DNS 服务支持权重路由，让您更容易地实现中间层负载均衡、更灵活的路由策略、流量控制以及数据中心内网的简单DNS解析服务。动态DNS服务还能让您更容易地实现以 DNS 协议为基础的服务发现，以帮助您消除耦合到厂商私有服务发现 API 上的风险。
Nacos 提供了一些简单的 DNS APIs TODO 帮助您管理服务的关联域名和可用的 IP:PORT 列表.

服务及其元数据管理
Nacos能让您从微服务平台建设的视角管理数据中心的所有服务及元数据，包括管理服务的描述、生命周期、服务的静态依赖分析、服务的健康状态、服务的流量管理、路由及安全策略、服务的 SLA 以及最首要的 metrics 统计数据。


Nacos支持三种部署模式
单机模式 - 用于测试和单机试用。
集群模式 - 用于生产环境，确保高可用。
多集群模式 - 用于多数据中心场景。

官网：
https://nacos.io/zh-cn/docs/use-nacos-with-kubernetes.html


[root@master-1 nacos-k8s]# mkdir /data/nfs-share
[root@master-1 nacos-k8s]# echo '/data/nfs-share *(rw,no_root_squash)' >> /etc/exports
[root@master-1 nacos-k8s]# echo '/data/mysql *(rw,no_root_squash)' >> /etc/exports                       
[root@master-1 nacos-k8s]# exportfs -r;exportfs;showmount -e 172.16.201.134
/data/storage/k8s/rocketmq
                <world>
/data/storage/k8s/rabbitmq
                <world>
/data/nfs-share
                <world>
Export list for 172.16.201.134:
/data/nfs-share            *
/data/storage/k8s/rabbitmq *
/data/storage/k8s/rocketmq *
[root@master-1 nacos-k8s]# 


[root@node-1 ~]# mount -t nfs 172.16.201.134:/data/nfs-share /data/nfs-share
[root@node-1 ~]# mount -t nfs 172.16.201.134:/data/mysql /data/mysql

[root@node-1 ~]# df -h|grep 134
172.16.201.134:/data/storage/k8s/rabbitmq  100G  8.2G   92G   9% /data/storage/k8s/rabbitmq
172.16.201.134:/data/nfs-share             100G  8.2G   92G   9% /data/nfs-share
172.16.201.134:/data/mysql                 100G  8.2G   92G   9% /data/mysql




[root@master-1 nacos-k8s]# kubectl create -f deploy/nfs/rbac.yaml
clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created


[root@master-1 nacos-k8s]# kubectl get ClusterRole|grep nfs-client-provisioner-runner
nfs-client-provisioner-runner                                          2021-12-22T23:56:07Z

[root@master-1 nacos-k8s]# kubectl create -f deploy/nfs/class.yaml
storageclass.storage.k8s.io/managed-nfs-storage created

[root@master-1 nacos-k8s]# kubectl get sc
NAME                   PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage    fuseim.pri/ifs   Delete          Immediate           false                  19s
rabbitmq-nfs-storage   rabbitmq/nfs     Retain          Immediate           false                  143m
rocketmq-nfs-storage   rocketmq/nfs     Retain          Immediate           false                  31h

注意修改ip、数据目录
[root@master-1 nacos-k8s]# kubectl create -f deploy/nfs/deployment.yaml
serviceaccount/nfs-client-provisioner created
deployment.apps/nfs-client-provisioner created

[root@master-1 nacos-k8s]# kubectl get pod -l app=nfs-client-provisioner --watch
NAME                                      READY   STATUS    RESTARTS   AGE
nfs-client-provisioner-7585ccfffd-b7knw   1/1     Running   0          22s

注意修改ip、数据目录
[root@master-1 nacos-k8s]# kubectl create -f deploy/mysql/mysql-nfs.yaml
replicationcontroller/mysql created
service/mysql created

root@master-1 nacos-k8s]# kubectl get pod  -o wide --watch
NAME                                      READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
activemq-deployment-68b8596b6b-m4x2c      1/1     Running   0          17h     10.244.1.31   node-1   <none>           <none>
mysql-9xvbk                               1/1     Running   0          62s     10.244.1.35   node-1   <none>           <none>
nfs-client-provisioner-7585ccfffd-b7knw   1/1     Running   0          9m15s   10.244.2.26   node-2   <none>           <none>
nginx-6799fc88d8-62ps5                    1/1     Running   0          17h     10.244.1.32   node-1   <none>           <none>



[root@master-1 nacos-k8s]# kubectl create -f deploy/nacos/nacos-pvc-nfs.yaml
service/nacos-headless created
configmap/nacos-cm created
statefulset.apps/nacos created

[root@master-1 nacos-k8s]# kubectl get pod -l app=nacos -o wide  --watch
NAME      READY   STATUS    RESTARTS   AGE     IP            NODE     NOMINATED NODE   READINESS GATES
nacos-0   1/1     Running   0          8m12s   10.244.2.27   node-2   <none>           <none>
nacos-1   1/1     Running   0          4m37s   10.244.1.36   node-1   <none>           <none>
nacos-2   0/1     Running   0          68s     <none>        <none>   <none>           <none>

[root@master-1 ~]# kubectl get pod -l app=nacos -o wide  
NAME      READY   STATUS    RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
nacos-0   1/1     Running   0          25m   10.244.2.27   node-2     <none>           <none>
nacos-1   1/1     Running   0          22m   10.244.1.36   node-1     <none>           <none>
nacos-2   1/1     Running   0          18m   10.244.0.2    master-1   <none>           <none>
[root@master-1 ~]# 


[root@master-1 nacos-k8s]# for i in 0 1 2; do echo nacos-$i; kubectl exec nacos-$i cat conf/cluster.conf; done
nacos-0
nacos-0.nacos-headless.default.svc.cluster.local:8848
nacos-1.nacos-headless.default.svc.cluster.local:8848
nacos-2.nacos-headless.default.svc.cluster.local:8848

nacos-1
nacos-0.nacos-headless.default.svc.cluster.local:8848
nacos-1.nacos-headless.default.svc.cluster.local:8848
nacos-2.nacos-headless.default.svc.cluster.local:8848

nacos-2
nacos-0.nacos-headless.default.svc.cluster.local:8848
nacos-1.nacos-headless.default.svc.cluster.local:8848
nacos-2.nacos-headless.default.svc.cluster.local:8848


[root@master-1 ~]# kubectl exec nacos-1 curl GET "http://localhost:8848/nacos/v1/ns/raft/state" 
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: GET; Unknown error
{"services":0,"peers":[{"ip":"nacos-0.nacos-headless.default.svc.cluster.local:8848","term":0,"leaderDueMs":10108,"heartbeatDueMs":2518,"state":"FOLLOWER"},{"ip":"nacos-1.nacos-headless.default.svc.cluster.local:8848","term":-1,"leaderDueMs":4100   422    0   422    0     0  16127      0 --:--:-- --:--:-- --:--:-- 16127fault.svc.cluster.local:8848","term":0,"leaderDueMs":3057,"heartbeatDueMs":3198,"state":"FOLLOWER"}]}
[root@master-1 ~]# 



[root@master-1 ~]# kubectl scale sts nacos --replicas=2
statefulset.apps/nacos scaled

[root@master-1 ~]# kubectl get pod -l app=nacos -o wide  --watch
NAME      READY   STATUS        RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
nacos-0   1/1     Terminating   0          30m   10.244.2.27   node-2     <none>           <none>
nacos-1   1/1     Running       0          26m   10.244.1.36   node-1     <none>           <none>
nacos-2   1/1     Running       0          23m   10.244.0.2    master-1   <none>           <none>





[root@master-1 ~]# kubectl get pod -l app=nacos -o wide  --watch
NAME      READY     STATUS        RESTARTS   AGE   IP            NODE       NOMINATED NODE   READINESS GATES
nacos-0   1/1       Running       0          30m   10.244.2.27   node-2     <none>           <none>
nacos-1   1/1       Running       0          26m   10.244.1.36   node-1     <none>           <none>
nacos-2   1/1       Running       0          23m   10.244.0.2    master-1   <none>           <none>
rabbitmq-0  1/1     Running       0          71m   172.30.85.206    k8s-node01   <none>           <none>
rabbitmq-1  1/1     Running       0          68m   172.30.217.69    k8s-node04   <none>           <none>
rabbitmq-2  1/1     Running       0          67m   172.30.135.145   k8s-node03   <none>           <none>
mysql-9xvbk 1/1     Running       0          41m



单个服务测试地址：
Service registration
curl -X PUT 'http://cluster-ip:8848/nacos/v1/ns/instance?serviceName=nacos.naming.serviceName&ip=20.18.7.10&port=8080'
Service discovery
curl -X GET 'http://cluster-ip:8848/nacos/v1/ns/instances?serviceName=nacos.naming.serviceName'
Publish config
curl -X POST "http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test&content=helloWorld"
Get config
curl -X GET "http://cluster-ip:8848/nacos/v1/cs/configs?dataId=nacos.cfg.dataId&group=test"







##(二十四)、K8S Pod容器应用"优雅发布"
K8S自身带有优雅终止Pod容器的机制，发送SIGTERM终止信号，在规定的terminationGracePeriodSeconds优雅时间内完成Pod优雅终止动作。
terminationGracePeriodSeconds默认是30秒，该时间是从Pod的Termination状态开始计算的，包括了Prestop钩子处理时间、SIGTERM信号发送即程序优雅处理时间。


Pod容器终止流程：
1）新Pod启动，通过Readiness就绪性探测，加入service的endpoint服务列表。
2）老pod进入Termination状态，从service的endpoint服务列表摘除，此时不会有新请求打到即将终止的老pod上。
3）如果设置了Prestop钩子，则优先执行Prestop里的优雅动作。如果在规定的terminationGracePeriodSeconds优雅时间内（默认30s）完成不了，则kubelet会发送SIGTERM终止信号，并等待2秒，如果2秒后还未终止pod容器，则发送SIGKILL信号强制终止。
4）如果没有设置Prestop钩子，则发送SIGTERM终止信号优雅关闭容器进程，如果在规定的terminationGracePeriodSeconds优雅时间内（默认30s）未能终止pod容器，则发送SIGKILL信号强制终止。

 

需要注意：
1）SIGTERM终止信号只能被那些pid为1的父进程捕捉到，并优雅关闭容器进程。对于那些pid不为1的子进程是捕捉不到SIGTERM终止信号的。
所以对于单个容器只有一个pid为1的进程来说，使用K8S默认的优雅机制就可以，只需要拉长terminationGracePeriodSeconds优雅时间，确保在规定时间内完成容器优雅终止。

2）对于那些单个容器里有多个进程，即除了pid为1的进程外，还有子进程。这种情况下就需要设置Prestop钩子函数，在prestop里提前优雅处理掉那些子进程，然后再通过SIGTERM正常终止掉pod容器。
注意设置好terminationGracePeriodSeconds优雅时间。

 

线上基于nacos注册中心的优雅上线

对于请求通过k8s的service层到达pod容器的情况，可以通过k8s优雅机制来确保pod容器在上线滚动更新期间，做到业务"无感知"。但是目前线上pod容器服务主动注册到nacos配置中心，业务方通过nacos网关调用pod容器服务，即调用请求绕过了k8s的service层。

这就出现了一个问题：pod容器更新期间，老pod已经优雅终止掉了，但是其ip和端口还在nacos的网关缓存里，调用请求会在nacos网关缓存未完全更新之前打到已经终止掉的pod地址，这就会出现连接超时，调用失败错误，从而造成业务流量损失。

 

正确的做法：

1）拉长terminationGracePeriodSeconds的优雅时间。
2）设置Prestop钩子，在Pod容器终止之前，在Prestop里通过nacos提供的API接口，主动摘除nacos注册。接着sleep 30秒时间，用于刷新nacos网关缓存，摘除下线的pod地址。
3）最后再执行pod容器的优雅终止。

 
容器优雅发布的配置记录：
这里以customer-services应用模块的pod容器优雅配置为例:

1）将nacos主动下线的脚本在镜像制作阶段推送到容器内部
编写customer-services主动下线nacos的脚本：
[root@k8s-storage01 ~]# ls /home/k8s_deploy/fin/online/deploy/customer-services/
Dockerfile  service_offline_nacos.sh customer-services.jar
 
[root@k8s-storage01 ~]# cat /home/k8s_deploy/fin/online/deploy/customer-services/service_offline_nacos.sh
####!/bin/bash
  
serviceName="customer-services"
groupName="kevin-app"
metadata="preserved.register.source=SPRING_CLOUD"
namespaceId="online"
port="9810"
Token=$(curl -s --location --request POST 'http://nacos:8848/nacos/v1/auth/users/login' --form 'username=nacos' --form 'password=nacos'|awk -F"accessToken" '{print $2}'|awk -F":" '{print $2}'|awk -F'"' '{print $2}')
  
#####从nacos注册中心下线
curl --location --request PUT "http://nacos:8848/nacos/v1/ns/instance?&accessToken=${Token}" \
--form "serviceName=${serviceName}" \
--form "clusterName=DEFAULT" \
--form "groupName=${groupName}" \
--form "metadata=${metadata}" \
--form "namespaceId=${namespaceId}" \
--form "ip=${podip}" \
--form "port=${port}" \
--form "ephemeral=true" \
--form "weight=1" \
--form "enabled=false"
  
#####等待30s，刷新nacos网关缓存
sleep 30
　　

制作finhub-customer-services服务的容器镜像
[root@k8s-storage01 ~]# cat /home/k8s_deploy/fin/online/deploy/customer-services/Dockerfile
FROM 172.16.60.196/kevin/jdk1.8.0_192
RUN rm -f /etc/localtime \
&& ln -sv /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \
&& echo "Asia/Shanghai" > /etc/timezone
  
ENV LANG en_US.UTF-8
 
COPY customer-services.jar /usr/local/src
COPY service_offline_nacos.sh /opt/
WORKDIR /usr/local/src
EXPOSE 9810
CMD ["nohup","java","-jar","customer-services.jar","&"]
　　

制作和推送镜像
[root@k8s-storage01 ~]# docker build -t 172.16.60.196/kevin/customer-services_v1 .
[root@k8s-storage01 ~]# docker push 172.16.60.196/kevin/customer-services_v1
　　

2）配置pod的yml部署文件，添加env变量动态获取pod ip地址、拉长terminationGracePeriodSeconds优雅时间、设置prestop钩子
[root@k8s-master01 customer-services]# pwd
/opt/k8s/k8s-project/kevin/customer-services
  
[root@k8s-master01 customer-services]# cat customer-services.yml
apiVersion: v1
kind: Service
metadata:
  name: customer-services
  namespace: kevin
  labels:
    app: customer-services
spec:
  type: NodePort
  selector:
    app: customer-services
  ports:
  - name: http
    port: 9810
    targetPort: 9810
    nodePort: 39810
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: customer-services
  namespace: kevin
spec:
  replicas: 2
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  selector:
    matchLabels:
      app: customer-services
  template:
    metadata:
      labels:
        app: customer-services
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                      - customer-services
              topologyKey: "kubernetes.io/hostname"
      terminationGracePeriodSeconds: 60
      containers:
      - name: customer-services
        image: 172.16.60.196/kevin/customer-services_v1
        imagePullPolicy: Always
        ports:
        - name: customer-port
          containerPort: 9810
        env:
        - name: podip
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        resources:
          requests:
            cpu: 950m
            memory: 2048Mi
          limits:
            cpu: 1500m
            memory: 4096Mi
        lifecycle:
          postStart:
            exec:
              command: ["/bin/sh","-c","touch /tmp/health"]
          preStop:
            exec:
              command: ["/bin/sh","-c","sh /opt/service_offline_nacos.sh"]
        livenessProbe:
          exec:
            command: ["test","-e","/tmp/health"]
          initialDelaySeconds: 5
          timeoutSeconds: 5
          periodSeconds: 10
        readinessProbe:
          tcpSocket:
            port: customer-port
          initialDelaySeconds: 15
          timeoutSeconds: 5
          periodSeconds: 20
        volumeMounts:
        - name: customerlog
          mountPath: /var/log/customer-services
          readOnly: false
      volumes:
      - name: customerlog
        hostPath:
          path: /var/log/k8s_log/customer-services


生产环境通过Jenkins配置的Pod容器部署流程：即从Gitlab拉取代码、打包、制作镜像、上传镜像到Harbor仓库、更新pod等发布流程。完成如上配置，后续通过Jenkins进行"容器应用的优雅"发版了。



##()、K8S部署ES集群
https://www.cnblogs.com/kevingrace/p/14444075.html




##()、k8s部署MysqL Mycat

##()、k8s部署Istio 高可用部署方案
##()、k8s部署Calico
https://www.cnblogs.com/kevingrace/p/6864804.html
##()、k8s部署Consul
##()、k8s部署Weave
 https://www.cnblogs.com/kevingrace/p/6859173.html
 
 
 


k8s helm  kafka
https://www.kubernetes.org.cn/7592.html


rm -fr OPSer;git clone git@github.com:lex921/OPSer.git
git add -A;git commit -s -m "MQ相关";git push






























































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































