##(十)、k8s部署 Prometheus

###1、Kubernetes Operator 介绍
在 Kubernetes 的支持下，管理和伸缩 Web 应用、移动应用后端以及 API 服务都变得比较简单了。其原因是这些应用一般都是无状态的，所以 Deployment 这样的基础 Kubernetes API 对象就可以在无需附加操作的情况下，对应用进行伸缩和故障恢复了。

而对于数据库、缓存或者监控系统等有状态应用的管理，就是个挑战了。这些系统需要应用领域的知识，来正确的进行伸缩和升级，当数据丢失或不可用的时候，要进行有效的重新配置。我们希望这些应用相关的运维技能可以编码到软件之中，从而借助 Kubernetes 的能力，正确的运行和管理复杂应用。

Operator 这种软件，使用 TPR(第三方资源，现在已经升级为 CRD) 机制对 Kubernetes API 进行扩展，将特定应用的知识融入其中，让用户可以创建、配置和管理应用。和 Kubernetes 的内置资源一样，Operator 操作的不是一个单实例应用，而是集群范围内的多实例。

Kubernetes 的 Prometheus Operator 为 Kubernetes 服务和 Prometheus 实例的部署和管理提供了简单的监控定义。

安装完毕后，Prometheus Operator提供了以下功能：

创建/毁坏： 在 Kubernetes namespace 中更容易启动一个 Prometheus 实例，一个特定的应用程序或团队更容易使用Operator。
简单配置: 配置 Prometheus 的基础东西，比如在 Kubernetes 的本地资源 versions, persistence, retention policies, 和 replicas。
Target Services 通过标签： 基于常见的Kubernetes label查询，自动生成监控target 配置；不需要学习普罗米修斯特定的配置语言。

######Prometheus Operator 系统架构：
Operator： Operator 资源会根据自定义资源（Custom Resource Definition / CRDs）来部署和管理 Prometheus Server，同时监控这些自定义资源事件的变化来做相应的处理，是整个系统的控制中心。
Prometheus： Prometheus 资源是声明性地描述 Prometheus 部署的期望状态。
Prometheus Server： Operator 根据自定义资源 Prometheus 类型中定义的内容而部署的 Prometheus Server 集群，这些自定义资源可以看作是用来管理 Prometheus Server 集群的 StatefulSets 资源。
ServiceMonitor： ServiceMonitor 也是一个自定义资源，它描述了一组被 Prometheus 监控的 targets 列表。该资源通过 Labels 来选取对应的 Service Endpoint，让 Prometheus Server 通过选取的 Service 来获取 Metrics 信息。
Service： Service 资源主要用来对应 Kubernetes 集群中的 Metrics Server Pod，来提供给 ServiceMonitor 选取让 Prometheus Server 来获取信息。简单的说就是 Prometheus 监控的对象，例如 Node Exporter Service、Mysql Exporter Service 等等。
Alertmanager： Alertmanager 也是一个自定义资源类型，由 Operator 根据资源描述内容来部署 Alertmanager 集群。

######Prometheus 三大套件
Server 主要负责数据采集和存储，提供PromQL查询语言的支持。
Alertmanager 警告管理器，用来进行报警。
Push Gateway 支持临时性Job主动推送指标的中间网关。

######Prometheus服务过程
Prometheus Daemon 负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。
Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。
Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。
PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。
Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。


###2、版本说明
线上kubernetes集群为1.16版本 Prometheus oprator 分支为0.4关于Prometheus oprator与kubernetes版本对应关系如下图。可见https://github.com/prometheus-operator/kube-prometheus.
注： Prometheus operator？ kube-prometheus? kube-prometheus 就是 Prometheus的一种operator的部署方式…Prometheus-operator 已经改名为 Kube-promethues。

官网：https://github.com/prometheus-operator/kube-prometheus


ubernetes compatibility matrix
The following versions are supported and work as we test against these versions in their respective branches. But note that other versions might work!

kube-prometheus stack	Kubernetes1.18	Kubernetes1.19	Kubernetes1.20	Kubernetes1.21
release-0.5	          ✔	               ✗	             ✗	             ✗
release-0.6	          ✗	               ✔	             ✗	             ✗
release-0.7	          ✗	               ✔	             ✔	             ✗
release-0.8	          ✗	               ✗	             ✔	             ✔
HEAD	                ✗	               ✗	             ✔	             ✔




###3、Kubernetes1.19下载v0.7.0版本：

[root@master-1 prometheus]# wget https://github.com/prometheus-operator/kube-prometheus/archive/refs/tags/v0.7.0.tar.gz
[root@master-1 prometheus]# tar -xvf v0.7.0.tar.gz 
[root@master-1 prometheus]# ll
total 292
drwxrwxr-x 11 root root   4096 Dec 10  2020 kube-prometheus-0.7.0
-rw-r--r--  1 root root 294457 Nov  4 15:45 v0.7.0.tar.gz
[root@master-1 prometheus]# 

由于它的文件都存放在项目源码的 manifests 文件夹下，所以需要进入其中进行启动这些 kubernetes 应用 yaml 文件。又由于这些文件堆放在一起，不利于分类启动，所以这里将它们分类。

进入源码的 manifests 文件夹

先简单部署一下Prometheus oprator（or或者叫kube-promethus）。完成微信报警的集成，其他的慢慢在生成环境中研究。
基本过程就是Prometheus oprator 添加存储，增加微信报警，外部traefik代理应用。

[root@master-1 manifests]# cd /root/prometheus/kube-prometheus-0.7.0/manifests

部署Prometheus+Grafana+Alertmanager

###### 修改镜像源
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' setup/prometheus-operator-deployment.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-prometheus.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' alertmanager-alertmanager.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' kube-state-metrics-deployment.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' node-exporter-daemonset.yaml
[root@master-1 manifests]# sed -i 's/quay.io/quay.mirrors.ustc.edu.cn/g' prometheus-adapter-deployment.yaml
[root@master-1 manifests]# grep -r quay.io ./

###4、安装CRD和prometheus-operator
[root@master-1 manifests]# kubectl apply -f setup/
namespace/monitoring created
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
service/prometheus-operator created
serviceaccount/prometheus-operator created
[root@master-1 manifests]# 

###5、查看进度：
[root@master-1 manifests]# kubectl get pod -n monitoring --watch
NAME                                   READY   STATUS              RESTARTS   AGE
prometheus-operator-66bd8d7bd7-n4hl6   0/2     ContainerCreating   0          3s
prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running             0          44s

装完了：
[root@master-1 ~]# kubectl get pod -n monitoring 
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running   0          2m19s


看看都建立了什么：
[root@master-1 ~]# kubectl get all -n monitoring    
NAME                                       READY   STATUS    RESTARTS   AGE
pod/prometheus-operator-66bd8d7bd7-n4hl6   2/2     Running   0          2m26s

NAME                          TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
service/prometheus-operator   ClusterIP   None         <none>        8443/TCP   2m27s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/prometheus-operator   1/1     1            1           2m27s

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/prometheus-operator-66bd8d7bd7   1         1         1       2m27s


也可以在部署前各节点提前下载镜像
为了保证服务启动速度，所以最好部署节点提前下载所需镜像。
docker pull quay.io/coreos/configmap-reload:v0.0.1
docker pull quay.io/coreos/prometheus-config-reloader:v0.29.0
docker pull quay.io/coreos/prometheus-operator:v0.44.1
docker pull quay.io/coreos/k8s-prometheus-adapter-amd64:v0.4.1
docker pull quay.io/prometheus/alertmanager:v0.17.0
docker pull quay.io/prometheus/node-exporter:v0.17.0 
docker pull quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0
docker pull quay.io/coreos/kube-state-metrics:v1.5.0
docker pull registry.aliyuncs.com/google_containers/addon-resizer:1.8.4
docker pull quay.io/prometheus/prometheus:v2.7.2


"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/alertmanager:v0.21.0"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"grafana/grafana:7.3.4"
"quay.mirrors.ustc.edu.cn/coreos/kube-state-metrics:v1.9.7"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0"
"quay.mirrors.ustc.edu.cn/prometheus/node-exporter:v1.0.1"
"directxman12/k8s-prometheus-adapter:v0.8.2"
"quay.mirrors.ustc.edu.cn/prometheus/prometheus:v2.22.1"
"quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-config-reloader:v0.44.1"
"quay.mirrors.ustc.edu.cn/prometheus/prometheus:v2.22.1"


使用国外服务器下载镜像，并打包为tar包下载到本地。
docker pull k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0-rc.0
docker save k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0-rc.0 -o kube-state-metrics.tar





[root@master-1 manifests]# kubectl get event -n monitoring
LAST SEEN   TYPE      REASON              OBJECT                                      MESSAGE
25s         Normal    Scheduled           pod/prometheus-operator-66bd8d7bd7-4qx5g    Successfully assigned monitoring/prometheus-operator-66bd8d7bd7-4qx5g to node-2
23s         Normal    Pulled              pod/prometheus-operator-66bd8d7bd7-4qx5g    Container image "quay.mirrors.ustc.edu.cn/prometheus-operator/prometheus-operator:v0.44.1" already present on machine
22s         Normal    Created             pod/prometheus-operator-66bd8d7bd7-4qx5g    Created container prometheus-operator
22s         Normal    Started             pod/prometheus-operator-66bd8d7bd7-4qx5g    Started container prometheus-operator
22s         Normal    Pulled              pod/prometheus-operator-66bd8d7bd7-4qx5g    Container image "quay.mirrors.ustc.edu.cn/brancz/kube-rbac-proxy:v0.8.0" already present on machine
22s         Normal    Created             pod/prometheus-operator-66bd8d7bd7-4qx5g    Created container kube-rbac-proxy
21s         Normal    Started             pod/prometheus-operator-66bd8d7bd7-4qx5g    Started container kube-rbac-proxy
26s         Warning   FailedCreate        replicaset/prometheus-operator-66bd8d7bd7   Error creating: pods "prometheus-operator-66bd8d7bd7-" is forbidden: error looking up service account monitoring/prometheus-operator: serviceaccount "prometheus-operator" not found
25s         Normal    SuccessfulCreate    replicaset/prometheus-operator-66bd8d7bd7   Created pod: prometheus-operator-66bd8d7bd7-4qx5g
27s         Normal    ScalingReplicaSet   deployment/prometheus-operator              Scaled up replica set prometheus-operator-66bd8d7bd7 to 1


###6、安装prometheus, alertmanager, grafana, kube-state-metrics, node-exporter等资源,时间较长
#####增加 prometheus访问端口：
type: NodePort
nodePort: 32101


[root@master-1 manifests]# cat prometheus-service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    prometheus: k8s
  name: prometheus-k8s
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: web
    port: 9090
    targetPort: web
		nodePort: 32101
  selector:
    app: prometheus
    prometheus: k8s
  sessionAffinity: ClientIP



#####增加grafana访问端口：
type: NodePort
nodePort: 32102

[root@master-1 manifests]# cat grafana-service.yaml 
apiVersion: v1
kind: Service
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - name: http
    port: 3000
    targetPort: http
    nodePort: 32102
  selector:
    app: grafana

#####执行安装：
[root@master-1 manifests]# kubectl apply -f .
alertmanager.monitoring.coreos.com/main created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager created
secret/grafana-datasources created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-statefulset created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
servicemonitor.monitoring.coreos.com/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-operator created
prometheus.monitoring.coreos.com/k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-rules created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
[root@master-1 manifests]# 

#####查看：
[root@master-1 manifests]# kubectl get pod -n monitoring --watch
NAME                                   READY   STATUS              RESTARTS   AGE
alertmanager-main-0                    0/2     ContainerCreating   0          30s
alertmanager-main-1                    0/2     ContainerCreating   0          30s
alertmanager-main-2                    0/2     ContainerCreating   0          30s
grafana-f8cd57fcf-pz56g                0/1     ContainerCreating   0          23s
kube-state-metrics-58c88f48b7-5bqrj    0/3     ContainerCreating   0          21s
node-exporter-5hzp8                    0/2     ContainerCreating   0          16s
node-exporter-k2cmg                    0/2     ContainerCreating   0          18s
node-exporter-qtph2                    0/2     ContainerCreating   0          16s
prometheus-adapter-69b8496df6-5z9x5    0/1     ContainerCreating   0          8s
prometheus-k8s-0                       0/2     ContainerCreating   0          6s
prometheus-k8s-1                       0/2     ContainerCreating   0          6s
prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running             0          2m20s


#####纠结了很长时间：
[root@master-1 manifests]# kubectl get pod -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
alertmanager-main-0                    2/2     Running   0          7m5s
alertmanager-main-1                    2/2     Running   0          7m5s
alertmanager-main-2                    2/2     Running   0          7m5s
grafana-f8cd57fcf-pz56g                1/1     Running   0          6m58s
kube-state-metrics-58c88f48b7-5bqrj    3/3     Running   0          6m56s
node-exporter-5hzp8                    2/2     Running   0          6m51s
node-exporter-k2cmg                    2/2     Running   0          6m53s
node-exporter-qtph2                    2/2     Running   0          6m51s
prometheus-adapter-69b8496df6-5z9x5    1/1     Running   0          6m43s
prometheus-k8s-0                       2/2     Running   1          6m41s
prometheus-k8s-1                       2/2     Running   2          6m41s
prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running   0          8m55s



#####查看所有信息：
[root@master-1 manifests]# kubectl get all -n monitoring 
NAME                                       READY   STATUS    RESTARTS   AGE
pod/alertmanager-main-0                    2/2     Running   0          11m
pod/alertmanager-main-1                    2/2     Running   0          11m
pod/alertmanager-main-2                    2/2     Running   0          11m
pod/grafana-f8cd57fcf-pz56g                1/1     Running   0          10m
pod/kube-state-metrics-58c88f48b7-5bqrj    3/3     Running   0          10m
pod/node-exporter-5hzp8                    2/2     Running   0          10m
pod/node-exporter-k2cmg                    2/2     Running   0          10m
pod/node-exporter-qtph2                    2/2     Running   0          10m
pod/prometheus-adapter-69b8496df6-5z9x5    1/1     Running   0          10m
pod/prometheus-k8s-0                       2/2     Running   1          10m
pod/prometheus-k8s-1                       2/2     Running   2          10m
pod/prometheus-operator-66bd8d7bd7-hdq2p   2/2     Running   0          12m

NAME                            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.1.24.38     <none>        9093/TCP                     11m
service/alertmanager-operated   ClusterIP   None           <none>        9093/TCP,9094/TCP,9094/UDP   11m
service/grafana                 ClusterIP   10.1.109.224   <none>        3000/TCP                     11m
service/kube-state-metrics      ClusterIP   None           <none>        8443/TCP,9443/TCP            10m
service/node-exporter           ClusterIP   None           <none>        9100/TCP                     10m
service/prometheus-adapter      ClusterIP   10.1.101.198   <none>        443/TCP                      10m
service/prometheus-k8s          ClusterIP   10.1.102.83    <none>        9090/TCP                     10m
service/prometheus-operated     ClusterIP   None           <none>        9090/TCP                     10m
service/prometheus-operator     ClusterIP   None           <none>        8443/TCP                     12m

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   3         3         3       3            3           kubernetes.io/os=linux   10m

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana               1/1     1            1           11m
deployment.apps/kube-state-metrics    1/1     1            1           10m
deployment.apps/prometheus-adapter    1/1     1            1           10m
deployment.apps/prometheus-operator   1/1     1            1           12m

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-f8cd57fcf                1         1         1       11m
replicaset.apps/kube-state-metrics-58c88f48b7    1         1         1       10m
replicaset.apps/prometheus-adapter-69b8496df6    1         1         1       10m
replicaset.apps/prometheus-operator-66bd8d7bd7   1         1         1       12m

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   3/3     11m
statefulset.apps/prometheus-k8s      2/2     10m




#####查看端口
[root@master-1 manifests]# kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.1.24.38     <none>        9093/TCP                     13m
alertmanager-operated   ClusterIP   None           <none>        9093/TCP,9094/TCP,9094/UDP   13m
grafana                 ClusterIP   10.1.109.224   <none>        3000/TCP                     12m
kube-state-metrics      ClusterIP   None           <none>        8443/TCP,9443/TCP            12m
node-exporter           ClusterIP   None           <none>        9100/TCP                     12m
prometheus-adapter      ClusterIP   10.1.101.198   <none>        443/TCP                      12m
prometheus-k8s          ClusterIP   10.1.102.83    <none>        9090/TCP                     12m
prometheus-operated     ClusterIP   None           <none>        9090/TCP                     12m
prometheus-operator     ClusterIP   None           <none>        8443/TCP                     14m


#####访问地址：
prometheus
http://172.16.201.134:32101/graph


grafana
http://172.16.201.134:32102/
admin/admin




###7、安装metrics-server（下载指定版本，不能下最新的）

####Kubernetes Metrics Server：

Kubernetes Metrics Server 是 Cluster 的核心监控数据的聚合器，kubeadm 默认是不部署的。

Metrics Server 供 Dashboard 等其他组件使用，是一个扩展的 APIServer，依赖于 API Aggregator。所以，在安装 Metrics Server 之前需要先在 kube-apiserver 中开启 API Aggregator。
Metrics API 只可以查询当前的度量数据，并不保存历史数据。
Metrics API URI 为 /apis/metrics.k8s.io/，在 k8s.io/metrics 下维护。
必须部署 metrics-server 才能使用该 API，metrics-server 通过调用 kubelet Summary API 获取数据。

####前提条件

注意：使用 Metrics Server 有必备两个条件：

1、API Server 启用 Aggregator Routing 支持。否则 API Server 不识别请求：

Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)
2、API Server 能访问 Metrics Server Pod IP。否则 API Server 无法访问 Metrics Server：

E1223 07:23:04.330206       1 available_controller.go:420] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.171.248.214:4443/apis/metrics.k8s.io/v1beta1: Get https://10.171.248.214:4443/apis/metrics.k8s.io/v1beta1: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)

3、启用API Aggregator，API Aggregation 允许在不修改 Kubernetes 核心代码的同时扩展 Kubernetes API，即：将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问第三方服务了，例如：Metrics Server API。注：另外一种扩展 Kubernetes API 的方法是使用 CRD（Custom Resource Definition，自定义资源定义）。


###安装：
####1、修改每个 API Server 的 kube-apiserver.yaml 配置开启 Aggregator Routing：
####修改 manifests 配置后 API Server 会自动重启生效。
[root@master-1 prometheus]# vim /etc/kubernetes/manifests/kube-apiserver.yaml
    - --secure-port=6443
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-cluster-ip-range=10.1.0.0/16
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --enable-aggregator-routing=true # 添加本行

[root@master-1 base]# systemctl restart kubelet
[root@master-1 base]# kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml
######不行执行2次，最好重启kubelet

####2、下载：
[root@master-1 prometheus]# wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml

####3、修改：
[root@master-1 prometheus]# vim components.yaml
        - --kubelet-preferred-address-types=InternalIP   # 删掉 ExternalIP,Hostname这两个，这里已经改好了，你那边要自己核对一下
        - --kubelet-use-node-status-port
        - --kubelet-insecure-tls                    #   加上该启动参数
        image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1                 # 镜像地址根据情况修改

####4、执行安装：
[root@master-1 base]# kubectl delete -f components.yaml
[root@master-1 base]# kubectl apply -f components.yaml

[root@master-1 prometheus]# kubectl apply -f components.yaml
serviceaccount/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
service/metrics-server created
deployment.apps/metrics-server created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created

####5、查看pod：
[root@master-1 prometheus]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-bfcc967d6-b7s8z     0/1     ContainerCreating   0          7s
[root@master-1 prometheus]# 
[root@master-1 prometheus]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-bfcc967d6-b7s8z     1/1     Running   0          75s

####6、查看具体信息：
[root@master-1 prometheus]# kubectl describe svc metrics-server -n kube-system
Name:              metrics-server
Namespace:         kube-system
Labels:            k8s-app=metrics-server
Annotations:       <none>
Selector:          k8s-app=metrics-server
Type:              ClusterIP
IP:                10.1.21.56
Port:              https  443/TCP
TargetPort:        https/TCP
Endpoints:         10.244.1.24:4443
Session Affinity:  None
Events:            <none>

####7、测试网络：
[root@master-1 prometheus]# ping 10.244.1.24
PING 10.244.1.24 (10.244.1.24) 56(84) bytes of data.
64 bytes from 10.244.1.24: icmp_seq=1 ttl=63 time=0.693 ms
64 bytes from 10.244.1.24: icmp_seq=2 ttl=63 time=0.693 ms
64 bytes from 10.244.1.24: icmp_seq=3 ttl=63 time=0.868 ms

####8、测试命令：
[root@master-1 prometheus]#  kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master-1   294m         14%    1337Mi          36%       
node-1     1317m        65%    1432Mi          83%       
node-2     192m         9%     1189Mi          69%       
[root@master-1 prometheus]# 


[root@master-1 prometheus]#  kubectl top pods
NAME                           CPU(cores)   MEMORY(bytes)   
test-centos-6ccff47f57-vhtvc   0m           5Mi  

[root@master-1 prometheus]# kubectl top pod --all-namespaces
NAMESPACE     NAME                                   CPU(cores)   MEMORY(bytes)   
default       test-centos-6ccff47f57-vhtvc           0m           5Mi             
kube-system   coredns-6d56c8448f-7c4dp               4m           16Mi            
kube-system   coredns-6d56c8448f-pm6r6               4m           16Mi            
kube-system   etcd-master-1                          34m          96Mi            
kube-system   kube-apiserver-master-1                97m          455Mi           
kube-system   kube-controller-manager-master-1       20m          58Mi            
kube-system   kube-flannel-ds-cvmdw                  10m          18Mi            
kube-system   kube-flannel-ds-gcckr                  5m           32Mi            
kube-system   kube-flannel-ds-nkl2p                  3m           31Mi            
kube-system   kube-proxy-fkj8w                       1m           30Mi            
kube-system   kube-proxy-glbfj                       1m           17Mi            
kube-system   kube-proxy-pxfg7                       10m          16Mi            
kube-system   kube-scheduler-master-1                5m           25Mi            
kube-system   metrics-server-bfcc967d6-b7s8z         31m          15Mi            
monitoring    alertmanager-main-0                    27m          22Mi            
monitoring    alertmanager-main-1                    5m           28Mi            
monitoring    alertmanager-main-2                    19m          22Mi            
monitoring    grafana-f8cd57fcf-pz56g                10m          52Mi            
monitoring    kube-state-metrics-58c88f48b7-5bqrj    3m           43Mi            
monitoring    node-exporter-5hzp8                    6m           38Mi            
monitoring    node-exporter-k2cmg                    21m          17Mi            
monitoring    node-exporter-qtph2                    6m           21Mi            
monitoring    prometheus-adapter-69b8496df6-6lcbj    9m           23Mi            
monitoring    prometheus-k8s-0                       126m         308Mi           
monitoring    prometheus-k8s-1                       18m          340Mi           
monitoring    prometheus-operator-66bd8d7bd7-hdq2p   3m           48Mi            
[root@master-1 prometheus]# 



[root@master-1 base]# kubectl top nodes
NAME       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
master-1   303m         15%    1719Mi          46%       
node-1     164m         8%     1651Mi          96%       
node-2     209m         10%    1310Mi          76%    


[root@master-1 base]# kubectl top pods -n monitoring
NAME                                   CPU(cores)   MEMORY(bytes)   
alertmanager-main-0                    7m           21Mi            
alertmanager-main-1                    4m           23Mi            
alertmanager-main-2                    5m           19Mi            
grafana-f8cd57fcf-pz56g                10m          43Mi            
kube-state-metrics-58c88f48b7-5bqrj    1m           40Mi            
node-exporter-5hzp8                    8m           14Mi            
node-exporter-k2cmg                    6m           25Mi            
node-exporter-qtph2                    8m           18Mi            
prometheus-adapter-69b8496df6-6lcbj    7m           23Mi            
prometheus-k8s-0                       41m          273Mi           
prometheus-k8s-1                       17m          277Mi           
prometheus-operator-66bd8d7bd7-hdq2p   10m          42Mi      


[root@master-1 prometheus]# kubectl  get endpoints -n kube-system 
NAME                      ENDPOINTS                                                                    AGE
kube-controller-manager   <none>                                                                       6d23h
kube-dns                  10.244.0.13:53,10.244.0.14:53,10.244.0.13:53 + 3 more...                     6d23h
kube-scheduler            <none>                                                                       6d23h
kubelet                   172.16.201.134:10250,172.16.201.135:10250,172.16.201.136:10250 + 6 more...   22h
metrics-server            10.244.1.24:4443                                                             7m51s


[root@master-1 prometheus]# kubectl get apiservices |egrep metrics
v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        8m15s




####9、故障涉及命令如下：

[root@master-1 base]# kubectl get apiservices |egrep metrics
v1beta1.metrics.k8s.io                 kube-system/metrics-server   False (MissingEndpoints)   2m45s


[root@master-1 base]# kubectl get pod -n kube-system | grep metrics-server
metrics-server-95b5cb586-4nsm8     1/1     Running   0          7m21s

[root@master-1 base]# kubectl get endpoints
NAME         ENDPOINTS             AGE
kubernetes   172.16.201.134:6443   6d6h

[root@master-1 base]# kubectl  get endpoints -n kube-system 
NAME                      ENDPOINTS                                                                    AGE
kube-controller-manager   <none>                                                                       6d6h
kube-dns                  10.244.0.11:53,10.244.0.12:53,10.244.0.11:53 + 3 more...                     6d6h
kube-scheduler            <none>                                                                       6d6h
kubelet                   172.16.201.134:10250,172.16.201.135:10250,172.16.201.136:10250 + 6 more...   5h20m
metrics-server            <none>                                                                       7m55s
[root@master-1 base]# 

metrics-server ENDPOINTS没地址



[root@master-1 base]#  kubectl  get svc -n kube-system             
NAME             TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                        AGE
kube-dns         ClusterIP   10.1.0.10     <none>        53/UDP,53/TCP,9153/TCP         6d6h
kubelet          ClusterIP   None          <none>        10250/TCP,10255/TCP,4194/TCP   5h21m
metrics-server   ClusterIP   10.1.89.103   <none>        443/TCP                        8m38s
	
	
[root@master-1 base]#  kubectl api-versions|grep metrics
metrics.k8s.io/v1beta1
[root@master-1 base]# 


kubectl logs metrics-server-c778b76f8-9kzp4 -n kube-system
kubectl logs metrics-server-c778b76f8-9kzp4 -n kube-system



[root@master-1 etcd-v3.5.1-linux-amd64]# etcdctl version
etcdctl version: 3.5.1
API version: 3.5



ETCDCTL_API=3 /usr/local/bin/etcdctl --endpoints=https://127.0.0.1:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
--key=/etc/kubernetes/pki/etcd/healthcheck-client.key

alias etcdctl="ETCDCTL_API=3 /bin/etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key"

etcdctl get / --prefix --keys-only



[root@m1master kubernetes]# etcdctl get /registry/services --prefix --keys-only
/registry/services/endpoints/default/kubernetes
/registry/services/endpoints/istio-system/istio-ingressgateway
/registry/services/endpoints/istio-system/istiod-1-6-10
/registry/services/endpoints/istio-system/jaeger-collector
/registry/services/endpoints/istio-system/jaeger-collector-headless
/registry/services/endpoints/istio-system/jaeger-operator-metrics



[root@master-1 base]# kubectl describe APIService  
。。。。。。。。。。。
Name:         v1beta1.metrics.k8s.io
Namespace:    
Labels:       <none>
Annotations:  <none>
API Version:  apiregistration.k8s.io/v1
Kind:         APIService
Metadata:
  Creation Timestamp:  2021-11-04T14:12:27Z
  Resource Version:    373642
  Self Link:           /apis/apiregistration.k8s.io/v1/apiservices/v1beta1.metrics.k8s.io
  UID:                 73f03067-8e6c-4097-8d17-b9689572e9c8
Spec:
  Group:                     metrics.k8s.io
  Group Priority Minimum:    100
  Insecure Skip TLS Verify:  true
  Service:
    Name:            metrics-server
    Namespace:       kube-system
    Port:            443
  Version:           v1beta1
  Version Priority:  100
Status:
  Conditions:
    Last Transition Time:  2021-11-04T14:12:27Z
    Message:               endpoints for service/metrics-server in "kube-system" have no addresses with port name "https"
    Reason:                MissingEndpoints
    Status:                False
    Type:                  Available
Events:                    <none>
。。。。。。。。。。。



[root@master-1 base]# kubectl get apiservice v1beta1.metrics.k8s.io
NAME                     SERVICE                      AVAILABLE                  AGE
v1beta1.metrics.k8s.io   kube-system/metrics-server   False (MissingEndpoints)   1


[root@master-1 ~]# kubectl exec -it metrics-server-5646c885bc-2568r -- bash    



也可以直接通过 kubectl 命令来访问这些 API，比如：
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes
kubectl get --raw /apis/metrics.k8s.io/v1beta1/pods
kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/<node-name>
kubectl get --raw /apis/metrics.k8s.io/v1beta1/namespace/<namespace-name>/pods/<pod-name>



[root@master-1 base]# kubectl -n kube-system edit deploy metrics-server
 

kubectl  get endpoints -n kube-system

####10、引入服务：
kind: Endpoints
apiVersion: v1
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: metrics-server
subsets:
  - addresses:
      - ip: 10.1.168.106
    ports:
      - port: 443
        protocol: TCP
        name: https

####11、不出图问题处理
访问：
http://172.16.201.134:32101/service-discovery

注意时间 如果提示如下，就是服务器时间跟浏览器时间不一致，意思就是服务器与浏览器时间不同步
然后我进行了核实，确实相差这么多时间。时间调整完后，不用重启Prometheus ，Prometheus 的web端报警消失。
Warning! Detected 254.70 seconds time difference between your browser and the server. Prometheus relies on accurate time and time drift might cause unexpected query results.


左侧加号-->Import via grafana.com输入：11074-->点load-->点Import

添加 Dashboard -> New Dashboard -> Import Dashboard -> 输入11074，导入Linux监控模板. 并配置数据源为Prometheus，即上一步中的name
配置完保存后即可看到逼格非常高的系统主机节点监控信息，包括系统运行时间, 内存和CPU的配置, CPU、内存、磁盘、网络流量等信息, 以及磁盘IO、CPU温度等信息。


####12、参考资料:
https://www.cnblogs.com/shunzi115/p/13950131.html
https://www.cnblogs.com/walkersss/p/12553189.html
官网地址：https://prometheus.io/
GitHub: https://github.com/prometheus
官方文档中文版: https://github.com/Alrights/prometheus
官方监控agent列表:https://prometheus.io/docs/instrumenting/exporters/
