##(十三)、k8s部署 Tidb集群
#####软件版本要求
软件名称	版本
Docker	Docker CE 18.09.6
Kubernetes	v1.12.5+
CentOS	CentOS 7.6，内核要求为 3.10.0-957 或之后版本
Helm	v3.0.0+

#####建议关闭防火墙：
systemctl stop firewalld
systemctl disable firewalld

如果无法关闭 firewalld 服务，为了保证 Kubernetes 正常运行，需要打开以下端口：
在 Master 节点上，打开以下端口，然后重新启动服务：
firewall-cmd --permanent --add-port=6443/tcp
firewall-cmd --permanent --add-port=2379-2380/tcp
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10251/tcp
firewall-cmd --permanent --add-port=10252/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --add-masquerade --permanent

当需要在 Master 节点上暴露 NodePort 时候设置
firewall-cmd --permanent --add-port=30000-32767/tcp
systemctl restart firewalld


#####在 Node 节点上，打开以下端口，然后重新启动服务：
firewall-cmd --permanent --add-port=10250/tcp
firewall-cmd --permanent --add-port=10255/tcp
firewall-cmd --permanent --add-port=8472/udp
firewall-cmd --permanent --add-port=30000-32767/tcp
firewall-cmd --add-masquerade --permanent
systemctl restart firewalld

#####配置 Iptables
FORWARD 链默认配置成 ACCEPT，并将其设置到开机启动脚本里：
iptables -P FORWARD ACCEPT

#####禁用 SELinux
setenforce 0
sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

#####关闭 Swap
Kubelet 正常工作需要关闭 Swap，并且把 /etc/fstab 里面有关 Swap 的那行注释掉：
swapoff -a
sed -i 's/^\(.*swap.*\)$/#\1/' /etc/fstab 


#####内核参数设置
按照下面的配置设置内核参数，也可根据自身环境进行微调：
modprobe br_netfilter

cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-arptables = 1
net.core.somaxconn = 32768
vm.swappiness = 0
net.ipv4.tcp_syncookies = 0
net.ipv4.ip_forward = 1
fs.file-max = 1000000
fs.inotify.max_user_watches = 1048576
fs.inotify.max_user_instances = 1024
net.ipv4.conf.all.rp_filter = 1
net.ipv4.neigh.default.gc_thresh1 = 80000
net.ipv4.neigh.default.gc_thresh2 = 90000
net.ipv4.neigh.default.gc_thresh3 = 100000
EOF

sysctl --system

#####配置 Irqbalance 服务
Irqbalance 服务可以将各个设备对应的中断号分别绑定到不同的 CPU 上，以防止所有中断请求都落在同一个 CPU 上而引发性能瓶颈。
systemctl enable irqbalance
systemctl start irqbalance

#####CPUfreq 调节器模式设置
为了让 CPU 发挥最大性能，请将 CPUfreq 调节器模式设置为 performance 模式。详细参考在部署目标机器上配置 CPUfreq 调节器模式。
cpupower frequency-set --governor performance

#####Ulimit 设置
TiDB 集群默认会使用很多文件描述符，需要将工作节点上面的 ulimit 设置为大于等于 1048576：
cat <<EOF >>  /etc/security/limits.conf
root        soft        nofile        1048576
root        hard        nofile        1048576
root        soft        stack         10240
EOF
sysctl --system

#####Docker 服务
安装 Docker 时，建议选择 Docker CE 18.09.6 及以上版本。请参考 Docker 安装指南 进行安装。
安装完 Docker 服务以后，执行以下步骤：
#####1、将 Docker 的数据保存到一块单独的盘上，Docker 的数据主要包括镜像和容器日志数据。通过设置 --data-root 参数来实现：
cat > /etc/docker/daemon.json <<EOF
{
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2",
  "storage-opts": [
    "overlay2.override_kernel_check=true"
  ],
  "data-root": "/data1/docker"
}
EOF
上面会将 Docker 的数据目录设置为 /data1/docker




#####2、设置 Docker daemon 的 ulimit。

创建 docker service 的 systemd drop-in 目录 /etc/systemd/system/docker.service.d：

mkdir -p /etc/systemd/system/docker.service.d
创建 /etc/systemd/system/docker.service.d/limit-nofile.conf 文件，并配置 LimitNOFILE 参数的值，取值范围为大于等于 1048576 的数字即可。

cat > /etc/systemd/system/docker.service.d/limit-nofile.conf <<EOF
[Service]
LimitNOFILE=1048576
EOF
注意：

请勿将 LimitNOFILE 的值设置为 infinity。由于 systemd 的 bug，infinity 在 systemd 某些版本中指的是 65536。

重新加载配置：
systemctl daemon-reload && systemctl restart docker



#####3、Kubernetes 服务
参考 Kubernetes 官方文档，部署一套多 Master 节点高可用集群。

Kubernetes Master 节点的配置取决于 Kubernetes 集群中 Node 节点个数，节点数越多，需要的资源也就越多。节点数可根据需要做微调。

Kubernetes 集群 Node 节点个数	Kubernetes Master 节点配置
1-5	1vCPUs 4GB Memory
6-10	2vCPUs 8GB Memory
11-100	4vCPUs 16GB Memory
101-250	8vCPUs 32GB Memory
251-500	16vCPUs 64GB Memory
501-5000	32vCPUs 128GB Memory

安装完 Kubelet 之后，执行以下步骤：
将 Kubelet 的数据保存到一块单独盘上（可跟 Docker 共用一块盘），Kubelet 主要占盘的数据是 emptyDir 所使用的数据。通过设置 --root-dir 参数来实现：

echo "KUBELET_EXTRA_ARGS=--root-dir=/data1/kubelet" > /etc/sysconfig/kubelet
systemctl restart kubelet
上面会将 Kubelet 数据目录设置为 /data1/kubelet。

通过 kubelet 设置预留资源，保证机器上的系统进程以及 Kubernetes 的核心进程在工作负载很高的情况下仍然有足够的资源来运行，从而保证整个系统的稳定。

#####TiDB 集群资源需求
请根据服务器建议配置来规划机器的配置。
另外，在生产环境中，尽量不要在 Kubernetes Master 节点部署 TiDB 实例，或者尽可能少地部署 TiDB 实例。因为网卡带宽的限制，Master 节点网卡满负荷工作会影响到 Worker 节点和 Master 节点之间的心跳信息汇报，导致比较严重的问题。





####部署 TiDB Operator4.0

#####一、创建 TiDB Operator CRD
######1、下载 TiDB Cluster CRD 部署文件
wget https://raw.githubusercontent.com/pingcap/tidb-operator/v1.1.7/manifests/crd.yaml

######2、创建 TiDB Cluster CRD
[root@master-1 tidb]# kubectl apply -f crd.yaml
Warning: apiextensions.k8s.io/v1beta1 CustomResourceDefinition is deprecated in v1.16+, unavailable in v1.22+; use apiextensions.k8s.io/v1 CustomResourceDefinition
customresourcedefinition.apiextensions.k8s.io/tidbclusters.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/backups.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/restores.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/backupschedules.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbmonitors.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbinitializers.pingcap.com created
customresourcedefinition.apiextensions.k8s.io/tidbclusterautoscalers.pingcap.com created
[root@master-1 tidb]# 

######3、检查 CRD 状态
[root@master-1 tidb]# kubectl get crd
NAME                                 CREATED AT
backups.pingcap.com                  2021-11-10T04:59:24Z
backupschedules.pingcap.com          2021-11-10T04:59:27Z
restores.pingcap.com                 2021-11-10T04:59:26Z
tidbclusterautoscalers.pingcap.com   2021-11-10T04:59:28Z
tidbclusters.pingcap.com             2021-11-10T04:59:22Z
tidbinitializers.pingcap.com         2021-11-10T04:59:28Z
tidbmonitors.pingcap.com             2021-11-10T04:59:27Z
[root@master-1 tidb]# 



#####二、部署 TiDB Operator
######1、下载 TiDB Operator 的 docker image
[root@master-1 ~]# docker pull pingcap/tidb-operator:v1.1.7
[root@master-1 ~]# docker pull pingcap/tidb-backup-manager:v1.1.7
[root@master-1 ~]# docker pull pingcap/advanced-statefulset:v0.3.3

mkdir -p /opt/soft/docker-image
docker save -o tidb-backup-manager.tar pingcap/tidb-backup-manager
docker save -o tidb-operator.tar pingcap/tidb-operator
docker save -o advanced-statefulset.tar pingcap/advanced-statefulset


[root@master-1 ~]# docker images|grep pingcap
pingcap/tidb-backup-manager                                               v1.1.7              8d9ce547af25        12 months ago       805MB
pingcap/tidb-operator                                                     v1.1.7              021b4825bf26        12 months ago       168MB
pingcap/advanced-statefulset                                              v0.3.3              ec5f3bf9faf8        18 months ago       88.7MB
[root@master-1 ~]# 

 



################################################
######其他教程参考：
1、导入需要的镜像（所有节点）
(1).联网环境镜像 pull 地址
docker pull pingcap/pd:v4.0.8
docker pull pingcap/tikv:v4.0.8
docker pull pingcap/tidb:v4.0.8
docker pull pingcap/tidb-binlog:v4.0.8
docker pull pingcap/ticdc:v4.0.8
docker pull pingcap/tiflash:v4.0.8
docker pull pingcap/tidb-monitor-reloader:v1.0.1
docker pull pingcap/tidb-monitor-initializer:v4.0.8
docker pull grafana/grafana:6.0.1
docker pull prom/prometheus:v2.18.1
docker pull busybox:1.26.2
docker pull quay.io/external_storage/local-volume-provisioner:v2.3.4
docker pull pingcap/tidb-operator:v1.1.7
docker pull pingcap/tidb-backup-manager:v1.1.7
docker pull bitnami/kubectl:latest
docker pull pingcap/advanced-statefulset:v0.3.3

(2).导出镜像
docker save -o local-volume-provisioner-v2.3.4.tar quay.io/external_storage/local-volume-provisioner:v2.3.4
docker save -o tidb-operator-v1.1.7.tar pingcap/tidb-operator:v1.1.7
docker save -o tidb-backup-manager-v1.1.7.tar pingcap/tidb-backup-manager:v1.1.7
docker save -o bitnami-kubectl.tar bitnami/kubectl:latest
docker save -o advanced-statefulset-v0.3.3.tar pingcap/advanced-statefulset:v0.3.3
docker save -o pd-v4.0.8.tar pingcap/pd:v4.0.8
docker save -o tikv-v4.0.8.tar pingcap/tikv:v4.0.8
docker save -o tidb-v4.0.8.tar pingcap/tidb:v4.0.8
docker save -o tidb-binlog-v4.0.8.tar pingcap/tidb-binlog:v4.0.8
docker save -o ticdc-v4.0.8.tar pingcap/ticdc:v4.0.8
docker save -o tiflash-v4.0.8.tar pingcap/tiflash:v4.0.8
docker save -o tidb-monitor-reloader-v1.0.1.tar pingcap/tidb-monitor-reloader:v1.0.1
docker save -o tidb-monitor-initializer-v4.0.8.tar pingcap/tidb-monitor-initializer:v4.0.8
docker save -o grafana-6.0.1.tar grafana/grafana:6.0.1
docker save -o prometheus-v2.18.1.tar prom/prometheus:v2.18.1
docker save -o busybox-1.26.2.tar busybox:1.26.2

(3).导入镜像
docker load -i advanced-statefulset-v0.3.3.tar
docker load -i bitnami-kubectl.tar
docker load -i busybox-1.26.2.tar
docker load -i grafana-6.0.1.tar
docker load -i kube-scheduler-v1.15.9.tar
docker load -i kube-scheduler-v1.16.9.tar
docker load -i local-volume-provisioner-v2.3.4.tar
docker load -i mysqlclient-latest.tar
docker load -i pd-v4.0.8.tar
docker load -i prometheus-v2.18.1.tar
docker load -i ticdc-v4.0.8.tar
docker load -i tidb-backup-manager-v1.1.7.tar
docker load -i tidb-binlog-v4.0.8.tar
docker load -i tidb-monitor-initializer-v4.0.8.tar
docker load -i tidb-monitor-reloader-v1.0.1.tar
docker load -i tidb-operator-v1.1.7.tar
docker load -i tidb-v4.0.8.tar
docker load -i tiflash-v4.0.8.tar
docker load -i tikv-v4.0.8.tar
docker load -i tiller-v2.16.7.tar


################################################

######2、创建 tidb-operator 部署文件

[root@master-1 tidb]# vim tidb-operator-deploy.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: tidb-scheduler-policy
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
data:
  policy.cfg: |-
    {
      "kind" : "Policy",
      "apiVersion" : "v1",
      "predicates": [
        {"name": "NoVolumeZoneConflict"},
        {"name": "MaxEBSVolumeCount"},
        {"name": "MaxAzureDiskVolumeCount"},
        {"name": "NoDiskConflict"},
        {"name": "GeneralPredicates"},
        {"name": "PodToleratesNodeTaints"},
        {"name": "CheckVolumeBinding"},
        {"name": "MaxGCEPDVolumeCount"},
        {"name": "MatchInterPodAffinity"},
        {"name": "CheckVolumeBinding"}
      ],
      "priorities": [
        {"name": "SelectorSpreadPriority", "weight": 1},
        {"name": "InterPodAffinityPriority", "weight": 1},
        {"name": "LeastRequestedPriority", "weight": 1},
        {"name": "BalancedResourceAllocation", "weight": 1},
        {"name": "NodePreferAvoidPodsPriority", "weight": 1},
        {"name": "NodeAffinityPriority", "weight": 1},
        {"name": "TaintTolerationPriority", "weight": 1}
      ],
      "extenders": [
        {
          "urlPrefix": "http://127.0.0.1:10262/scheduler",
          "filterVerb": "filter",
          "preemptVerb": "preempt",
          "weight": 1,
          "httpTimeout": 30000000000,
          "enableHttps": false
        }
      ]
    }


---


kind: ServiceAccount
apiVersion: v1
metadata:
  name: tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
rules:
- apiGroups: [""]
  resources:
  - services
  - events
  verbs: ["*"]
- apiGroups: [""]
  resources: ["endpoints","configmaps"]
  verbs: ["create", "get", "list", "watch", "update","delete"]
- apiGroups: [""]
  resources: ["serviceaccounts"]
  verbs: ["create","get","update","delete"]
- apiGroups: ["batch"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "delete"]
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["create", "update", "get", "list", "watch","delete"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "watch", "create", "update", "delete", "patch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch","update", "delete"]
- apiGroups: ["apps"]
  resources: ["statefulsets","deployments", "controllerrevisions"]
  verbs: ["*"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["*"]
- apiGroups: ["apps.pingcap.com"]
  resources: ["statefulsets", "statefulsets/status"]
  verbs: ["*"]
- apiGroups: ["pingcap.com"]
  resources: ["*"]
  verbs: ["*"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "list", "watch", "patch","update"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "list", "watch"]

- apiGroups: ["rbac.authorization.k8s.io"]
  resources: [clusterroles,roles]
  verbs: ["escalate","create","get","update", "delete"]
- apiGroups: ["rbac.authorization.k8s.io"]
  resources: ["rolebindings","clusterrolebindings"]
  verbs: ["create","get","update", "delete"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-controller-manager
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: tidb-operator:tidb-controller-manager
  apiGroup: rbac.authorization.k8s.io

---


kind: ServiceAccount
apiVersion: v1
metadata:
  name: tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
rules:

- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list"]
- apiGroups: ["pingcap.com"]
  resources: ["tidbclusters"]
  verbs: ["get"]
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["get", "list", "update"]

- apiGroups: [""]
  resources: ["endpoints"]
  verbs: ["delete", "get", "patch", "update"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  verbs: ["create"]
- apiGroups: ["coordination.k8s.io"]
  resources: ["leases"]
  resourceNames: ["tidb-scheduler"]
  verbs: ["get", "update"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: tidb-operator:tidb-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:kube-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: system:kube-scheduler
  apiGroup: rbac.authorization.k8s.io
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tidb-operator:volume-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
subjects:
- kind: ServiceAccount
  name: tidb-scheduler
  namespace: tidb-admin
roleRef:
  kind: ClusterRole
  name: system:volume-scheduler
  apiGroup: rbac.authorization.k8s.io

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tidb-controller-manager
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: controller-manager
    helm.sh/chart: tidb-operator-v1.1.7
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-operator
      app.kubernetes.io/instance: tidb-operator
      app.kubernetes.io/component: controller-manager
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-operator
        app.kubernetes.io/instance: tidb-operator
        app.kubernetes.io/component: controller-manager
    spec:
      serviceAccount: tidb-controller-manager
      containers:
      - name: tidb-operator
        image: pingcap/tidb-operator:v1.1.7
        imagePullPolicy: IfNotPresent
        resources:
            requests:
              cpu: 80m
              memory: 50Mi

        command:
          - /usr/local/bin/tidb-controller-manager
          - -tidb-backup-manager-image=pingcap/tidb-backup-manager:v1.1.7
          - -tidb-discovery-image=pingcap/tidb-operator:v1.1.7
          - -cluster-scoped=true
          - -auto-failover=true
          - -pd-failover-period=5m
          - -tikv-failover-period=5m
          - -tiflash-failover-period=5m
          - -tidb-failover-period=5m
          - -v=2
        env:
          - name: NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: TZ
            value: UTC


---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: tidb-scheduler
  labels:
    app.kubernetes.io/name: tidb-operator
    app.kubernetes.io/managed-by: Tiller
    app.kubernetes.io/instance: tidb-operator
    app.kubernetes.io/component: scheduler
    helm.sh/chart: tidb-operator-v1.1.7
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: tidb-operator
      app.kubernetes.io/instance: tidb-operator
      app.kubernetes.io/component: scheduler
  template:
    metadata:
      labels:
        app.kubernetes.io/name: tidb-operator
        app.kubernetes.io/instance: tidb-operator
        app.kubernetes.io/component: scheduler
    spec:
      serviceAccount: tidb-scheduler
      containers:
      - name: tidb-scheduler
        image: pingcap/tidb-operator:v1.1.7
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi

        command:
          - /usr/local/bin/tidb-scheduler
          - -v=2
          - -port=10262
      - name: kube-scheduler
        image: k8s.gcr.io/kube-scheduler:v1.14.0
        imagePullPolicy: IfNotPresent
        resources:
            limits:
              cpu: 250m
              memory: 150Mi
            requests:
              cpu: 80m
              memory: 50Mi

        command:
        - kube-scheduler
        - --port=10261
        - --leader-elect=true
        - --lock-object-name=tidb-scheduler
        - --lock-object-namespace=tidb-admin
        - --scheduler-name=tidb-scheduler
        - --v=2
        - --policy-configmap=tidb-scheduler-policy
        - --policy-configmap-namespace=tidb-admin



######3、创建 tidb-operator
create tidb-admin namespace
[root@master-1 tidb]# kubectl create namespace tidb-admin
namespace/tidb-admin created

create tidb-operator 
[root@master-1 tidb]# kubectl apply -f tidb-operator-deploy.yaml -n tidb-admin
configmap/tidb-scheduler-policy created
serviceaccount/tidb-controller-manager created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRole is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRole
clusterrole.rbac.authorization.k8s.io/tidb-operator:tidb-controller-manager created
Warning: rbac.authorization.k8s.io/v1beta1 ClusterRoleBinding is deprecated in v1.17+, unavailable in v1.22+; use rbac.authorization.k8s.io/v1 ClusterRoleBinding
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:tidb-controller-manager created
serviceaccount/tidb-scheduler created
clusterrole.rbac.authorization.k8s.io/tidb-operator:tidb-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:tidb-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:kube-scheduler created
clusterrolebinding.rbac.authorization.k8s.io/tidb-operator:volume-scheduler created
deployment.apps/tidb-controller-manager created
deployment.apps/tidb-scheduler created
[root@master-1 tidb]# 


######4、检查 TiDB-Operator 状态
[root@master-1 tidb]# kubectl get pods -n tidb-admin --watch
NAME                                       READY   STATUS              RESTARTS   AGE
tidb-controller-manager-76796956f8-4flwj   1/1     Running             0          9s
tidb-scheduler-6d68c54f5f-nz6t2            0/2     ContainerCreating   0          9s

[root@master-1 tidb]# kubectl get pods -n tidb-admin --watch
NAME                                       READY   STATUS    RESTARTS   AGE
tidb-controller-manager-76796956f8-4flwj   1/1     Running   0          27s
tidb-scheduler-6d68c54f5f-nz6t2            2/2     Running   0          27s



调试命令
kubectl delete -f 
kubectl create -f tidb-operator-deploy.yaml
kubectl create -f tidb-operator-deploy.yaml

kubectl get pods -o wide
kubectl get svc
kubectl get event
kubectl describe pod mysql-master-6h7wh

kubectl exec -it   -- bash

############################################################常用语句：

[root@master-1 tidb]# for i in `seq 9`; do mkdir -p /mnt/disks/pv0$i; done
[root@master-1 tidb]# ll /mnt/disks/
total 0
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv01
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv02
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv03
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv04
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv05
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv06
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv07
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv08
drwxr-xr-x 2 root root 6 Nov 10 13:49 pv09


用以下命令为 TiDB Cluster 创建 9 个 PV。创建 多少个 PV 个你的需求有关，但至少创建 9 个吧，防止不够用。
delete apply
[root@master-1 tidb]#for i in `seq 9`; do
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: PersistentVolume
metadata:
  name: tidb-cluster-master-pv0${i}
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/pv0${i}
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - 172.16.201.134
EOF
done


delete apply
[root@master-1 ~]# for i in `seq 9`; do
cat <<EOF | kubectl delete -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tidb-pvc-pv0${i}
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage
EOF
done
############################################################常用语句：
#####三、为 TiDB Cluster 创建 PV
######1、准备创建持久化存储 local-volume-provisioner.yaml

[root@master-1 tidb]# vim local-volume-provisioner.yaml 
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: "local-storage"
provisioner: "kubernetes.io/no-provisioner"
volumeBindingMode: "WaitForFirstConsumer"
 
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: local-provisioner-config
  namespace: kube-system
data:
  setPVOwnerRef: "true"
  nodeLabelsForPV: |
    - kubernetes.io/hostname
  storageClassMap: |
    local-storage:
      hostDir: /mnt/disks
      mountDir: /mnt/disks
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: local-volume-provisioner
  namespace: kube-system
  labels:
    app: local-volume-provisioner
spec:
  selector:
    matchLabels:
      app: local-volume-provisioner
  template:
    metadata:
      labels:
        app: local-volume-provisioner
    spec:
      serviceAccountName: local-storage-admin
      containers:
        - image: "quay.io/external_storage/local-volume-provisioner:v2.3.4"
          name: provisioner
          securityContext:
            privileged: true
          env:
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: JOB_CONTAINER_IMAGE
            value: "quay.io/external_storage/local-volume-provisioner:v2.3.4"
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
            limits:
              cpu: 100m
              memory: 100Mi
          volumeMounts:
            - mountPath: /etc/provisioner/config
              name: provisioner-config
              readOnly: true
            - mountPath: /mnt/disks
              name: local-disks
              mountPropagation: "HostToContainer"
      volumes:
        - name: provisioner-config
          configMap:
            name: local-provisioner-config
        - name: local-disks
          hostPath:
            path: /mnt/disks
 
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: local-storage-admin
  namespace: kube-system
 
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-pv-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: system:persistent-volume-provisioner
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: local-storage-provisioner-node-clusterrole
  namespace: kube-system
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: local-storage-provisioner-node-binding
  namespace: kube-system
subjects:
- kind: ServiceAccount
  name: local-storage-admin
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: local-storage-provisioner-node-clusterrole
  apiGroup: rbac.authorization.k8s.io

挂载磁盘，其Provisioner本身其并不提供local volume，但它在各个节点上的provisioner会去动态的“发现”挂载点（discovery directory），当某node的provisioner在/mnt/disks目录下发现有挂载点时，会创建PV，该PV的local.path就是挂载点，并设置nodeAffinity为该node。lvp.sh


########!/bin/bash
for i in $(seq 1 10); do
  mkdir -p /mnt/disks-bind/vol${i}
  mkdir -p /mnt/disks/vol${i}
  mount --bind /mnt/disks-bind/vol${i} /mnt/disks/vol${i}
done

######2、创建 静态PVC

[root@master-1 tidb]# ./lvp.sh 
[root@master-1 tidb]# ls /mnt/*
/mnt/disks:
vol1  vol2  vol3  vol4  vol5

/mnt/disks-bind:
vol1  vol2  vol3  vol4  vol5
[root@master-1 tidb]# 



########拉镜像
docker pull quay.io/external_storage/local-volume-provisioner:v2.3.4
docker save -o local-volume-provisioner-v2.3.4.tar quay.io/external_storage/local-volume-provisioner:v2.3.4
docker load -i local-volume-provisioner-v2.3.4.tar

########执行：
[root@master-1 tidb]# kubectl apply -f local-volume-provisioner.yaml
 
 
########查看：
[root@master-1 tidb]# kubectl get po -n kube-system -l app=local-volume-provisioner && kubectl get pv | grep local-storage
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          2m8s
local-volume-provisioner-f5l6c   1/1     Running   0          2m8s
local-volume-provisioner-ldjvd   1/1     Running   0          2m8s
local-pv-3208a670   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-52f01d7c   49Gi       RWO            Delete           Available           local-storage            2m3s
local-pv-8090c14a   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-836fbe4d   49Gi       RWO            Delete           Available           local-storage            2m2s
local-pv-aff38cdf   49Gi       RWO            Delete           Available           local-storage            2m2s
 

#####四、 部署并测试 TiDB Cluster

#####1、部署 TiDB Cluster
下载 TiDB cluster 的 docker images
docker pull pingcap/pd:v4.0.8
docker pull pingcap/tikv:v4.0.8
docker pull pingcap/tidb:v4.0.8
docker pull pingcap/tidb-binlog:v4.0.8
docker pull pingcap/ticdc:v4.0.8
docker pull pingcap/tiflash:v4.0.8
docker pull pingcap/tidb-monitor-reloader:v1.0.1
docker pull pingcap/tidb-monitor-initializer:v4.0.8
docker pull grafana/grafana:6.0.1
docker pull prom/prometheus:v2.18.1
docker pull busybox:1.26.2

[root@master-1 tidb]# docker pull pingcap/pd:v4.0.8;docker pull pingcap/tikv:v4.0.8;docker pull pingcap/tidb:v4.0.8;docker pull pingcap/tidb-binlog:v4.0.8;docker pull pingcap/ticdc:v4.0.8;docker pull pingcap/tiflash:v4.0.8;docker pull pingcap/tidb-monitor-reloader:v1.0.1;docker pull pingcap/tidb-monitor-initializer:v4.0.8;docker pull grafana/grafana:6.0.1;docker pull prom/prometheus:v2.18.1;docker pull busybox:1.26.2


docker save -o pd-v4.0.8.tar pingcap/pd:v4.0.8
docker save -o tikv-v4.0.8.tar pingcap/tikv:v4.0.8
docker save -o tidb-v4.0.8.tar pingcap/tidb:v4.0.8
docker save -o tidb-binlog-v4.0.8.tar pingcap/tidb-binlog:v4.0.8
docker save -o ticdc-v4.0.8.tar pingcap/ticdc:v4.0.8
docker save -o tiflash-v4.0.8.tar pingcap/tiflash:v4.0.8
docker save -o tidb-monitor-reloader-v1.0.1.tar pingcap/tidb-monitor-reloader:v1.0.1
docker save -o tidb-monitor-initializer-v4.0.8.tar pingcap/tidb-monitor-initializer:v4.0.8
docker save -o grafana-6.0.1.tar grafana/grafana:6.0.1
docker save -o prometheus-v2.18.1.tar prom/prometheus:v2.18.1
docker save -o busybox-1.26.2.tar busybox:1.26.2


docker load -i pd-v4.0.8.tar
docker load -i tikv-v4.0.8.tar
docker load -i tidb-v4.0.8.tar
docker load -i tidb-binlog-v4.0.8.tar
docker load -i ticdc-v4.0.8.tar
docker load -i tiflash-v4.0.8.tar
docker load -i tidb-monitor-reloader-v1.0.1.tar
docker load -i tidb-monitor-initializer-v4.0.8.tar
docker load -i grafana-6.0.1.tar
docker load -i prometheus-v2.18.1.tar
docker load -i busybox-1.26.2.tar


#####2、下载 TiDB 的部署 yaml 文件

Download TiDB deployment yaml file 
[root@master-1 tidb]# wget https://github.com/pingcap/tidb-operator/blob/v1.1.7/examples/advanced/tidb-cluster.yaml

部署 TiDB 集群（master 节点）[我这里是在测试搭建环境 只有一个k8s-master和k8s-node 所以副本是2，生产应该是>=3的奇数]
TiDB deployment yaml
[root@master-1 tidb]# vim tidb-cluster.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbCluster
metadata:
  name: mycluster
  namespace: mycluster
 
spec:
  version: "v4.0.8"
  timezone: UTC
  configUpdateStrategy: RollingUpdate
  hostNetwork: false
  imagePullPolicy: IfNotPresent
  helper:
    image: busybox:1.26.2
  enableDynamicConfiguration: true
 
  pd:
    enableDashboardInternalProxy: true
    baseImage: pingcap/pd
    config: {}
    replicas: 3
    requests:
      cpu: "100m"
      storage: 1Gi
    mountClusterClientSecret: false
    storageClassName: "local-storage"
 
  tidb:
    baseImage: pingcap/tidb
    replicas: 3
    requests:
      cpu: "100m"
    config: {}
    service:
      type: NodePort
      externalTrafficPolicy: Cluster
      mysqlNodePort: 30011
      statusNodePort: 30012
 
  tikv:
    baseImage: pingcap/tikv
    config: {}
    replicas: 2
    requests:
      cpu: "100m"
      storage: 1Gi
    mountClusterClientSecret: false
    storageClassName: "local-storage"
 
  tiflash:
    baseImage: pingcap/tiflash
    maxFailoverCount: 1
    replicas: 1
    storageClaims:
    - resources:
        requests:
          storage: 1Gi
      storageClassName: local-storage
 
  pump:
    baseImage: pingcap/tidb-binlog
    replicas: 1
    storageClassName: local-storage
    requests:
      storage: 1Gi
    schedulerName: default-scheduler
    config:
      addr: 0.0.0.0:8250
      gc: 7
      heartbeat-interval: 2
 
  ticdc:
    baseImage: pingcap/ticdc
    replicas: 2
    config:
      logLevel: info
 
  enablePVReclaim: false
  pvReclaimPolicy: Retain
  tlsCluster: {}
[root@master-1 tidb]# 

#####3、创建 TiDB Cluster
[root@master-1 tidb]# kubectl create namespace mycluster

[root@master-1 tidb]# kubectl apply -f tidb-cluster.yaml -n mycluster

#####4、检查 TiDB cluster 状态
[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE     IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          2m36s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          2m35s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          2m35s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          2m35s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          2m21s   10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          105s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          104s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          103s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          105s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          105s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          104s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          2m22s   10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          2m22s   10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m    10.244.2.58   node-2     <none>           <none>
[root@master-1 tidb]# 



[root@master-1 tidb]# kubectl get TidbCluster -o wide  -n mycluster       
NAME        READY   PD                  STORAGE   READY   DESIRE   TIKV                  STORAGE   READY   DESIRE   TIDB                  READY   DESIRE   STATUS                        AGE
mycluster   False   pingcap/pd:v4.0.8   1Gi       2       2        pingcap/tikv:v4.0.8   1Gi       2       2        pingcap/tidb:v4.0.8   2       2        TiFlash store(s) are not up   13m
[root@master-1 tidb]# 


[root@master-1 tidb]# kubectl get pod -n kube-system -l app=local-volume-provisioner 
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          18m
local-volume-provisioner-f5l6c   1/1     Running   0          18m
local-volume-provisioner-ldjvd   1/1     Running   0          18m
[root@master-1 tidb]# 



[root@master-1 tidb]# kubectl get po -n kube-system -l app=local-volume-provisioner && kubectl get pv | grep local-storage
NAME                             READY   STATUS    RESTARTS   AGE
local-volume-provisioner-5twgt   1/1     Running   0          6h19m
local-volume-provisioner-f5l6c   1/1     Running   0          6h19m
local-volume-provisioner-ldjvd   1/1     Running   0          6h19m
local-pv-26438be0   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-3208a670   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-1           local-storage            6h19m
local-pv-4ddc9629   49Gi       RWO            Retain           Bound       mycluster/data0-mycluster-tiflash-0   local-storage            6h8m
local-pv-52f01d7c   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-0           local-storage            6h19m
local-pv-5ebc1c66   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-660f09b0   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-2       local-storage            6h8m
local-pv-6be69e27   49Gi       RWO            Delete           Available                                         local-storage            6h8m
local-pv-8090c14a   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-0       local-storage            6h19m
local-pv-836fbe4d   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-1       local-storage            6h19m
local-pv-83bf5cf6   49Gi       RWO            Delete           Available                                         local-storage            6h8m
local-pv-95d1a179   49Gi       RWO            Retain           Bound       mycluster/tikv-mycluster-tikv-3       local-storage            6h7m
local-pv-aff38cdf   49Gi       RWO            Retain           Bound       mycluster/data-mycluster-pump-0       local-storage            6h19m
local-pv-ced17877   49Gi       RWO            Delete           Available                                         local-storage            6h7m
local-pv-e3f48c6a   49Gi       RWO            Retain           Bound       mycluster/pd-mycluster-pd-2           local-storage            6h8m
local-pv-eec6f63d   49Gi       RWO            Delete           Available                                         local-storage            6h7m

############调试命令
kubectl delete -f 
kubectl apply -f tidb-cluster.yaml -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster

kubectl get pods -n mycluster -o wide
kubectl get pv,pvc -n mycluster -o wide
kubectl get svc  -n mycluster
kubectl get event  -n mycluster
kubectl describe pod  mycluster-pd-0 -n mycluster
kubectl exec -it   -- bash
kubectl patch pv pd-detailed-tidb-pd-0  -p '{"metadata":{"finalizers":null}}' -n mycluster                 
kubectl delete pvc pd-mycluster-pd-0 -n mycluster
kubectl delete pv tidb-cluster-node-1-pv05


kubectl describe pod   mycluster-tiflash-0 -n mycluster
kubectl delete -f tidb-cluster.yaml -n mycluster
############调试命令





[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          108s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          107s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          107s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          107s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          93s    10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          57s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          56s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          55s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          57s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          57s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          56s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          94s    10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          94s    10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m   10.244.2.58   node-2     <none>           <none>
[root@master-1 tidb]# 

NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
service/mycluster-discovery      ClusterIP   10.1.158.207   <none>        10261/TCP,10262/TCP              28m
service/mycluster-pd             ClusterIP   10.1.178.77    <none>        2379/TCP                         28m
service/mycluster-pd-peer        ClusterIP   None           <none>        2380/TCP                         28m
service/mycluster-pump           ClusterIP   None           <none>        8250/TCP                         28m
service/mycluster-ticdc-peer     ClusterIP   None           <none>        8301/TCP                         27m
service/mycluster-tidb           NodePort    10.1.226.247   <none>        4000:30011/TCP,10080:30012/TCP   27m
service/mycluster-tidb-peer      ClusterIP   None           <none>        10080/TCP                        27m
service/mycluster-tiflash-peer   ClusterIP   None           <none>        3930/TCP,20170/TCP               27m
service/mycluster-tikv-peer      ClusterIP   None           <none>        20160/TCP                        28m

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mycluster-discovery   1/1     1            1           28m

NAME                                             DESIRED   CURRENT   READY   AGE
replicaset.apps/mycluster-discovery-66899c8ff6   1         1         1       28m

NAME                                 READY   AGE
statefulset.apps/mycluster-pd        2/2     28m
statefulset.apps/mycluster-pump      1/1     28m
statefulset.apps/mycluster-ticdc     2/2     27m
statefulset.apps/mycluster-tidb      2/2     27m
statefulset.apps/mycluster-tiflash   0/1     27m
statefulset.apps/mycluster-tikv      2/2     28m
[root@master-1 tidb]# 



#####五、 测试 TiDB Cluster

######1、测试链接：默认无密码：
[root@node-1 mnt]# mysql -uroot -p -h172.16.201.134 -P30011
Enter password: 
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MySQL connection id is 379
Server version: 5.7.25-TiDB-v4.0.8 TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible

Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

MySQL [(none)]> show databases;
+--------------------+
| Database           |
+--------------------+
| INFORMATION_SCHEMA |
| METRICS_SCHEMA     |
| PERFORMANCE_SCHEMA |
| mysql              |
| test               |
+--------------------+
5 rows in set (0.02 sec)

MySQL [(none)]> 

######2、查询版本号：
MySQL [(none)]> select tidb_version();
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| tidb_version()                                                                                                                                                                                                                                                                                                     |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Release Version: v4.0.8
Edition: Community
Git Commit Hash: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
Git Branch: heads/refs/tags/v4.0.8
UTC Build Time: 2020-10-30 08:21:16
GoVersion: go1.13
Race Enabled: false
TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306
Check Table Before Drop: false |
+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.03 sec)


######3、查询集群存储状态：
MySQL [(none)]> select * from INFORMATION_SCHEMA.tikv_store_status\G;
*************************** 1. row ***************************
         STORE_ID: 4
          ADDRESS: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 36.5GiB
     LEADER_COUNT: 10
    LEADER_WEIGHT: 1
     LEADER_SCORE: 10
      LEADER_SIZE: 10
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:05:52
LAST_HEARTBEAT_TS: 2021-11-10 18:12:04
           UPTIME: 6m12.048304662s
*************************** 2. row ***************************
         STORE_ID: 1001
          ADDRESS: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Down
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 0B
        AVAILABLE: 0B
     LEADER_COUNT: 0
    LEADER_WEIGHT: 1
     LEADER_SCORE: 0
      LEADER_SIZE: 0
     REGION_COUNT: 0
    REGION_WEIGHT: 1
     REGION_SCORE: 0
      REGION_SIZE: 0
         START_TS: 2021-11-10 18:09:39
LAST_HEARTBEAT_TS: 1970-01-01 00:00:00
           UPTIME: 
*************************** 3. row ***************************
         STORE_ID: 1002
          ADDRESS: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 40.35GiB
     LEADER_COUNT: 1
    LEADER_WEIGHT: 1
     LEADER_SCORE: 1
      LEADER_SIZE: 1
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:11:43
LAST_HEARTBEAT_TS: 2021-11-10 18:12:03
           UPTIME: 20.538416648s
*************************** 4. row ***************************
         STORE_ID: 1
          ADDRESS: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160
      STORE_STATE: 0
 STORE_STATE_NAME: Up
            LABEL: null
          VERSION: 4.0.8
         CAPACITY: 49.98GiB
        AVAILABLE: 36.5GiB
     LEADER_COUNT: 10
    LEADER_WEIGHT: 1
     LEADER_SCORE: 10
      LEADER_SIZE: 10
     REGION_COUNT: 21
    REGION_WEIGHT: 1
     REGION_SCORE: 21
      REGION_SIZE: 21
         START_TS: 2021-11-10 18:05:52
LAST_HEARTBEAT_TS: 2021-11-10 18:12:03
           UPTIME: 6m11.092363558s
4 rows in set (0.01 sec)

ERROR: No query specified

MySQL [(none)]> 

######4、查询tidb集群基本信息：
MySQL [(none)]> select TYPE,INSTANCE,STATUS_ADDRESS,VERSION,START_TIME from INFORMATION_SCHEMA.cluster_info;
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
| TYPE | INSTANCE                                                 | STATUS_ADDRESS                                           | VERSION | START_TIME           |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
| tidb | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:49Z |
| tidb | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:48Z |
| tidb | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 2021-11-10T18:06:49Z |
| pd   | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:37Z |
| pd   | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:37Z |
| pd   | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:05:52Z |
| tikv | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 2021-11-10T18:09:39Z |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+----------------------+
9 rows in set (0.20 sec)

MySQL [(none)]> select * from INFORMATION_SCHEMA.cluster_info;                                              
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
| TYPE | INSTANCE                                                 | STATUS_ADDRESS                                           | VERSION | GIT_HASH                                 | START_TIME           | UPTIME          |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
| tidb | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:48Z | 4m47.468737723s |
| tidb | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:49Z | 4m46.468746566s |
| tidb | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000  | mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080 | 4.0.8   | 66ac9fc31f1733e5eb8d11891ec1b38f9c422817 | 2021-11-10T18:06:49Z | 4m46.468757835s |
| pd   | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:37Z | 5m58.468760058s |
| pd   | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:37Z | 5m58.468762034s |
| pd   | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379      | 4.0.8   | 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc | 2021-11-10T18:05:52Z | 5m43.468763504s |
| tikv | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:09:39Z | 1m56.468765234s |
| tikv | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:05:52Z | 5m43.46876678s  |
| tikv | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180 | 4.0.8   | 83091173e960e5a0f5f417e921a0801d2f6635ae | 2021-11-10T18:05:52Z | 5m43.468768286s |
+------+----------------------------------------------------------+----------------------------------------------------------+---------+------------------------------------------+----------------------+-----------------+
9 rows in set (0.03 sec)



MySQL [(none)]> 
MySQL [(none)]> select * from INFORMATION_SCHEMA.cluster_info\G;
*************************** 1. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-2.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:49Z
        UPTIME: 6m1.268138556s
*************************** 2. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-1.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:48Z
        UPTIME: 6m2.268150039s
*************************** 3. row ***************************
          TYPE: tidb
      INSTANCE: mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:4000
STATUS_ADDRESS: mycluster-tidb-0.mycluster-tidb-peer.mycluster.svc:10080
       VERSION: 4.0.8
      GIT_HASH: 66ac9fc31f1733e5eb8d11891ec1b38f9c422817
    START_TIME: 2021-11-10T18:06:49Z
        UPTIME: 6m1.268151855s
*************************** 4. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-1.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:37Z
        UPTIME: 7m13.268153527s
*************************** 5. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-0.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:37Z
        UPTIME: 7m13.268155133s
*************************** 6. row ***************************
          TYPE: pd
      INSTANCE: mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379
STATUS_ADDRESS: mycluster-pd-2.mycluster-pd-peer.mycluster.svc:2379
       VERSION: 4.0.8
      GIT_HASH: 775b6a5ef517f8ab2f43fef6418bbfc7d6c9c9dc
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268156933s
*************************** 7. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:11:43Z
        UPTIME: 1m7.268158437s
*************************** 8. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268160807s
*************************** 9. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:05:52Z
        UPTIME: 6m58.268163087s
*************************** 10. row ***************************
          TYPE: tikv
      INSTANCE: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160
STATUS_ADDRESS: mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20180
       VERSION: 4.0.8
      GIT_HASH: 83091173e960e5a0f5f417e921a0801d2f6635ae
    START_TIME: 2021-11-10T18:09:39Z
        UPTIME: 3m11.268165802s
10 rows in set (10.10 sec)

ERROR: No query specified
MySQL [(none)]> 


MySQL [(none)]> create database pingcap;
Query OK, 0 rows affected (0.59 sec)

MySQL [(none)]> use pingcap;
Database changed
MySQL [pingcap]> CREATE TABLE `tab_tidb` (
    -> `id` int(11) NOT NULL AUTO_INCREMENT,
    -> `name` varchar(20) NOT NULL DEFAULT '',
    -> `age` int(11) NOT NULL DEFAULT 0,
    -> `version` varchar(20) NOT NULL DEFAULT '',
    -> PRIMARY KEY (`id`),
    -> KEY `idx_age` (`age`));
Query OK, 0 rows affected (0.28 sec)

MySQL [pingcap]> insert into `tab_tidb` values (1,'TiDB',5,'TiDB-v5.0.0');
Query OK, 1 row affected (0.08 sec)

MySQL [pingcap]> select * from tab_tidb;
+----+------+-----+-------------+
| id | name | age | version     |
+----+------+-----+-------------+
|  1 | TiDB |   5 | TiDB-v5.0.0 |
+----+------+-----+-------------+
1 row in set (0.04 sec)

MySQL [pingcap]> 


查看 TiKV store 状态、store_id、存储情况以及启动时间
MySQL [pingcap]> select STORE_ID,ADDRESS,STORE_STATE,STORE_STATE_NAME,CAPACITY,AVAILABLE,UPTIME from INFORMATION_SCHEMA.TIKV_STORE_STATUS;
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
| STORE_ID | ADDRESS                                                  | STORE_STATE | STORE_STATE_NAME | CAPACITY | AVAILABLE | UPTIME           |
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
|     1002 | mycluster-tikv-3.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 40.35GiB  | 4m30.639851204s  |
|        1 | mycluster-tikv-1.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m21.228623636s |
|        4 | mycluster-tikv-0.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m22.188095387s |
|     1001 | mycluster-tikv-2.mycluster-tikv-peer.mycluster.svc:20160 |           0 | Up               | 49.98GiB | 36.5GiB   | 10m20.223623636s |
+----------+----------------------------------------------------------+-------------+------------------+----------+-----------+------------------+
4 rows in set (0.13 sec)

MySQL [pingcap]> 


######5、初始化 TiDB 集群设置密码（master 节点）
创建 Secret 指定 root 账号密码
[root@master-1 tidb]# kubectl create secret generic tidb-secret --from-literal=root=123456 --namespace=mycluster

创建 Secret 指定 root 账号密码 自动创建其它用户
[root@master-1 tidb]# kubectl create secret generic tidb-secret --from-literal=root=123456 --from-literal=developer=123456 --namespace=mycluster
[root@master-1 tidb]#  cat ./tidb-initializer.yaml
---
apiVersion: pingcap.com/v1alpha1
kind: TidbInitializer
metadata:
  name: mycluster
  namespace: mycluster
spec:
  image: tnir/mysqlclient
  cluster:
    namespace: tidb
    name: tidb
  initSql: |-
    create database app;
  passwordSecret: tidb-secret

 
[root@master-1 tidb]# kubectl apply -f ./tidb-initializer.yaml --namespace=mycluster


######查看 TiDB 集群信息命令
kubectl get pods -n mycluster -o wide
kubectl get all -n mycluster 
kubectl get svc -n mycluster 

####### TiDB 集群配置命令
kubectl edit tc -n mycluster

#####六、 部署TiDB Cluster监控
######1、部署TidbMonitor
[root@master-1 tidb]# vim tidb-monitor.yaml
apiVersion: pingcap.com/v1alpha1
kind: TidbMonitor
metadata:
  name: mycluster
spec:
  clusters:
  - name: mycluster
  prometheus:
    baseImage: prom/prometheus
    version: v2.18.1
  grafana:
    baseImage: grafana/grafana
    version: 6.0.1
  initializer:
    baseImage: pingcap/tidb-monitor-initializer
    version: v4.0.8
  reloader:
    baseImage: pingcap/tidb-monitor-reloader
    version: v1.0.1
  imagePullPolicy: IfNotPresent
 
[root@master-1 tidb]# kubectl -n mycluster apply -f tidb-monitor.yaml
tidbinitializer.pingcap.com/mycluster configured

[root@master-1 tidb]# kubectl get TidbMonitor -n mycluster -o wide       
NAME               AGE
myclustertidbmon   4m39s


[root@master-1 tidb]# kubectl get pods -n mycluster -o wide  
NAME                                        READY   STATUS    RESTARTS   AGE    IP            NODE       NOMINATED NODE   READINESS GATES
mycluster-discovery-66899c8ff6-w66ds        1/1     Running   0          108s   10.244.1.79   node-1     <none>           <none>
mycluster-pd-0                              1/1     Running   0          107s   10.244.0.80   master-1   <none>           <none>
mycluster-pd-1                              1/1     Running   0          107s   10.244.0.81   master-1   <none>           <none>
mycluster-pd-2                              1/1     Running   0          107s   10.244.1.80   node-1     <none>           <none>
mycluster-pump-0                            1/1     Running   0          93s    10.244.0.83   master-1   <none>           <none>
mycluster-ticdc-0                           1/1     Running   0          57s    10.244.0.86   master-1   <none>           <none>
mycluster-ticdc-1                           1/1     Running   0          56s    10.244.1.82   node-1     <none>           <none>
mycluster-ticdc-2                           1/1     Running   0          55s    10.244.2.64   node-2     <none>           <none>
mycluster-tidb-0                            2/2     Running   0          57s    10.244.1.81   node-1     <none>           <none>
mycluster-tidb-1                            2/2     Running   0          57s    10.244.0.85   master-1   <none>           <none>
mycluster-tidb-2                            2/2     Running   0          56s    10.244.2.63   node-2     <none>           <none>
mycluster-tikv-0                            1/1     Running   0          94s    10.244.0.82   master-1   <none>           <none>
mycluster-tikv-1                            1/1     Running   0          94s    10.244.0.84   master-1   <none>           <none>
myclustertidbmon-monitor-6cdfdcf447-vzs2t   3/3     Running   0          177m   10.244.2.58   node-2     <none>           <none>


######2、部署ingress
借用ingress来访问，创建 tidb-ingress.yaml 文件如下：【 kubectl -n tidb apply -f ./tidb-ingress.yaml  然后配置域名】

[root@master-1 tidb]#vim tidb-ingress.yaml
apiVersion: extensions/v1beta1 
kind: Ingress 
metadata:
  name: myclusterIngress
  namespace: mycluster
spec:
  rules:
    - host: k8s.grafana.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-grafana
              servicePort: 3000
    - host: k8s.tidb.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-pd
              servicePort: 2379
    - host: k8s.prometheus.com
      http:
        paths: 
          - path: /
            backend:
              serviceName: tidb-prometheus
              servicePort: 9090

[root@master-1 tidb]# kubectl -n mycluster apply -f tidb-ingress.yaml  
ingress.extensions/myclusteringress created

[root@master-1 tidb]# kubectl get Ingress -n mycluster -o wide    
NAME               CLASS    HOSTS                                             ADDRESS   PORTS   AGE
myclusteringress   <none>   k8s.grafana.com,k8s.tidb.com,k8s.prometheus.com             80      29s


[root@master-1 tidb]# kubectl get svc -n mycluster 
NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                          AGE
mycluster-discovery                 ClusterIP   10.1.158.207   <none>        10261/TCP,10262/TCP              3h23m
mycluster-pd                        ClusterIP   10.1.178.77    <none>        2379/TCP                         3h23m
mycluster-pd-peer                   ClusterIP   None           <none>        2380/TCP                         3h23m
mycluster-pump                      ClusterIP   None           <none>        8250/TCP                         3h22m
mycluster-ticdc-peer                ClusterIP   None           <none>        8301/TCP                         3h22m
mycluster-tidb                      NodePort    10.1.226.247   <none>        4000:30011/TCP,10080:30012/TCP   3h22m
mycluster-tidb-peer                 ClusterIP   None           <none>        10080/TCP                        3h22m
mycluster-tiflash-peer              ClusterIP   None           <none>        3930/TCP,20170/TCP               3h22m
mycluster-tikv-peer                 ClusterIP   None           <none>        20160/TCP                        3h22m
myclustertidbmon-grafana            ClusterIP   10.1.228.93    <none>        3000/TCP                         5m54s
myclustertidbmon-monitor-reloader   ClusterIP   10.1.251.244   <none>        9089/TCP                         5m55s
myclustertidbmon-prometheus         ClusterIP   10.1.120.77    <none>        9090/TCP                         5m55s




######3、配置域名
lex@lexliudeMacBook-Pro Downloads %cat /etc/hosts
172.16.201.134 k8s.grafana.com
172.16.201.134 k8s.tidb.com
172.16.201.134 k8s.prometheus.com

访问即可：
访问 http://k8s.tidb.com/dashboard 用户名root 密码 空
访问 http://k8s.grafana.com/login  用户名和密码 都是admin
访问 http://k8s.prometheus.com/

以上ingress的svc 要根据自己的实际配置


######4，日志查看
[root@master-1 tidb]# kubectl logs myclustertidbmon-monitor-6cdfdcf447-vzs2t -n mycluster prometheus
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:302 msg="No time or size retention was set so using the default time retention" duration=15d
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:337 msg="Starting Prometheus" version="(version=2.18.1, branch=HEAD, revision=ecee9c8abfd118f139014cb1b174b08db3f342cf)"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:338 build_context="(go=go1.14.2, user=root@2117a9e64a7e, date=20200507-16:51:47)"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:339 host_details="(Linux 3.10.0-1160.el7.x86_64 #1 SMP Mon Oct 19 16:18:59 UTC 2020 x86_64 myclustertidbmon-monitor-6cdfdcf447-vzs2t (none))"
level=info ts=2021-11-10T15:29:05.394Z caller=main.go:340 fd_limits="(soft=1048576, hard=1048576)"
level=info ts=2021-11-10T15:29:05.395Z caller=main.go:341 vm_limits="(soft=unlimited, hard=unlimited)"
level=info ts=2021-11-10T15:29:05.420Z caller=main.go:678 msg="Starting TSDB ..."
level=info ts=2021-11-10T15:29:05.447Z caller=web.go:523 component=web msg="Start listening for connections" address=0.0.0.0:9090
level=info ts=2021-11-10T15:29:05.457Z caller=head.go:575 component=tsdb msg="Replaying WAL, this may take awhile"
level=info ts=2021-11-10T15:29:05.466Z caller=head.go:624 component=tsdb msg="WAL segment loaded" segment=0 maxSegment=0
level=info ts=2021-11-10T15:29:05.466Z caller=head.go:627 component=tsdb msg="WAL replay completed" duration=8.860784ms
level=info ts=2021-11-10T15:29:05.468Z caller=main.go:694 fs_type=XFS_SUPER_MAGIC
level=info ts=2021-11-10T15:29:05.473Z caller=main.go:695 msg="TSDB started"
level=info ts=2021-11-10T15:29:05.473Z caller=main.go:799 msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2021-11-10T15:29:05.506Z caller=kubernetes.go:253 component="discovery manager scrape" discovery=k8s msg="Using pod service account via in-cluster config"
level=info ts=2021-11-10T15:29:05.880Z caller=main.go:827 msg="Completed loading of configuration file" filename=/etc/prometheus/prometheus.yml
level=info ts=2021-11-10T15:29:05.880Z caller=main.go:646 msg="Server is ready to receive web requests."
[root@master-1 tidb]# 


[root@master-1 tidb]# kubectl logs myclustertidbmon-monitor-6cdfdcf447-vzs2t -n mycluster reloader  
2021/11/10 15:29:05 need to store latest file to store path
[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.

[GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
 - using env:   export GIN_MODE=release
 - using code:  gin.SetMode(gin.ReleaseMode)

[GIN-debug] GET    /monitoring/rules         --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).ListRules-fm (3 handlers)
[GIN-debug] GET    /monitoring/configs       --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).ListConfigs-fm (3 handlers)
[GIN-debug] GET    /monitoring/configs/:config --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).GetConfig-fm (3 handlers)
[GIN-debug] PUT    /monitoring/configs/:config --> github.com/pingcap/monitoring/reload/server/bizlogic.(*server).UpdateConfig-fm (3 handlers)
[GIN-debug] Listening and serving HTTP on 0.0.0.0:9089
[root@master-1 tidb]# 

######5，日志查看暂时开放端口：
[root@master-1 tidb]# kubectl port-forward mycluster-discovery-66899c8ff6-59qxx  -n mycluster 10262:10262                   
Forwarding from 127.0.0.1:10262 -> 10262
