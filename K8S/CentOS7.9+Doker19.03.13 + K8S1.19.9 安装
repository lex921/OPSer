# CentOS7.9+Doker19.03.13 + K8S1.19.9 安装


## 重要概念
1、cluster是 计算、存储和网络资源的集合，k8s利用这些资源运行各种基于容器的应用。   
2、master是cluster的大脑，他的主要职责是调度，即决定将应用放在那里运行。master运行linux操作系统，可以是物理机或者虚拟机。为了实现高可用，可以运行多个master。     
3、node的职责是运行容器应用。node由master管理，node负责监控并汇报容器的状态，同时根据master的要求管理容器的生命周期。node运行在linux的操作系统上，可以是物理机或者是虚拟机。     
4、pod是k8s的最小工作单元。每个pod包含一个或者多个容器。pod中的容器会作为一个整体被master调度到一个node上运行。       
5、controller k8s通常不会直接创建pod,而是通过controller来管理pod的。controller中定义了pod的部署特性，比如有几个剧本，在什么样的node上运行等。为了满足不同的业务场景，k8s提供了多种controller，包括deployment、replicaset、daemonset、statefulset、job等。     
6、deployment是最常用的controller。deployment可以管理pod的多个副本，并确保pod按照期望的状态运行。      
7、replicaset实现了pod的多副本管理。使用deployment时会自动创建replicaset，也就是说deployment是通过replicaset来管理pod的多个副本的，我们通常不需要直接使用replicaset。     
8、daemonset用于每个node最多只运行一个pod副本的场景。正如其名称所示的，daemonset通常用于运行daemon。     
9、statefuleset能够保证pod的每个副本在整个生命周期中名称是不变的，而其他controller不提供这个功能。当某个pod发生故障需要删除并重新启动时，pod的名称会发生变化，同时statefulset会保证副本按照固定的顺序启动、更新或者删除。     
10、job用于运行结束就删除的应用，而其他controller中的pod通常是长期持续运行的。     
11、service k8s的 service定义了外界访问一组特定pod的方式。service有自己的IP和端口，service为pod提供了负载均衡。    
k8s运行容器pod与访问容器这两项任务分别由controller和service执行。      
12、namespace 可以将一个物理的cluster逻辑上划分成多个虚拟cluster，每个cluster就是一个namespace。不同的namespace里的资源是完全隔离的。     


## 一、基础环境准备：

centos7.9:
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2

[root@master-1 network-scripts]# vim /etc/sysconfig/network-scripts/ifcfg-ens33
UUID=dffff97a-dd35-4535-a5b9-3856a9c49c51

TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
DEVICE=ens33
IPADDR=172.16.201.134
PREFIX=24
GATEWAY=172.16.201.2
DNS1=114.114.114.114
IPV6_PRIVACY=no
ONBOOT=yes





#### 1、设置主机名
echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts

hostnamectl set-hostname master-1
hostnamectl set-hostname node-1
hostnamectl set-hostname node-2


#### 2、关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

#### 3、关闭selinux
sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
[root@master-1 ~]# sestatus
SELinux status:                 disabled

#### 4、关闭swap
swapoff -a
vi /etc/fstab
#####/dev/mapper/centos-swap swap                    swap    defaults        0 0

echo vm.swappiness=0 >> /etc/sysctl.conf


#### 5、安装ntp
yum -y install ntp
systemctl enable ntpd
systemctl restart ntpd

crontab -e
*/10 * * * * ntpdate 1.cn.pool.ntp.org

#### 6、将桥接的IPV4流量传递到iptables 的链
[root@master-1 ~]# cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

echo '
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1' >> /etc/hosts



[root@master-1 ~]# cat /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1

[root@master-1 ~]# sysctl --system    
* Applying /usr/lib/sysctl.d/00-system.conf ...
* Applying /usr/lib/sysctl.d/10-default-yama-scope.conf ...
kernel.yama.ptrace_scope = 0
* Applying /usr/lib/sysctl.d/50-default.conf ...
kernel.sysrq = 16
kernel.core_uses_pid = 1
kernel.kptr_restrict = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.promote_secondaries = 1
net.ipv4.conf.all.promote_secondaries = 1
fs.protected_hardlinks = 1
fs.protected_symlinks = 1
* Applying /etc/sysctl.d/99-sysctl.conf ...
* Applying /etc/sysctl.d/k8s.conf ...
* Applying /etc/sysctl.conf ...


#### 7、安装Docker
##### 1）设置镜像的仓库
[root@master-1 yum.repos.d]# cd /etc/yum.repos.d/            
[root@master-1 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O/etc/yum.repos.d/docker-ce.repo        

##### 2）安装docker
[root@master-1 yum.repos.d]# yum install -y yum-utils device-mapper-persistent-data lvm2​

[root@master-1 yum.repos.d]# yum install docker-ce-19.03.13 docker-ce-cli-19.03.13 containerd.io     

指定安装的docker版本为19.03.13

##### 3)启动
[root@master ~] systemctl start docker

##### 4)配置systemd服务
[root@master-1 system]# cd  /usr/lib/systemd/system/       
[root@master-1 system]# vim docker.service      
[Unit]     
Description=Docker Application Container Engine     
Documentation=https://docs.docker.com       
After=network-online.target firewalld.service      
Wants=network-online.target      

[Service]     
Type=notify       

ExecStart=/usr/bin/dockerd     
ExecReload=/bin/kill -s HUP $MAINPID     
LimitNOFILE=infinity        
LimitNPROC=infinity       
LimitCORE=infinity      

TimeoutStartSec=0  
Delegate=yes       
KillMode=process     
Restart=on-failure       
StartLimitBurst=3     
StartLimitInterval=60s      
      
[Install]    
WantedBy=multi-user.target       


[root@master-1 system]# vim  docker.service     
[Unit]     
Description=Docker Application Container Engine  
Documentation=https://docs.docker.com       
After=network-online.target firewalld.service     
Wants=network-online.target      

[Service]      
Type=notify     
ExecStart=/usr/bin/dockerd      
ExecReload=/bin/kill -s HUP $MAINPID      
LimitNOFILE=infinity      
LimitNPROC=infinity       
LimitCORE=infinity      
TimeoutStartSec=0      
Delegate=yes    
KillMode=process       
Restart=on-failure     
StartLimitBurst=3       
StartLimitInterval=60s    

[Install]            
WantedBy=multi-user.target          

###### 5)启动一套
[root@master-1 system]# systemctl daemon-reload              
[root@master-1 system]# systemctl enable docker.service                   
Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /etc/systemd/system/docker.service.          

[root@master-1 system]# ps -aux | grep dockerd      
root       1670  0.2  3.6 591900 67708 ?        Ssl  10:52   0:00 /usr/bin/dockerd

[root@master-1 system]# systemctl status docker     
● docker.service - Docker Application Container Engine           
   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)      
   Active: active (running) since Wed 2021-09-22 10:52:20 CST; 3min 51s ago      
     Docs: https://docs.docker.com      
 Main PID: 1670 (dockerd)       
   CGroup: /system.slice/docker.service         
           ├─1670 /usr/bin/dockerd        
           └─1677 containerd --config /var/run/docker/containerd/containerd.toml --log-level info       

##### 6)镜像加速
[root@master-1 system]# cd /etc/docker       
[root@master-1 docker]# vim daemon.json                  
{             
"registry-mirrors": ["https://23h04een.mirror.aliyuncs.com"]                    
}           

##### 7)查看版本      
[root@master-1 docker]# docker version      
Client: Docker Engine - Community     
 Version:           19.03.13     
 API version:       1.40         
 Go version:        go1.13.15          
 Git commit:        4484c46d9d             
 Built:             Wed Sep 16 17:03:45 2020     
 OS/Arch:           linux/amd64      
 Experimental:      false       

Server: Docker Engine - Community       
 Engine:
  Version:          19.03.13       
  API version:      1.40 (minimum version 1.12)      
  Go version:       go1.13.15     
  Git commit:       4484c46d9d       
  Built:            Wed Sep 16 17:02:21 2020     
  OS/Arch:          linux/amd64    
  Experimental:     false 
 containerd:     
  Version:          1.4.9                 
  GitCommit:        e25210fe30a0a703442421b0f60afac609f950a3       
 runc:       
  Version:          1.0.1       
  GitCommit:        v1.0.1-0-g4144b63      
 docker-init:             
  Version:          0.18.0       
  GitCommit:        fec3683       
[root@master-1 docker]#          

##### 8)测试
[root@master-1 docker]# docker pull hello-world            
Using default tag: latest      
latest: Pulling from library/hello-world      
b8dfde127a29: Pull complete           
Digest: sha256:61bd3cb6014296e214ff4c6407a5a7e7092dfa8eefdbbec539e133e97f63e09f        
Status: Downloaded newer image for hello-world:latest      
docker.io/library/hello-world:latest           
         
[root@master-1 docker]# docker run hello-world         

Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/

[root@master-1 docker]#                    
若是出现了上图的内容则说明hello-world运行成功。



#####初始化脚本：

[root@master-1 ~]# cat set


echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts
echo "################/etc/hosts is ok #####################"

systemctl stop firewalld && systemctl disable firewalld
echo "################firewalld is ok #####################"

sed -i s/SELINUX=enforcing/SELINUX=disabled/g /etc/selinux/config
echo "################selinux is ok #####################"

swapoff -a
echo "################swapoff is ok #####################"

yum -y install ntp;systemctl enable ntpd;systemctl restart ntpd
echo "################ntpd is ok #####################"

echo '
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1' >> /etc/sysctl.d/k8s.conf

sysctl --system

echo "################bridge is ok #####################"

yum install wget net-tools vim -y
cd /etc/yum.repos.d/
wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O/etc/yum.repos.d/docker-ce.repo  
yum install -y yum-utils device-mapper-persistent-data lvm2
yum install -y docker-ce-19.03.13 docker-ce-cli-19.03.13 containerd.io 
echo '
{             
"registry-mirrors": ["https://23h04een.mirror.aliyuncs.com"]                    
} 
' >> /etc/docker/daemon.json

systemctl start docker; systemctl enable docker.service
systemctl status docker;

docker version
echo "################docker is ok #####################"



docker pull hello-world
docker run hello-world

echo "################docker running is ok #####################"
[root@master-1 ~]# 

执行：
[root@master-1 ~]# ./set 

#####手动修改：
1、vi /etc/fstab
#####/dev/mapper/centos-swap swap                    swap    defaults        0 0
2、vim /usr/lib/systemd/system/docker.service        
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd

#REBOOT


##### 9)复制虚拟机，到其他两台，修改ip 和 主机名，配置免密     
[root@node-1 ~]# vim /etc/sysconfig/network-scripts/ifcfg-ens33 
修改IP
注掉UUID=

echo '
172.16.201.134  master-1
172.16.201.135  node-1
172.16.201.136  node-2' >> /etc/hosts

hostnamectl set-hostname master-1
hostnamectl set-hostname node-1
hostnamectl set-hostname node-2

   
分别复制key到master   
[root@node-1 ~]# ssh-keygen
[root@node-1 .ssh]# ssh-copy-id root@172.16.201.134  
[root@node-2 .ssh]# ssh-keygen   
[root@node-2 .ssh]# ssh-copy-id root@172.16.201.134            

分别复制key到node-1、node-2    
[root@master-1 .ssh]# ssh-keygen  
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.135
[root@master-1 .ssh]# ssh-copy-id root@172.16.201.136

ssh-copy-id root@172.16.201.134
ssh-copy-id root@172.16.201.135
ssh-copy-id root@172.16.201.136

ssh-copy-id master-1
ssh-copy-id node-1
ssh-copy-id node-2

ssh master-1
ssh node-1
ssh node-2

master：       
[root@master-1 .ssh]# cat authorized_keys            
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0Z5AFJNpJMy/dKI39iGMUOVOMGiczhzqdnRKLX2ikyPrAHpXCdOz+3nhXE4m3V1pGWo9gXZNyUG8b2cqtVHLCLGsq4j1qTB9JqVBsWl0kE137E5UEJ0bs9OoKMofeiBQhKPTbS9uCoTrJbJHLr0DAFEE30EYmMtq6thPkTn6eTAzaMHfVH16b76orOLYQ1SWKYPrFMAAz8uDBQ8ncbslneYJF9K9rVHVzNdLJ5/FUmygI2sKXwVpcH7DwpuNK0wPOrMlp0HWeeVeaBoW73POi3Gd2ON7NUP37/U6veai6p7HbAU7AuteUdQlhdE8sj9kD49aDQCwuv4UkEFzuPRO1 root@node-1         
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC5EpSsO0h0nfj3kTAKkF/IW5MnmWBN0W3SAw26lh51K/TVmt9v6nvESpqiD1NkSFpNxqx77JljDI0N6fwZN8V3+FyPKU863fPZT0WU1KlwLUp70yCRXjpyO1QaT/f1sdlPs+Tkrair2vXbav41snbT5/2Sze3QiNUMS/Z6g3RfwpXmmd5epetp3VwwkGdrJGt1LDrPnE+YUx8rjQknv1rZCJISXsejdeQFDA01/CPFQxt1Qhu/uhywS2g+qIZPbxf/vBdm779x2q/ctX+3bjnaPmZdaIXG2JRgD5a//f0Uur/wjVHwrJzgPbo9GIE4dGdP8dW7Ni04Uym4aL9q2Iv5 root@node-2          

##### 10)测试
[root@node-2 .ssh]# ssh root@172.16.201.134         
Last login: Wed Sep 22 11:14:30 2021 from 172.16.201.1       
[root@master-1 ~]# exit      
[root@node-2 .ssh]#       

master：      
[root@master-1 ~]# ssh root@172.16.201.136       
Last login: Wed Sep 22 11:14:35 2021 from 172.16.201.1      
[root@node-2 ~]# exit       
logout
Connection to 172.16.201.136 closed.       
[root@master-1 ~]# ssh root@172.16.201.135      
Last login: Wed Sep 22 11:14:33 2021 from 172.16.201.1           
[root@node-1 ~]# exit       
logout                 
Connection to 172.16.201.135 closed.            






##二、安装 Kubernetes

k8s所有版本的github库：
https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#downloads-for-v1222


####1、添加阿里云YUM软件源
[root@master-1 ~]#  cd /etc/yum.repos.d/
[root@master-1 yum.repos.d]# vim kubernetes.repo     
 
[kubernetes]
name = kubernetes
baseurl =  https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled = 1
gpgcheck = 0
repo_gpgcheck = 0
gpgkey =  https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg 
https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg

######注意：gpgcheck = 0，repo_gpgcheck = 0一定关掉，否则报错。


####2、版本查看
[root@master-1 yum.repos.d]# yum repolist

[root@master-1 yum.repos.d]# yum list kubelet --showduplicates |sort -r
 * updates: mirrors.cn99.com
Loading mirror speeds from cached hostfile
Loaded plugins: fastestmirror
kubelet.x86_64                       1.22.3-0                         kubernetes
kubelet.x86_64                       1.22.2-0                         kubernetes
kubelet.x86_64                       1.22.1-0                         kubernetes
kubelet.x86_64                       1.22.0-0                         kubernetes
kubelet.x86_64                       1.21.6-0                         kubernetes
kubelet.x86_64                       1.21.5-0                         kubernetes
kubelet.x86_64                       1.21.4-0                         kubernetes
kubelet.x86_64                       1.21.3-0                         kubernetes
kubelet.x86_64                       1.21.2-0                         kubernetes
kubelet.x86_64                       1.21.1-0                         kubernetes
kubelet.x86_64                       1.21.0-0                         kubernetes
kubelet.x86_64                       1.20.9-0                         kubernetes
kubelet.x86_64                       1.20.8-0                         kubernetes
kubelet.x86_64                       1.20.7-0                         kubernetes
kubelet.x86_64                       1.20.6-0                         kubernetes
kubelet.x86_64                       1.20.5-0                         kubernetes
kubelet.x86_64                       1.20.4-0                         kubernetes
kubelet.x86_64                       1.20.2-0                         kubernetes
kubelet.x86_64                       1.20.12-0                        kubernetes
kubelet.x86_64                       1.20.11-0                        kubernetes
kubelet.x86_64                       1.20.1-0                         kubernetes
kubelet.x86_64                       1.20.10-0                        kubernetes
kubelet.x86_64                       1.20.0-0                         kubernetes
kubelet.x86_64                       1.19.9-0                         kubernetes
kubelet.x86_64                       1.19.8-0                         kubernetes
kubelet.x86_64                       1.19.7-0                         kubernetes
kubelet.x86_64                       1.19.6-0                         kubernetes
kubelet.x86_64                       1.19.5-0                         kubernetes
kubelet.x86_64                       1.19.4-0                         kubernetes
kubelet.x86_64                       1.19.3-0                         kubernetes
kubelet.x86_64                       1.19.2-0                         kubernetes
kubelet.x86_64                       1.19.16-0                        kubernetes
kubelet.x86_64                       1.19.15-0                        kubernetes
kubelet.x86_64                       1.19.14-0                        kubernetes
kubelet.x86_64                       1.19.13-0                        kubernetes
kubelet.x86_64                       1.19.12-0                        kubernetes
kubelet.x86_64                       1.19.11-0                        kubernetes
kubelet.x86_64                       1.19.1-0                         kubernetes
kubelet.x86_64                       1.19.10-0                        kubernetes
kubelet.x86_64                       1.19.0-0                         kubernetes
kubelet.x86_64                       1.18.9-0                         kubernetes
kubelet.x86_64                       1.18.8-0                         kubernetes
kubelet.x86_64                       1.18.6-0                         kubernetes
kubelet.x86_64                       1.18.5-0                         kubernetes
kubelet.x86_64                       1.18.4-1                         kubernetes
kubelet.x86_64                       1.18.4-0                         kubernetes
kubelet.x86_64                       1.18.3-0                         kubernetes
kubelet.x86_64                       1.18.2-0                         kubernetes
kubelet.x86_64                       1.18.20-0                        kubernetes
kubelet.x86_64                       1.18.19-0                        kubernetes
kubelet.x86_64                       1.18.18-0                        kubernetes
kubelet.x86_64                       1.18.17-0                        kubernetes
kubelet.x86_64                       1.18.16-0                        kubernetes
kubelet.x86_64                       1.18.15-0                        kubernetes
kubelet.x86_64                       1.18.14-0                        kubernetes
kubelet.x86_64                       1.18.13-0                        kubernetes
kubelet.x86_64                       1.18.12-0                        kubernetes
kubelet.x86_64                       1.18.1-0                         kubernetes
kubelet.x86_64                       1.18.10-0                        kubernetes
kubelet.x86_64                       1.18.0-0                         kubernetes
kubelet.x86_64                       1.17.9-0                         kubernetes
kubelet.x86_64                       1.17.8-0                         kubernetes
kubelet.x86_64                       1.17.7-1                         kubernetes
kubelet.x86_64                       1.17.7-0                         kubernetes
kubelet.x86_64                       1.17.6-0                         kubernetes
kubelet.x86_64                       1.17.5-0                         kubernetes
kubelet.x86_64                       1.17.4-0                         kubernetes
kubelet.x86_64                       1.17.3-0                         kubernetes
kubelet.x86_64                       1.17.2-0                         kubernetes
kubelet.x86_64                       1.17.17-0                        kubernetes
kubelet.x86_64                       1.17.16-0                        kubernetes
kubelet.x86_64                       1.17.15-0                        kubernetes
kubelet.x86_64                       1.17.14-0                        kubernetes
kubelet.x86_64                       1.17.13-0                        kubernetes
kubelet.x86_64                       1.17.12-0                        kubernetes
kubelet.x86_64                       1.17.11-0                        kubernetes
kubelet.x86_64                       1.17.1-0                         kubernetes
kubelet.x86_64                       1.17.0-0                         kubernetes
kubelet.x86_64                       1.16.9-0                         kubernetes
kubelet.x86_64                       1.16.8-0                         kubernetes
kubelet.x86_64                       1.16.7-0                         kubernetes
kubelet.x86_64                       1.16.6-0                         kubernetes
kubelet.x86_64                       1.16.5-0                         kubernetes
kubelet.x86_64                       1.16.4-0                         kubernetes
kubelet.x86_64                       1.16.3-0                         kubernetes
kubelet.x86_64                       1.16.2-0                         kubernetes
kubelet.x86_64                       1.16.15-0                        kubernetes
kubelet.x86_64                       1.16.14-0                        kubernetes
kubelet.x86_64                       1.16.13-0                        kubernetes
kubelet.x86_64                       1.16.12-0                        kubernetes
kubelet.x86_64                       1.16.11-1                        kubernetes
kubelet.x86_64                       1.16.11-0                        kubernetes
kubelet.x86_64                       1.16.1-0                         kubernetes
kubelet.x86_64                       1.16.10-0                        kubernetes
kubelet.x86_64                       1.16.0-0                         kubernetes
kubelet.x86_64                       1.15.9-0                         kubernetes
kubelet.x86_64                       1.15.8-0                         kubernetes
kubelet.x86_64                       1.15.7-0                         kubernetes
kubelet.x86_64                       1.15.6-0                         kubernetes
kubelet.x86_64                       1.15.5-0                         kubernetes
kubelet.x86_64                       1.15.4-0                         kubernetes
kubelet.x86_64                       1.15.3-0                         kubernetes
kubelet.x86_64                       1.15.2-0                         kubernetes
kubelet.x86_64                       1.15.12-0                        kubernetes
kubelet.x86_64                       1.15.11-0                        kubernetes
kubelet.x86_64                       1.15.1-0                         kubernetes
kubelet.x86_64                       1.15.10-0                        kubernetes
kubelet.x86_64                       1.15.0-0                         kubernetes
kubelet.x86_64                       1.14.9-0                         kubernetes
kubelet.x86_64                       1.14.8-0                         kubernetes
kubelet.x86_64                       1.14.7-0                         kubernetes
kubelet.x86_64                       1.14.6-0                         kubernetes
kubelet.x86_64                       1.14.5-0                         kubernetes
kubelet.x86_64                       1.14.4-0                         kubernetes
kubelet.x86_64                       1.14.3-0                         kubernetes
kubelet.x86_64                       1.14.2-0                         kubernetes
kubelet.x86_64                       1.14.1-0                         kubernetes
kubelet.x86_64                       1.14.10-0                        kubernetes
kubelet.x86_64                       1.14.0-0                         kubernetes
kubelet.x86_64                       1.13.9-0                         kubernetes
kubelet.x86_64                       1.13.8-0                         kubernetes
kubelet.x86_64                       1.13.7-0                         kubernetes
kubelet.x86_64                       1.13.6-0                         kubernetes
kubelet.x86_64                       1.13.5-0                         kubernetes
kubelet.x86_64                       1.13.4-0                         kubernetes
kubelet.x86_64                       1.13.3-0                         kubernetes
kubelet.x86_64                       1.13.2-0                         kubernetes
kubelet.x86_64                       1.13.12-0                        kubernetes
kubelet.x86_64                       1.13.11-0                        kubernetes
kubelet.x86_64                       1.13.1-0                         kubernetes
kubelet.x86_64                       1.13.10-0                        kubernetes
kubelet.x86_64                       1.13.0-0                         kubernetes
kubelet.x86_64                       1.12.9-0                         kubernetes
kubelet.x86_64                       1.12.8-0                         kubernetes
kubelet.x86_64                       1.12.7-0                         kubernetes
kubelet.x86_64                       1.12.6-0                         kubernetes
kubelet.x86_64                       1.12.5-0                         kubernetes
kubelet.x86_64                       1.12.4-0                         kubernetes
kubelet.x86_64                       1.12.3-0                         kubernetes
kubelet.x86_64                       1.12.2-0                         kubernetes
kubelet.x86_64                       1.12.1-0                         kubernetes
kubelet.x86_64                       1.12.10-0                        kubernetes
kubelet.x86_64                       1.12.0-0                         kubernetes
kubelet.x86_64                       1.11.9-0                         kubernetes
kubelet.x86_64                       1.11.8-0                         kubernetes
kubelet.x86_64                       1.11.7-0                         kubernetes
kubelet.x86_64                       1.11.6-0                         kubernetes
kubelet.x86_64                       1.11.5-0                         kubernetes
kubelet.x86_64                       1.11.4-0                         kubernetes
kubelet.x86_64                       1.11.3-0                         kubernetes
kubelet.x86_64                       1.11.2-0                         kubernetes
kubelet.x86_64                       1.11.1-0                         kubernetes
kubelet.x86_64                       1.11.10-0                        kubernetes
kubelet.x86_64                       1.11.0-0                         kubernetes
kubelet.x86_64                       1.10.9-0                         kubernetes
kubelet.x86_64                       1.10.8-0                         kubernetes
kubelet.x86_64                       1.10.7-0                         kubernetes
kubelet.x86_64                       1.10.6-0                         kubernetes
kubelet.x86_64                       1.10.5-0                         kubernetes
kubelet.x86_64                       1.10.4-0                         kubernetes
kubelet.x86_64                       1.10.3-0                         kubernetes
kubelet.x86_64                       1.10.2-0                         kubernetes
kubelet.x86_64                       1.10.13-0                        kubernetes
kubelet.x86_64                       1.10.12-0                        kubernetes
kubelet.x86_64                       1.10.11-0                        kubernetes
kubelet.x86_64                       1.10.1-0                         kubernetes
kubelet.x86_64                       1.10.10-0                        kubernetes
kubelet.x86_64                       1.10.0-0                         kubernetes
 * extras: mirrors.cn99.com
 * base: mirrors.163.com
Available Packages
[root@master-1 yum.repos.d]# 

本文安装的kubelet版本是1.16.4，该版本支持的docker最高版本是1.22.2


####3、安装kubelet、 kubeadm、 kubectl
[root@master-1 rpm-gpg]# yum install  kubelet-1.19.9-0 kubeadm-1.19.9-0 kubectl-1.19.9-0 --nogpgcheck -y
Installed:
  kubeadm.x86_64 0:1.19.9-0                           kubectl.x86_64 0:1.19.9-0                           kubelet.x86_64 0:1.19.9-0                          
Dependency Installed:
  conntrack-tools.x86_64 0:1.4.4-7.el7                 cri-tools.x86_64 0:1.13.0-0                          kubernetes-cni.x86_64 0:0.8.7-0                   
  libnetfilter_cthelper.x86_64 0:1.0.0-11.el7          libnetfilter_cttimeout.x86_64 0:1.0.0-7.el7          libnetfilter_queue.x86_64 0:1.0.2-2.el7_2         
  socat.x86_64 0:1.7.3.2-2.el7                        

Complete!


######安装包说明
kubelet 运行在集群所有节点上，用于启动Pod和容器等对象的工具
kubeadm 用于初始化集群，启动集群的命令工具
kubectl 用于和集群通信的命令行，通过kubectl可以部署和管理应用，查看各种资源，创建、删除和更新各种组件


######报错： 
Failing package is: kubectl-1.16.2-0.x86_64
GPG Keys are configured as: https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg

解：用--nogpgcheck跳过key校验


######此处不可以启动kubelet，由于还有fannel等插件未安装完成，启动会报失败
[root@master-1 ~]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@node-1 yum.repos.d]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@node-2 yum.repos.d]# systemctl enable kubelet 
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.


[root@master-1 yum.repos.d]# systemctl start kubelet       
[root@master-1 yum.repos.d]# systemctl status kubelet


#备注：以上操作所有的节点都要操作。
#如果以下步操作失败，可以通过 kubeadm reset 命令来清理环境重新安装。


####4、部署Kubernetes Master
######1)master：初始化kubeadm
[root@master-1 ~]# kubeadm init \
--apiserver-advertise-address=172.16.201.134 \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.19.9 \
--service-cidr=10.1.0.0/16 \
--pod-network-cidr=10.244.0.0/16

######参数说明：
–image-repository string：    这个用于指定从什么位置来拉取镜像（1.13版本才有的），默认值是k8s.gcr.io，我们将其指定为国内镜像地址：registry.aliyuncs.com/google_containers
–kubernetes-version string： 指定kubenets版本号，默认值是stable-1，会导致从https://dl.k8s.io/release/stable-1.txt下载最新的版本号，我们可以将其指定为固定版本（v1.15.1）来跳过网络请求。
–apiserver-advertise-address  指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。
–pod-network-cidr      指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 –pod-network-cidr有自己的要求，这里设置为10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。


[root@master-1 .ssh]# kubeadm init \
> --apiserver-advertise-address=172.16.201.134 \
> --image-repository registry.aliyuncs.com/google_containers \
> --kubernetes-version v1.19.9 \
> --service-cidr=10.1.0.0/16 \
> --pod-network-cidr=10.244.0.0/16
W1029 14:38:51.937827    2491 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
[init] Using Kubernetes version: v1.19.9
[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master-1] and IPs [10.1.0.1 172.16.201.134]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master-1] and IPs [172.16.201.134 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master-1] and IPs [172.16.201.134 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.
[apiclient] All control plane components are healthy after 68.505356 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.19" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node master-1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
[mark-control-plane] Marking the node master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 5hc28a.77wkxjobg7zkk1ag
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.201.134:6443 --token j3wis7.tg9szxlukb8ywtgv \
    --discovery-token-ca-cert-hash sha256:e1563969d45a83c8befb63037ce439d016fad27dbef0aae182507d2a121925c3
[root@master-1 .ssh]# 

######注意：
建议至少2 cpu ,2G，非硬性要求，1cpu，1G也可以搭建起集群。
1个cpu的话初始化master的时候会报 [WARNING NumCPU]: the number of available CPUs 1 is less than the required 2
部署插件或者pod时可能会报warning：FailedScheduling：Insufficient cpu, Insufficient memory
如果出现这种提示，说明你的虚拟机分配的CPU为1核，需要重新设置虚拟机master节点内核数。


######问题：
[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/

######解决
更改docker的启动参数
$ vim /usr/lib/systemd/system/docker.service
#####ExecStart=/usr/bin/dockerd
ExecStart=/usr/bin/dockerd --exec-opt native.cgroupdriver=systemd
重启docker
[root@master-1 ~]# systemctl daemon-reload
[root@master-1 ~]# systemctl restart docker



######2)node节点执行加入集群：
[root@node-1 ~]# kubeadm join 172.16.201.134:6443 --token j3wis7.tg9szxlukb8ywtgv \
    --discovery-token-ca-cert-hash sha256:e1563969d45a83c8befb63037ce439d016fad27dbef0aae182507d2a121925c3

[preflight] Running pre-flight checks
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

######3)node查看状态：
[root@node-1 yum.repos.d]# docker ps -a
CONTAINER ID        IMAGE                                                COMMAND                  CREATED             STATUS                         PORTS               NAMES
4e8fd3436209        registry.aliyuncs.com/google_containers/kube-proxy   "/usr/local/bin/kube…"   2 minutes ago       Up 2 minutes                                       k8s_kube-proxy_kube-proxy-bkck2_kube-system_13d1d3e9-de92-4601-9486-cb9fae99f600_0
251063bf8160        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 2 minutes ago       Up 2 minutes                                       k8s_POD_kube-proxy-bkck2_kube-system_13d1d3e9-de92-4601-9486-cb9fae99f600_0
[root@node-1 yum.repos.d]# 

[root@node-2 yum.repos.d]# docker ps -a
CONTAINER ID        IMAGE                                                COMMAND                  CREATED             STATUS                         PORTS               NAMES
529215b26f16        registry.aliyuncs.com/google_containers/kube-proxy   "/usr/local/bin/kube…"   2 minutes ago       Up 2 minutes                                       k8s_kube-proxy_kube-proxy-phjdh_kube-system_3f1d0d74-2dcd-4d48-82f8-37b2a5558a0c_0
2f5f9eaf2f56        registry.aliyuncs.com/google_containers/pause:3.2    "/pause"                 2 minutes ago       Up 2 minutes                                       k8s_POD_kube-proxy-phjdh_kube-system_3f1d0d74-2dcd-4d48-82f8-37b2a5558a0c_0



[root@node-1 yum.repos.d]#  docker images
REPOSITORY                                           TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy   v1.19.9             4a76fb49d490        6 months ago        118MB
hello-world                                          latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/pause        3.2                 80d28bedfe5d        19 months ago       683kB
[root@node-1 yum.repos.d]# 


[root@node-2 yum.repos.d]# docker images
REPOSITORY                                           TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy   v1.19.9             4a76fb49d490        6 months ago        118MB
hello-world                                          latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/pause        3.2                 80d28bedfe5d        19 months ago       683kB

######4) master查看状态：
[root@master-1 kubernetes]# docker images
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
registry.aliyuncs.com/google_containers/kube-proxy                v1.19.9             4a76fb49d490        6 months ago        118MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.19.9             1a4f1f05177f        6 months ago        119MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.19.9             a8fd6520f73d        6 months ago        111MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.19.9             8f1e66e40394        6 months ago        46.5MB
hello-world                                                       latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/etcd                      3.4.13-0            0369cf4303ff        13 months ago       253MB
registry.aliyuncs.com/google_containers/coredns                   1.7.0               bfe3a36ebd25        15 months ago       45.2MB
registry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        19 months ago       683kB
[root@master-1 kubernetes]# 



######5) master:使用kubectl工具
[root@master-1 ~]#  kubectl get nodes
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")


[root@master-1 ~]# mkdir -p $HOME/.kube
[root@master-1 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
cp: overwrite ‘/root/.kube/config’? y
[root@master-1 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config
[root@master-1 ~]#  kubectl get nodes                                      
NAME       STATUS     ROLES    AGE     VERSION
master-1   NotReady   master   2m54s   v1.19.9
node-1     NotReady   <none>   2m9s    v1.19.9
node-2     NotReady   <none>   97s     v1.19.9
重新创建集群时，这个目录还是存在的，于是我尝试在执行这几个命令前先执行rm -rf $HOME/.kube命令删除这个目录


######6) master:安装Pod网络插件（CNI）(master)
master:目前是NotReady
[root@master-1 kubernetes]#  kubectl get nodes
NAME       STATUS     ROLES    AGE   VERSION
master-1   NotReady   master   30m   v1.19.9
node-1     NotReady   <none>   13m   v1.19.9
node-2     NotReady   <none>   13m   v1.19.9


[root@master-1 ~]#wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

[root@master-1 ~]# cat kube-flannel.yml
---
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp.flannel.unprivileged
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default
    seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default
    apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default
    apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default
spec:
  privileged: false
  volumes:
  - configMap
  - secret
  - emptyDir
  - hostPath
  allowedHostPaths:
  - pathPrefix: "/etc/cni/net.d"
  - pathPrefix: "/etc/kube-flannel"
  - pathPrefix: "/run/flannel"
  readOnlyRootFilesystem: false
  # Users and groups
  runAsUser:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  # Privilege Escalation
  allowPrivilegeEscalation: false
  defaultAllowPrivilegeEscalation: false
  # Capabilities
  allowedCapabilities: ['NET_ADMIN', 'NET_RAW']
  defaultAddCapabilities: []
  requiredDropCapabilities: []
  # Host namespaces
  hostPID: false
  hostIPC: false
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  # SELinux
  seLinux:
    # SELinux is unused in CaaSP
    rule: 'RunAsAny'
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs: ['use']
  resourceNames: ['psp.flannel.unprivileged']
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-system
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-system
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-system
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni
        image: quay.io/coreos/flannel:v0.14.0
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: quay.io/coreos/flannel:v0.14.0
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
          limits:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
[root@master-1 ~]# 

######7)master:安装 ，执行命令：
[root@master-1 ~]#kubectl apply -f kube-flannel.yml
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created


######8)master:查看安装状态： 
[root@master-1 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS            RESTARTS   AGE
coredns-6d56c8448f-mp5lz           0/1     Pending           0          38m
coredns-6d56c8448f-wnqxj           0/1     Pending           0          38m
etcd-master-1                      1/1     Running           0          38m
kube-apiserver-master-1            1/1     Running           0          38m
kube-controller-manager-master-1   1/1     Running           0          38m
kube-flannel-ds-mmhsm              1/1     Running           0          37s
kube-flannel-ds-rz2xj              0/1     PodInitializing   0          37s
kube-flannel-ds-ts9fm              1/1     Running           0          37s
kube-proxy-bkck2                   1/1     Running           0          22m
kube-proxy-c6fdx                   1/1     Running           0          38m
kube-proxy-phjdh                   1/1     Running           0          21m
kube-scheduler-master-1            1/1     Running           0          38m

######master:安装好了，kube-flannel都Running了
[root@master-1 ~]# kubectl get pods -n kube-system
NAME                               READY   STATUS    RESTARTS   AGE
coredns-6d56c8448f-mp5lz           1/1     Running   0          75m
coredns-6d56c8448f-wnqxj           1/1     Running   0          75m
etcd-master-1                      1/1     Running   0          38m
kube-apiserver-master-1            1/1     Running   0          38m
kube-controller-manager-master-1   1/1     Running   0          38m
kube-flannel-ds-mmhsm              1/1     Running   0          38s
kube-flannel-ds-rz2xj              1/1     Running   0          38s
kube-flannel-ds-ts9fm              1/1     Running   0          38s
kube-proxy-bkck2                   1/1     Running   0          22m
kube-proxy-c6fdx                   1/1     Running   0          38m
kube-proxy-phjdh                   1/1     Running   0          21m
kube-scheduler-master-1            1/1     Running   0          38m



######master:再次查看node，可以看到状态为ready
[root@master-1 ~]# kubectl get node
NAME       STATUS   ROLES    AGE   VERSION
master-1   Ready    master   39m   v1.19.9
node-1     Ready    <none>   22m   v1.19.9
node-2     Ready    <none>   22m   v1.19.9

######开机器启动配置：
systemctl enable kubelet      
systemctl status kubelet
systemctl restart kubelet



######master:
[root@master-1 ~]# docker images
REPOSITORY                                                        TAG                 IMAGE ID            CREATED             SIZE
quay.io/coreos/flannel                                            v0.14.0             8522d622299c        4 months ago        67.9MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.19.9             4a76fb49d490        6 months ago        118MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.19.9             1a4f1f05177f        6 months ago        119MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.19.9             8f1e66e40394        6 months ago        46.5MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.19.9             a8fd6520f73d        6 months ago        111MB
hello-world                                                       latest              d1165f221234        6 months ago        13.3kB
registry.aliyuncs.com/google_containers/etcd                      3.4.13-0            0369cf4303ff        13 months ago       253MB
registry.aliyuncs.com/google_containers/coredns                   1.7.0               bfe3a36ebd25        15 months ago       45.2MB
registry.aliyuncs.com/google_containers/pause                     3.2                 80d28bedfe5d        19 months ago       683kB

######master:
[root@master-1 ~]# docker ps -a
CONTAINER ID        IMAGE                                               COMMAND                  CREATED             STATUS                     PORTS               NAMES
992baf6abefa        8522d622299c                                        "/opt/bin/flanneld -…"   2 minutes ago       Up 2 minutes                                   k8s_kube-flannel_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
78be74ae6cad        quay.io/coreos/flannel                              "cp -f /etc/kube-fla…"   2 minutes ago       Exited (0) 2 minutes ago                       k8s_install-cni_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
e30ad41e86d3        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 3 minutes ago       Up 3 minutes                                   k8s_POD_kube-flannel-ds-rz2xj_kube-system_50c36e3b-0ad3-409c-a601-c8f85aa36428_0
72df4a6ccc2a        4a76fb49d490                                        "/usr/local/bin/kube…"   40 minutes ago      Up 40 minutes                                  k8s_kube-proxy_kube-proxy-c6fdx_kube-system_89fdfc40-c7c8-4152-91ac-94a45f83cb1d_0
ee455bc6ca60        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-proxy-c6fdx_kube-system_89fdfc40-c7c8-4152-91ac-94a45f83cb1d_0
802350660d95        8f1e66e40394                                        "kube-scheduler --au…"   40 minutes ago      Up 40 minutes                                  k8s_kube-scheduler_kube-scheduler-master-1_kube-system_0b7a3f4a1ea56044c3ad536d56a87725_0
ee4008c4f1d5        a8fd6520f73d                                        "kube-controller-man…"   40 minutes ago      Up 40 minutes                                  k8s_kube-controller-manager_kube-controller-manager-master-1_kube-system_6f065f021a2517457fa9368ab63b44f2_0
effb22b25ffa        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-scheduler-master-1_kube-system_0b7a3f4a1ea56044c3ad536d56a87725_0
60930447b83b        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 40 minutes ago      Up 40 minutes                                  k8s_POD_kube-controller-manager-master-1_kube-system_6f065f021a2517457fa9368ab63b44f2_0
d85b5859f2bb        1a4f1f05177f                                        "kube-apiserver --ad…"   41 minutes ago      Up 41 minutes                                  k8s_kube-apiserver_kube-apiserver-master-1_kube-system_7fb363525d550d682b33d84cb74dda9d_0
61ee22184e58        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 41 minutes ago      Up 41 minutes                                  k8s_POD_kube-apiserver-master-1_kube-system_7fb363525d550d682b33d84cb74dda9d_0
d0fc20c3ba74        0369cf4303ff                                        "etcd --advertise-cl…"   41 minutes ago      Up 41 minutes                                  k8s_etcd_etcd-master-1_kube-system_d9c8fc0f468e5e6013762f18eeab5cda_0
fdc7cad113c4        registry.aliyuncs.com/google_containers/pause:3.2   "/pause"                 41 minutes ago      Up 41 minutes                                  k8s_POD_etcd-master-1_kube-system_d9c8fc0f468e5e6013762f18eeab5cda_0


######master:
[root@master-1 ~]# kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
cdeix4.h84buwepsp1yx14v   23h         2021-09-23T12:04:22+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token



######master:
[root@master-1 ~]# kubectl get node -o wide
NAME       STATUS   ROLES    AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME
master-1   Ready    master   42m   v1.19.9   172.16.201.134   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-1     Ready    <none>   26m   v1.19.9   172.16.201.135   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13
node-2     Ready    <none>   25m   v1.19.9   172.16.201.136   <none>        CentOS Linux 7 (Core)   3.10.0-1160.el7.x86_64   docker://19.3.13




#####master:问题：健康检查错误：
[root@master-1 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS      MESSAGE                                                                                       ERROR
scheduler            Unhealthy   Get "http://127.0.0.1:10251/healthz": dial tcp 127.0.0.1:10251: connect: connection refused   
controller-manager   Unhealthy   Get "http://127.0.0.1:10252/healthz": dial tcp 127.0.0.1:10252: connect: connection refused   
etcd-0               Healthy     {"health":"true"}    

解：
cd /etc/kubernetes/manifests/
[root@master-1 manifests]# vim kube-controller-manager.yaml
[root@master-1 manifests]# vim kube-scheduler.yaml 
2个文件的    - --port=0 注释掉后等一会



[root@master-1 manifests]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   

[root@master-1 manifests]# kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   


[root@master-1 manifests]# kubectl get ns
NAME              STATUS   AGE
default           Active   6m39s
kube-node-lease   Active   6m42s
kube-public       Active   6m42s
kube-system       Active   6m42s

[root@master-1 manifests]# kubectl get no
NAME       STATUS   ROLES    AGE     VERSION
master-1   Ready    master   7m23s   v1.19.9
node-1     Ready    <none>   6m38s   v1.19.9
node-2     Ready    <none>   6m6s    v1.19.9

[root@master-1 manifests]#  kubectl get pod -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE
kube-system   coredns-6d56c8448f-7c4dp           1/1     Running   0          7m28s
kube-system   coredns-6d56c8448f-fkwql           1/1     Running   0          7m28s
kube-system   etcd-master-1                      1/1     Running   0          7m47s
kube-system   kube-apiserver-master-1            1/1     Running   0          7m46s
kube-system   kube-controller-manager-master-1   1/1     Running   0          2m3s
kube-system   kube-flannel-ds-cvmdw              1/1     Running   0          3m48s
kube-system   kube-flannel-ds-gcckr              1/1     Running   0          3m48s
kube-system   kube-flannel-ds-nkl2p              1/1     Running   0          3m48s
kube-system   kube-proxy-fkj8w                   1/1     Running   0          6m31s
kube-system   kube-proxy-glbfj                   1/1     Running   0          7m28s
kube-system   kube-proxy-pxfg7                   1/1     Running   0          7m3s
kube-system   kube-scheduler-master-1            1/1     Running   0          103s


[root@master-1 manifests]# kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   8m


[root@master-1 ~]# kubectl get serviceaccount
NAME      SECRETS   AGE
default   1         104m


[root@master-1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.201.134:6443
KubeDNS is running at https://172.16.201.134:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.

####5、 测试kubernetes集群
######1、安装nginx来测试


[root@master-1 ~]# kubectl create deployment nginx --image=nginx
deployment.apps/nginx created
[root@master-1 ~]# kubectl expose deployment nginx --port=80 --type=NodePort
service/nginx exposed


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS    RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-lkmk7   1/1     Running   0          3m53s   10.244.2.5   node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h50m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   2m28s   app=nginx

####得到service端口转换后的端口为：80:30698


######访问效果测试：
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.134:30698/
200
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.135:30698/
200
[root@master-1 ~]# curl -I -m 10 -o /dev/null -s -w %{http_code} http://172.16.201.136:30698/
200


######2、测试扩容情况（扩容到3个副本）
[root@master-1 ~]# kubectl scale deployment nginx --replicas=3
deployment.apps/nginx scaled
[root@master-1 ~]# 


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS              RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-c25s6   0/1     ContainerCreating   0          16s     <none>       node-1   <none>           <none>
pod/nginx-6799fc88d8-lkmk7   1/1     Running             0          9m34s   10.244.2.5   node-2   <none>           <none>
pod/nginx-6799fc88d8-xwjqr   0/1     ContainerCreating   0          16s     <none>       node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h56m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   8m9s    app=nginx


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                         READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
pod/nginx-6799fc88d8-c25s6   1/1     Running   0          50s   10.244.1.4   node-1   <none>           <none>
pod/nginx-6799fc88d8-lkmk7   1/1     Running   0          10m   10.244.2.5   node-2   <none>           <none>
pod/nginx-6799fc88d8-xwjqr   1/1     Running   0          50s   10.244.2.6   node-2   <none>           <none>

NAME                 TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
service/kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        6h56m   <none>
service/nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   8m43s   app=nginx



[root@master-1 manifests]#  kubectl get pods  --all-namespaces -o wide
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE   IP               NODE       NOMINATED NODE   READINESS GATES
kube-system   coredns-6d56c8448f-7c4dp           1/1     Running   0          31m   10.244.0.2       master-1   <none>           <none>
kube-system   coredns-6d56c8448f-fkwql           1/1     Running   0          31m   10.244.2.2       node-2     <none>           <none>
kube-system   etcd-master-1                      1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-apiserver-master-1            1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-controller-manager-master-1   1/1     Running   0          26m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-flannel-ds-cvmdw              1/1     Running   0          27m   172.16.201.135   node-1     <none>           <none>
kube-system   kube-flannel-ds-gcckr              1/1     Running   0          27m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-flannel-ds-nkl2p              1/1     Running   0          27m   172.16.201.136   node-2     <none>           <none>
kube-system   kube-proxy-fkj8w                   1/1     Running   0          30m   172.16.201.136   node-2     <none>           <none>
kube-system   kube-proxy-glbfj                   1/1     Running   0          31m   172.16.201.134   master-1   <none>           <none>
kube-system   kube-proxy-pxfg7                   1/1     Running   0          31m   172.16.201.135   node-1     <none>           <none>
kube-system   kube-scheduler-master-1            1/1     Running   0          25m   172.16.201.134   master-1   <none>           <none>



[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide        
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
default                nginx-6799fc88d8-c25s6                       1/1     Running   0          11m     10.244.1.4       node-1     <none>           <none>
default                nginx-6799fc88d8-lkmk7                       1/1     Running   0          20m     10.244.2.5       node-2     <none>           <none>
default                nginx-6799fc88d8-xwjqr                       1/1     Running   0          11m     10.244.2.6       node-2     <none>           <none>
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          7h6m    10.244.1.2       node-1     <none>           <none>
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          7h6m    10.244.2.2       node-2     <none>           <none>
kube-system            etcd-master-1                                1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-apiserver-master-1                      1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-controller-manager-master-1             1/1     Running   1          6h21m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          6h29m   172.16.201.135   node-1     <none>           <none>
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          6h29m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          6h29m   172.16.201.136   node-2     <none>           <none>
kube-system            kube-proxy-bkck2                             1/1     Running   0          6h50m   172.16.201.135   node-1     <none>           <none>
kube-system            kube-proxy-c6fdx                             1/1     Running   1          7h6m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-proxy-phjdh                             1/1     Running   0          6h50m   172.16.201.136   node-2     <none>           <none>
kube-system            kube-scheduler-master-1                      1/1     Running   1          6h21m   172.16.201.134   master-1   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          5h1m    10.244.1.3       node-1     <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          5h1m    10.244.2.4       node-2     <none>           <none>







####查看顺序：
######查看 namespace信息：
[root@master-1 ~]# kubectl get namespace
NAME                   STATUS   AGE
default                Active   7h23m
kube-node-lease        Active   7h23m
kube-public            Active   7h23m
kube-system            Active   7h23m
kubernetes-dashboard   Active   5h17m

######查看namespace的service信息：
[root@master-1 ~]# kubectl get service -o wide --namespace=default
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE     SELECTOR
kubernetes   ClusterIP   10.1.0.1      <none>        443/TCP        7h22m   <none>
nginx        NodePort    10.1.198.44   <none>        80:30698/TCP   34m     app=nginx

另一种写法：
[root@master-1 ~]# kubectl get service/nginx
NAME    TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
nginx   NodePort   10.1.198.44   <none>        80:30698/TCP   43m


######查看svc nginx日志
[root@master-1 ~]# kubectl describe svc nginx
Name:                     nginx
Namespace:                default
Labels:                   app=nginx
Annotations:              <none>
Selector:                 app=nginx
Type:                     NodePort
IP:                       10.1.198.44
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  30698/TCP
Endpoints:                10.244.1.4:80,10.244.2.5:80,10.244.2.6:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
[root@master-1 ~]# 

######查看pod name
[root@master-1 ~]# kubectl get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP           NODE     NOMINATED NODE   READINESS GATES
nginx-6799fc88d8-c25s6   1/1     Running   0          24m   10.244.1.4   node-1   <none>           <none>
nginx-6799fc88d8-lkmk7   1/1     Running   0          34m   10.244.2.5   node-2   <none>           <none>
nginx-6799fc88d8-xwjqr   1/1     Running   0          24m   10.244.2.6   node-2   <none>           <none>
[root@master-1 ~]# 
[root@master-1 ~]# 
[root@master-1 ~]# 

######查看pod nginx-6799fc88d8-c25s6 日志：
[root@master-1 ~]# kubectl describe pod nginx-6799fc88d8-c25s6
Name:         nginx-6799fc88d8-c25s6
Namespace:    default
Priority:     0
Node:         node-1/172.16.201.135
Start Time:   Wed, 22 Sep 2021 19:00:21 +0800
Labels:       app=nginx
              pod-template-hash=6799fc88d8
Annotations:  <none>
Status:       Running
IP:           10.244.1.4
IPs:
  IP:           10.244.1.4
Controlled By:  ReplicaSet/nginx-6799fc88d8
Containers:
  nginx:
    Container ID:   docker://5cf5621c44344d802efb5d3c565aeaf98ce7f67732c009a10d798bf9919737d4
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:853b221d3341add7aaadf5f81dd088ea943ab9c918766e295321294b035f3f3e
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Wed, 22 Sep 2021 19:00:55 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-b4d9n (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-b4d9n:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-b4d9n
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  25m   default-scheduler  Successfully assigned default/nginx-6799fc88d8-c25s6 to node-1
  Normal  Pulling    25m   kubelet            Pulling image "nginx"
  Normal  Pulled     24m   kubelet            Successfully pulled image "nginx" in 32.403763647s
  Normal  Created    24m   kubelet            Created container nginx
  Normal  Started    24m   kubelet            Started container nginx



docker exec -it 5cf5621c4434 /bin/bash

######删除nginx：
[root@master-1 ~]# kubectl get deployment
NAME    READY   UP-TO-DATE   AVAILABLE   AGE
nginx   3/3     3            3           21h
[root@master-1 ~]# kubectl delete deployment nginx 
deployment.apps "nginx" deleted

[root@master-1 ~]# kubectl get deployment
No resources found in default namespace.

[root@master-1 ~]# kubectl delete service nginx
service "nginx" deleted

[root@master-1 ~]#  kubectl get pod,svc -o wide        
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   28h   <none>




###出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点
######查看k8s-master表示不运行pod
[root@master-1 nacos-k8s]# kubectl describe node k8s-master |grep Taints
Taints: node-role.kubernetes.io/master:NoSchedule
######查看k8s-master表示运行pod
[root@master-1 nacos-k8s]# kubectl describe node master-1 |grep Taints                 
Taints:             <none>

######让 master节点参与POD负载的命令为
[root@master-1 nacos-k8s]# kubectl taint nodes master-1 node-role.kubernetes.io/master-
node/master-1 untainted
######让 master节点恢复不参与POD负载的命令为 
[root@master-1 nacos-k8s]# kubectl taint nodes k8s-master node-role.kubernetes.io/master=:NoSchedule





##三、搭建K8S Dashboard
1. 下载dashboard文件：
[root@master-1 ~]#  wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml
[root@master-1 ~]# 


[root@master-1 ~]# cat recommended.yaml


apiVersion: v1
kind: Namespace
metadata:
  name: kubernetes-dashboard

---

apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001
  type: NodePort
  selector:
    k8s-app: kubernetes-dashboard

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-certs
  namespace: kubernetes-dashboard
type: Opaque

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-csrf
  namespace: kubernetes-dashboard
type: Opaque
data:
  csrf: ""

---

apiVersion: v1
kind: Secret
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-key-holder
  namespace: kubernetes-dashboard
type: Opaque

---

kind: ConfigMap
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard-settings
  namespace: kubernetes-dashboard

---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
rules:
  # Allow Dashboard to get, update and delete Dashboard exclusive secrets.
  - apiGroups: [""]
    resources: ["secrets"]
    resourceNames: ["kubernetes-dashboard-key-holder", "kubernetes-dashboard-certs", "kubernetes-dashboard-csrf"]
    verbs: ["get", "update", "delete"]
    # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["kubernetes-dashboard-settings"]
    verbs: ["get", "update"]
    # Allow Dashboard to get metrics.
  - apiGroups: [""]
    resources: ["services"]
    resourceNames: ["heapster", "dashboard-metrics-scraper"]
    verbs: ["proxy"]
  - apiGroups: [""]
    resources: ["services/proxy"]
    resourceNames: ["heapster", "http:heapster:", "https:heapster:", "dashboard-metrics-scraper", "http:dashboard-metrics-scraper"]
    verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
rules:
  # Allow Metrics Scraper to get metrics from the Metrics server
  - apiGroups: ["metrics.k8s.io"]
    resources: ["pods", "nodes"]
    verbs: ["get", "list", "watch"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kubernetes-dashboard
subjects:
  - kind: ServiceAccount
    name: kubernetes-dashboard
    namespace: kubernetes-dashboard

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
        - name: kubernetes-dashboard
          image: kubernetesui/dashboard:v2.2.0
          imagePullPolicy: Always
          ports:
            - containerPort: 8443
              protocol: TCP
          args:
            - --auto-generate-certificates
            - --namespace=kubernetes-dashboard
            # Uncomment the following line to manually specify Kubernetes API server Host
            # If not specified, Dashboard will attempt to auto discover the API server and connect
            # to it. Uncomment only if the default does not work.
            # - --apiserver-host=http://my-address:port
          volumeMounts:
            - name: kubernetes-dashboard-certs
              mountPath: /certs
              # Create on-disk volume to store exec logs
            - mountPath: /tmp
              name: tmp-volume
          livenessProbe:
            httpGet:
              scheme: HTTPS
              path: /
              port: 8443
            initialDelaySeconds: 30
            timeoutSeconds: 30
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      volumes:
        - name: kubernetes-dashboard-certs
          secret:
            secretName: kubernetes-dashboard-certs
        - name: tmp-volume
          emptyDir: {}
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule

---

kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  ports:
    - port: 8000
      targetPort: 8000
  selector:
    k8s-app: dashboard-metrics-scraper

---

kind: Deployment
apiVersion: apps/v1
metadata:
  labels:
    k8s-app: dashboard-metrics-scraper
  name: dashboard-metrics-scraper
  namespace: kubernetes-dashboard
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: dashboard-metrics-scraper
  template:
    metadata:
      labels:
        k8s-app: dashboard-metrics-scraper
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'
    spec:
      containers:
        - name: dashboard-metrics-scraper
          image: kubernetesui/metrics-scraper:v1.0.6
          ports:
            - containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              scheme: HTTP
              path: /
              port: 8000
            initialDelaySeconds: 30
            timeoutSeconds: 30
          volumeMounts:
          - mountPath: /tmp
            name: tmp-volume
          securityContext:
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            runAsUser: 1001
            runAsGroup: 2001
      serviceAccountName: kubernetes-dashboard
      nodeSelector:
        "kubernetes.io/os": linux
      # Comment the following tolerations if Dashboard must not be deployed on master
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      volumes:
        - name: tmp-volume
          emptyDir: {}
[root@master-1 ~]# 



2.下载镜像镜像【k8s.gcr.io#kubernetes-dashboard-amd64.tar】
链接: https://pan.baidu.com/s/1vQUGpx89TZw-HyxCVE972w 提取码: 5pui 复制这段内容后打开百度网盘手机App，操作更方便哦


3. 创建kubernetes-dashboard：注意：修改【recommended.yaml】文件里面的镜像地址
kubectl create -f  recommended.yaml



4. 卸载之前安装的内容：
kubectl delete -f  recommended.yaml


6、修改访问端口
[root@master-1 ~]# sed -i '/targetPort: 8443/a\ \ \ \ \ \ nodePort: 30001\n\ \ type: NodePort' recommended.yaml​ 
 访问nodePort: 30001 修改成30001


7. 重新安装最新版本dashboard：
kubectl create -f recommended.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml


[root@master-1 ~]# kubectl get pods --all-namespaces
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          126m
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          126m
kube-system            etcd-master-1                                1/1     Running   1          126m
kube-system            kube-apiserver-master-1                      1/1     Running   1          126m
kube-system            kube-controller-manager-master-1             1/1     Running   1          81m
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          89m
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          89m
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          89m
kube-system            kube-proxy-bkck2                             1/1     Running   0          110m
kube-system            kube-proxy-c6fdx                             1/1     Running   1          126m
kube-system            kube-proxy-phjdh                             1/1     Running   0          110m
kube-system            kube-scheduler-master-1                      1/1     Running   1          81m
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          72s
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          72s


[root@master-1 ~]# kubectl get pod,svc -o wide
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE    SELECTOR
service/kubernetes   ClusterIP   10.1.0.1     <none>        443/TCP   135m   <none>


kubernetes-dashboard直接访问30001端口即可：
[root@master-1 ~]# kubectl get all -n kubernetes-dashboard
NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          6m12s
pod/kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          6m12s

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
service/dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP        6m12s
service/kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP   6m12s

NAME                                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/dashboard-metrics-scraper   1/1     1            1           6m12s
deployment.apps/kubernetes-dashboard        1/1     1            1           6m12s

NAME                                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/dashboard-metrics-scraper-79c5968bdc   1         1         1       6m12s
replicaset.apps/kubernetes-dashboard-9f9799597         1         1         1       6m12s
[root@master-1 ~]# 


[root@master-1 ~]#  kubectl get services -o wide --all-namespaces
NAMESPACE              NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE    SELECTOR
default                kubernetes                  ClusterIP   10.1.0.1       <none>        443/TCP                  156m   <none>
kube-system            kube-dns                    ClusterIP   10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   156m   k8s-app=kube-dns
kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP                 30m    k8s-app=dashboard-metrics-scraper
kubernetes-dashboard   kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP            30m    k8s-app=kubernetes-dashboard



8. 获取token：

[root@master-1 ~]# kubectl -n kube-system describe $(kubectl -n kube-system get secret -n kube-system -o name | grep namespace) | grep token
Name:         namespace-controller-token-rzwtv
Type:  kubernetes.io/service-account-token
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImgzZVplbFI3YUx0R0pwX1lrNTlQQ3pNWTdHcmJmUkMxb1h3VHNtd0NGbVkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlci10b2tlbi1yend0diIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJuYW1lc3BhY2UtY29udHJvbGxlciIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjlhYTM3YTNkLTE4MTEtNDRhMS05ZDUxLThkMzBkZjQzNDdlNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTpuYW1lc3BhY2UtY29udHJvbGxlciJ9.zl8IraeIK3zIYhzyYl3lTWCqGfYgW_O2gnOAEKi44XQhvpFFoOaxxkS7wqjXWaenfCUZPHJtNUWYTH804Ku2y_Om6g6fJNv8C8Aj2dyYpgnaifLcIEVi3pGWLmV2eG78rf1sXbObIylSrshoTWGwV-CVXRWNvZGAMFs7uV4tY8tddA7UZi8QGjBP33Ea0s7rbnPUOQmWhe82szWqQZz5MTg9mFHzFsyJSfl2tzZVzGL7fp4Df5Ce3PSmoGGirUmL3pjnfSqbASMYD9xbGXHsDJC1N4h1OiAG6HxwnuD8DrS4hNPgWiAWnSysf1mKni9UiAL54xN6UA-aSjvxNX7N1Q
[root@master-1 ~]# 



9.如果谷歌浏览器访问不了，则用火狐访问：
http://nodeIp:nodePort
https://172.16.201.136:30001/#/workloads?namespace=default
接受风险，输入令牌，即可图形化管理k8s集群


查看服务被分配到哪个节点上：
[root@master-1 ~]# kubectl get pods  --all-namespaces -o wide                 
NAMESPACE              NAME                                         READY   STATUS    RESTARTS   AGE     IP               NODE       NOMINATED NODE   READINESS GATES
kube-system            coredns-6d56c8448f-mp5lz                     1/1     Running   0          3h16m   10.244.1.2       node-1     <none>           <none>
kube-system            coredns-6d56c8448f-wnqxj                     1/1     Running   0          3h16m   10.244.2.2       node-2     <none>           <none>
kube-system            etcd-master-1                                1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-apiserver-master-1                      1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-controller-manager-master-1             1/1     Running   1          151m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-mmhsm                        1/1     Running   0          159m    172.16.201.135   node-1     <none>           <none>
kube-system            kube-flannel-ds-rz2xj                        1/1     Running   1          159m    172.16.201.134   master-1   <none>           <none>
kube-system            kube-flannel-ds-ts9fm                        1/1     Running   0          159m    172.16.201.136   node-2     <none>           <none>
kube-system            kube-proxy-bkck2                             1/1     Running   0          3h      172.16.201.135   node-1     <none>           <none>
kube-system            kube-proxy-c6fdx                             1/1     Running   1          3h16m   172.16.201.134   master-1   <none>           <none>
kube-system            kube-proxy-phjdh                             1/1     Running   0          3h      172.16.201.136   node-2     <none>           <none>
kube-system            kube-scheduler-master-1                      1/1     Running   1          151m    172.16.201.134   master-1   <none>           <none>
kubernetes-dashboard   dashboard-metrics-scraper-79c5968bdc-bcm4d   1/1     Running   0          71m     10.244.1.3       node-1     <none>           <none>
kubernetes-dashboard   kubernetes-dashboard-9f9799597-529zk         1/1     Running   0          71m     10.244.2.4       node-2     <none>           <none>



10、清理测试环境
[root@master-1 ~]# kubectl delete -f recommended.yaml
namespace "kubernetes-dashboard" deleted
serviceaccount "kubernetes-dashboard" deleted
service "kubernetes-dashboard" deleted
secret "kubernetes-dashboard-certs" deleted
secret "kubernetes-dashboard-csrf" deleted
secret "kubernetes-dashboard-key-holder" deleted
configmap "kubernetes-dashboard-settings" deleted
role.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
clusterrole.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
rolebinding.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
clusterrolebinding.rbac.authorization.k8s.io "kubernetes-dashboard" deleted
deployment.apps "kubernetes-dashboard" deleted
service "dashboard-metrics-scraper" deleted
deployment.apps "dashboard-metrics-scraper" deleted




##四、管理K8S

######查看合并后的kubeconfig设置，或者一个指定的kubeconfig配置文件
[root@master-1 ~]# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://172.16.201.134:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED



######查看集群信息
[root@master-1 ~]# kubectl cluster-info
Kubernetes master is running at https://172.16.201.134:6443
KubeDNS is running at https://172.16.201.134:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.


######列出 Pod 中的容器
可以使用 range 操作进一步控制格式化，以单独操作每个元素。
[root@master-1 ~]# kubectl get pods --all-namespaces -o=jsonpath='{range .items[*]}{"\n"}{.metadata.name}{":\t"}{range .spec.containers[*]}{.image}{", "}{end}{end}' |\
> sort
coredns-6d56c8448f-mp5lz:       registry.aliyuncs.com/google_containers/coredns:1.7.0, 
coredns-6d56c8448f-wnqxj:       registry.aliyuncs.com/google_containers/coredns:1.7.0, 
dashboard-metrics-scraper-79c5968bdc-bcm4d:     kubernetesui/metrics-scraper:v1.0.6, 
etcd-master-1:  registry.aliyuncs.com/google_containers/etcd:3.4.13-0, 
kube-apiserver-master-1:        registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9, 
kube-controller-manager-master-1:       registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9, 
kube-flannel-ds-mmhsm:  quay.io/coreos/flannel:v0.14.0, 
kube-flannel-ds-rz2xj:  quay.io/coreos/flannel:v0.14.0, 
kube-flannel-ds-ts9fm:  quay.io/coreos/flannel:v0.14.0, 
kube-proxy-bkck2:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kube-proxy-c6fdx:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kube-proxy-phjdh:       registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9, 
kubernetes-dashboard-9f9799597-529zk:   kubernetesui/dashboard:v2.2.0, 
kube-scheduler-master-1:        registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9, 

######列出以标签过滤后的 Pod 的所有容器
要获取匹配特定标签的 Pod，请使用 -l 参数。以下匹配仅与标签 app=nginx 相符的 Pod。
kubectl get pods --all-namespaces -o=jsonpath="{.items[*].spec.containers[*].image}" -l app=nginx

######列出以命名空间过滤后的 Pod 的所有容器
要获取匹配特定命名空间的 Pod，请使用 namespace 参数。以下仅匹配 kube-system 命名空间下的 Pod。
[root@master-1 ~]# kubectl get pods --namespace kube-system -o jsonpath="{.items[*].spec.containers[*].image}"
registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9[root@master-1 ~]# 

######使用 go-template 代替 jsonpath 来获取容器
作为 jsonpath 的替代，Kubectl 支持使用 go-templates 来格式化输出
[root@master-1 ~]# kubectl get pods --all-namespaces -o go-template --template="{{range .items}}{{range .spec.containers}}{{.image}} {{end}}{{end}}"
registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/coredns:1.7.0 registry.aliyuncs.com/google_containers/etcd:3.4.13-0 registry.aliyuncs.com/google_containers/kube-apiserver:v1.19.9 registry.aliyuncs.com/google_containers/kube-controller-manager:v1.19.9 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 quay.io/coreos/flannel:v0.14.0 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-proxy:v1.19.9 registry.aliyuncs.com/google_containers/kube-scheduler:v1.19.9 kubernetesui/metrics-scraper:v1.0.6 kubernetesui/dashboard:v2.2.0 [root@master-1 ~]# 


#####注意：重新初始化得先执行这几步
docker rm -f `docker ps -a -q`
rm -rf /etc/kubernetes/*
rm -rf /var/lib/etcd/
kubeadm reset




#####移除NODE节点的方法（master执行）
1：先将节点设置为维护模式(k8s-node1是节点名称)

[root@k8s-master ~]# kubectl drain k8s-node1 --delete-local-data --force --ignore-daemonsets
node/k8s-node1 cordoned
node/k8s-node1 drained
2：然后删除节点

[root@k8s-master ~]# kubectl delete node k8s-node1
node "k8s-node1" drained
3：查看节点

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS   ROLES    AGE    VERSION
k8s-master   Ready    master   18m    v1.17.0
k8s-node2    Ready    <none>   5m7s   v1.17.0
发现k8s-node1节点已经被删除了

####如果这个时候再想添加进来这个node，需要执行两步操作
1：停掉kubelet(需要添加进来的节点操作)

[root@k8s-node2 ~]# systemctl stop kubelet
2：删除相关文件

[root@k8s-node2 ~]# rm -rf /etc/kubernetes/*
3：添加节点

kubeadm join 192.168.182.135:6443 --token 7rpjfp.n3vg39zrgstzr0rs \
--discovery-token-ca-cert-hash sha256:8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234
4：查看节点

[root@k8s-master ~]# kubectl get nodes
NAME         STATUS     ROLES    AGE   VERSION
k8s-master   Ready      master   24m   v1.17.0
k8s-node1    NotReady   <none>   6s    v1.17.0
k8s-node2    Ready      <none>   10m   v1.17.0





####忘掉token再次添加进k8s集群
1：主节点执行命令

获取token
[root@k8s-master ~]# kubeadm token list
TOKEN                     TTL         EXPIRES                     USAGES                   DESCRIPTION                                                EXTRA GROUPS
7rpjfp.n3vg39zrgstzr0rs   23h         2019-12-30T20:01:50+08:00   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token

2： 获取ca证书sha256编码hash值
[root@k8s-master ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234

3：从节点执行如下的命令
[root@k8s-master ~]# systemctl stop kubelet

4：删除相关文件
[root@k8s-master ~]# rm -rf /etc/kubernetes/*

5：加入集群
指定主节点IP，端口是6443
在生成的证书前有sha256:
kubeadm join 192.168.64.10:6443 --token 7rpjfp.n3vg39zrgstzr0rs  --discovery-token-ca-cert-hash sha256:8c5aa1a4e82e70fed62b02e8d7bff54c801251b5ee40c7cec68a8c214dcc1234


####常用命令
1、查看node
#######o wide以yaml格式显示详细信息
kubectl get node -o wide

2、创建deployments
kubectl run net-test --image=alpine --replicas=2 sleep 10

3、查看deployments详情
kubectl describe deployment net-test

4、删除deployments
kubectl delete deployment net-test -n default

5、查看pod
kubectl get pod -o wide

6、查看pod的详情
kubectl describe pod net-test-5767cb94df-7lwtq

7、手动扩容缩容
#######通过执行扩容命令，对某个deployment直接进行扩容：
$ kubectl  scale deployment net-test --replicas=4
#######当要缩容，减少副本数量即可：
$ kubectl  scale deployment net-test --replicas=2






####三个类型端口
#####1：三个类型端口所应用位置的不同
port是service的的端口

targetport是pod也就是容器的端口

nodeport是容器所在宿主机的端口(实质上也是通过service暴露给了宿主机，而port却没有)

#####2：在作用上

######port
的主要作用是集群内其他pod访问本pod的时候，需要的一个port
如nginx的pod访问mysql的pod，那么mysql的pod的service可以如下定义，由此可以这样理解，port是service的port，nginx访问service的33306
apiVersion: v1
kind: Service
metadata:
name: mysql-service
spec:
ports:
- port: 33306
targetPort: 3306
selector:
name: mysql-pod
targetport

同样的，看上面的targetport，targetport说过是pod暴露出来的port端口，当nginx的一个请求到达service的33306端口时，service就会将此请求根据selector中的name，将请求转发到mysql-pod这个pod的3306端口上


######nodeport
nodeport就很好理解了，它是集群外的客户访问，集群内的服务时，所访问的port，比如客户访问下面的集群中的nginx，就是这样的方式，ip:30001
apiVersion: v1
kind: Service
metadata:
name: nginx-service
spec:
type: NodePort // 有配置NodePort，外部流量可访问k8s中的服务
ports:
- port: 30080 // 服务访问端口
targetPort: 80 // 容器端口
nodePort: 30001 // NodePort
selector: name: nginx-pod

nodeport是集群外流量访问集群内服务的端口类型，比如客户访问nginx，apache，port是集群内的pod互相通信用的端口类型，比如nginx访问mysql，而mysql是不需要让客户访问到的，最后targetport，顾名思义，目标端口，也就是最终端口，也就是pod的端口。

#####3：总结一下
nodeport是集群外流量访问集群内服务的端口类型，比如客户访问nginx，apache，port是集群内的pod互相通信用的端口类型
比如nginx访问mysql，而mysql是不需要让客户访问到的，最后targetport，顾名思义，目标端口，也就是最终端口，也就是pod的端口。



[root@master-1 ~]# kubectl get svc  --all-namespaces
NAMESPACE              NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                  AGE
default                kubernetes                  ClusterIP   10.1.0.1       <none>        443/TCP                  6h5m
kube-system            kube-dns                    ClusterIP   10.1.0.10      <none>        53/UDP,53/TCP,9153/TCP   6h5m
kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.1.156.183   <none>        8000/TCP                 3h59m
kubernetes-dashboard   kubernetes-dashboard        NodePort    10.1.27.195    <none>        443:30001/TCP            3h59m
